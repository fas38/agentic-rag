{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda_envs\\rag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List,Optional, Dict, Any, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# llama index imports\n",
    "import llama_index.core\n",
    "from llama_index.core import SimpleDirectoryReader,VectorStoreIndex,SummaryIndex, StorageContext, Settings, load_index_from_storage, Response\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.node_parser import SentenceSplitter, CodeSplitter, LangchainNodeParser\n",
    "from llama_index.core.tools import FunctionTool,QueryEngineTool\n",
    "from llama_index.core.vector_stores import MetadataFilters,FilterCondition\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "from llama_index.readers.file import IPYNBReader, PandasCSVReader\n",
    "from llama_index.experimental.query_engine import PandasQueryEngine\n",
    "from llama_index.experimental.query_engine.pandas import (\n",
    "    PandasInstructionParser,\n",
    ")\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.query_pipeline import (\n",
    "    QueryPipeline as QP,\n",
    "    Link,\n",
    "    InputComponent,\n",
    "    AgentFnComponent,\n",
    "    AgentInputComponent\n",
    ")\n",
    "from llama_index.readers.file import IPYNBReader, PandasCSVReader\n",
    "from llama_index.experimental.query_engine import PandasQueryEngine\n",
    "from llama_index.experimental.query_engine.pandas import (\n",
    "    PandasInstructionParser,\n",
    ")\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.query_pipeline import (\n",
    "    QueryPipeline as QP,\n",
    "    Link,\n",
    "    InputComponent,\n",
    "    CustomQueryComponent,\n",
    "    FnComponent\n",
    ")\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "\n",
    "# llama index agent imports\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker, ReActAgent, Task, AgentChatResponse, AgentRunner, QueryPipelineAgentWorker\n",
    "\n",
    "# llama index llms and embeddings imports\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# custom package imports\n",
    "from llama_index.packs.tables.chain_of_table.base import ChainOfTableQueryEngine, serialize_table\n",
    "\n",
    "# langchain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "# tools\n",
    "import nest_asyncio # to allow running async functions in jupyter\n",
    "import chromadb # persistent storage for vectors\n",
    "# import nbconvert\n",
    "import tree_sitter\n",
    "import tree_sitter_languages\n",
    "import phoenix as px\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [phoenix.session.session] Existing running Phoenix instance detected! Shutting it down and starting a new instance...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "ðŸ“– For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "temperture = 0.0 #for deterministic results\n",
    "\n",
    "llm_model = \"mistral-large-latest\"\n",
    "MISTRAL_API_KEY =  \"BWdlihu9sUh5P2g3bHnzjAaHiT4anTVH\"\n",
    "os.environ[\"MISTRAL_API_KEY\"] = MISTRAL_API_KEY\n",
    "llm = MistralAI(model=llm_model, temperature=temperture)\n",
    "\n",
    "# llm_model = \"codellama\"\n",
    "# llm = Ollama(model=llm_model, request_timeout=1200.0, base_url=\"http://localhost:11434\", temperature=temperture)\n",
    "\n",
    "nest_asyncio.apply() # to allow running async functions in jupyter\n",
    "px.launch_app()\n",
    "llama_index.core.set_global_handler(\"arize_phoenix\")\n",
    "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading file\n",
    "file_path = \"./data_csv/Sepsis_Processed_IC.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Pipeline Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prompt modules\n",
    "instruction_str = (\n",
    "    \"1. Convert the query to executable Python code using Pandas.\\n\"\n",
    "    \"2. The final line of code should be a Python expression that can be called with the `eval()` function.\\n\"\n",
    "    \"3. The code should represent a solution to the query.\\n\"\n",
    "    \"4. PRINT ONLY THE EXPRESSION.\\n\"\n",
    "    \"5. Do not quote the expression.\\n\"\n",
    ")\n",
    "\n",
    "pandas_prompt_str = (\n",
    "    \"You are working with a pandas dataframe in Python.\\n\"\n",
    "    \"The name of the dataframe is `df`.\\n\"\n",
    "    \"This is the result of `print(df.head())`:\\n\"\n",
    "    \"{df_str}\\n\\n\"\n",
    "    \"This is a brief description of the dataframe:\\n\"\n",
    "    \"This table records patient data related to sepsis diagnosis. The Target column indicates whether the patient had sepsis. The duration_since_reg column describes the patient's stay after admission in days. Diagnosis-related columns detail specific diagnostic results and associated codes. The dataset includes patient demographics age, clinical measurements (crp, lacticacid, leucocytes), and diagnostic procedures (diagnosticartastrup, diagnosticblood, etc.). It also tracks clinical criteria for sepsis (sirscritheartrate, sirscritleucos, etc.), resource usage, and event transitions (e.g., CRP => ER Triage). Additional columns capture organ dysfunction, hypotension, hypoxia, suspected infection, and treatment details like infusions and oliguria. The dataset covers the transitions between various clinical events, highlighting the pathways in the patient's diagnostic and treatment journey. ER here refers to the emergency room.\\n\\n\"\n",
    "    \"Follow these instructions:\\n\"\n",
    "    \"{instruction_str}\\n\"\n",
    "    \"Query: {query_str}\\n\\n\"\n",
    "    \"Expression:\"\n",
    ")\n",
    "response_synthesis_prompt_str = (\n",
    "    \"Given an input question, synthesize a response from the query results.\\n\"\n",
    "    \"Query: {query_str}\\n\\n\"\n",
    "    \"Pandas Instructions (optional):\\n{pandas_instructions}\\n\\n\"\n",
    "    \"Pandas Output: {pandas_output}\\n\\n\"\n",
    "    \"Response: \"\n",
    ")\n",
    "\n",
    "pandas_prompt = PromptTemplate(pandas_prompt_str).partial_format(\n",
    "    instruction_str=instruction_str, df_str=df.head(5)\n",
    ")\n",
    "pandas_output_parser = PandasInstructionParser(df)\n",
    "response_synthesis_prompt = PromptTemplate(response_synthesis_prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define query pipeline with modules\n",
    "qp_table = QP(\n",
    "    modules={\n",
    "        \"input\": InputComponent(),\n",
    "        \"pandas_prompt\": pandas_prompt,\n",
    "        \"llm1\": llm,\n",
    "        \"pandas_output_parser\": pandas_output_parser,\n",
    "        \"response_synthesis_prompt\": response_synthesis_prompt,\n",
    "        \"llm2\": llm,\n",
    "    },\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "qp_table.add_chain([\"input\", \"pandas_prompt\", \"llm1\", \"pandas_output_parser\"])\n",
    "qp_table.add_links(\n",
    "    [\n",
    "        Link(\"input\", \"response_synthesis_prompt\", dest_key=\"query_str\"),\n",
    "        Link(\n",
    "            \"llm1\", \"response_synthesis_prompt\", dest_key=\"pandas_instructions\"\n",
    "        ),\n",
    "        Link(\n",
    "            \"pandas_output_parser\",\n",
    "            \"response_synthesis_prompt\",\n",
    "            dest_key=\"pandas_output\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "# add link from response synthesis prompt to llm2\n",
    "qp_table.add_link(\"response_synthesis_prompt\", \"llm2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qp_table.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"qp_table.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x24c7bcbe740>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create diagram of the query pipeline\n",
    "net.from_nx(qp_table.clean_dag)\n",
    "net.show(\"qp_table.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
      "query_str: how many positive cases?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_prompt with input: \n",
      "query_str: how many positive cases?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm1 with input: \n",
      "messages: You are working with a pandas dataframe in Python.\n",
      "The name of the dataframe is `df`.\n",
      "This is the result of `print(df.head())`:\n",
      "  case_id  Target  age  max_activity_count  duration_since_reg  crp  \\\n",
      "0...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_output_parser with input: \n",
      "input: assistant: df[df['Target'] == 1].shape[0]\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module response_synthesis_prompt with input: \n",
      "query_str: how many positive cases?\n",
      "pandas_instructions: assistant: df[df['Target'] == 1].shape[0]\n",
      "pandas_output: 98\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm2 with input: \n",
      "messages: Given an input question, synthesize a response from the query results.\n",
      "Query: how many positive cases?\n",
      "\n",
      "Pandas Instructions (optional):\n",
      "df[df['Target'] == 1].shape[0]\n",
      "\n",
      "Pandas Output: 98\n",
      "\n",
      "Response: \n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# run query\n",
    "response = qp_table.run(\n",
    "    query_str=\"how many positive cases?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the data provided, there are 98 positive cases.\n"
     ]
    }
   ],
   "source": [
    "# query result\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
      "query_str: what is the average time patients spend in the hospital?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_prompt with input: \n",
      "query_str: what is the average time patients spend in the hospital?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm1 with input: \n",
      "messages: You are working with a pandas dataframe in Python.\n",
      "The name of the dataframe is `df`.\n",
      "This is the result of `print(df.head())`:\n",
      "  case_id  Target  age  max_activity_count  duration_since_reg  crp  \\\n",
      "0...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_output_parser with input: \n",
      "input: assistant: df['duration_since_reg'].mean()\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module response_synthesis_prompt with input: \n",
      "query_str: what is the average time patients spend in the hospital?\n",
      "pandas_instructions: assistant: df['duration_since_reg'].mean()\n",
      "pandas_output: 11.568710217755443\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm2 with input: \n",
      "messages: Given an input question, synthesize a response from the query results.\n",
      "Query: what is the average time patients spend in the hospital?\n",
      "\n",
      "Pandas Instructions (optional):\n",
      "df['duration_since_reg'].mean()\n",
      "...\n",
      "\n",
      "\u001b[0mfinal response: ================================================================================================================================================================================================\n",
      "Based on the data provided, the average time patients spend in the hospital is approximately 11.57 days.\n"
     ]
    }
   ],
   "source": [
    "# run query\n",
    "response = qp_table.run(\n",
    "    query_str=\"what is the average time patients spend in the hospital?\",\n",
    ")\n",
    "# query result\n",
    "print(\"final response: ================================================================================================================================================================================================\")\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Agent Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR [asyncio] Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-5' coro=<BulkInserter._bulk_insert() running at d:\\conda_envs\\rag\\lib\\site-packages\\phoenix\\db\\bulk_inserter.py:103> wait_for=<Future pending cb=[Task.__wakeup()]>>\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "def evaluate_model(model_name:str) -> dict:\n",
    "    \"\"\"Load the trained model, evaluation data and evaluate the loaded model.\"\"\"\n",
    "    from sklearn.metrics import roc_auc_score, roc_auc_score, average_precision_score, confusion_matrix, f1_score, matthews_corrcoef\n",
    "    from joblib import dump, load\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    model_save_path = 'models'\n",
    "    model = load(f'./{model_save_path}/{model_name}.joblib')\n",
    "    X_test = pd.read_csv('./data_python/Sepsis_X_test.csv')\n",
    "    y_test = pd.read_csv('./data_python/Sepsis_y_test.csv')\n",
    "\n",
    "    pred_prob = model.predict_proba(X_test) # get the prediction probabilities for the test set\n",
    "    predictions = model.predict(X_test) # get the predictions for the test set\n",
    "\n",
    "    roc_auc = roc_auc_score(y_test, pred_prob[:,1]) # calculate the roc auc score\n",
    "    average_precision = average_precision_score(y_test, pred_prob[:,1]) # calculate the\n",
    "    mcc =  matthews_corrcoef(y_test, predictions)\n",
    "    f1_macro = f1_score(y_test, predictions, average='macro')\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "    return {\"roc_auc\":roc_auc, \"average_precision\":average_precision, \"mcc\":mcc, \"f1_macro\":f1_macro, \"confusion_matrix\":cm}\n",
    "\n",
    "# create tools\n",
    "evaluate_model_tool = FunctionTool.from_defaults(name=\"evaluate_model\", fn=evaluate_model)\n",
    "tools = [evaluate_model_tool]\n",
    "\n",
    "# setup ReAct agent\n",
    "# model_agent_prompt = \"\"\"You are a proficient python developer. Respond with the syntactically correct code for the question below. Make sure you follow these rules:\n",
    "#                                         1. Use context to understand the APIs and how to use them.\n",
    "#                                         2. Ensure all the requirements in the question are met.\n",
    "#                                         3. Ensure the output code syntax is correct.\n",
    "#                                         4. All required dependencies should be imported above the code.\n",
    "#                                         Question:\n",
    "#                                         {question}\n",
    "#                                         Context:\n",
    "#                                         {context}\n",
    "#                                         Helpful Response:\"\"\"\n",
    "# model_agent_prompt = PromptTemplate(model_agent_prompt)\n",
    "agent_model = ReActAgent.from_tools(tools=tools, \n",
    "                                     llm=llm,\n",
    "                                     verbose=True)\n",
    "# agent_model.update_prompts({\"agent_worker:system_prompt\": model_agent_prompt})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Agent run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use the 'evaluate_model' tool to help me answer the question.\n",
      "Action: evaluate_model\n",
      "Action Input: {'model_name': 'XGBoost'}\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda_envs\\rag\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:06:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\data\\../common/error_msg.h:80: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mObservation: {'roc_auc': 0.8659217877094972, 'average_precision': 0.7481294952146433, 'mcc': 0.7002038117109098, 'f1_macro': 0.8472989564149784, 'confusion_matrix': array([[176,   3],\n",
      "       [  7,  13]], dtype=int64)}\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: The current language of the user is: English. I have the information needed to answer the question without using any more tools.\n",
      "Answer: The AUC-ROC score of the XGBoost model is 0.8659217877094972.\n",
      "\u001b[0mThe AUC-ROC score of the XGBoost model is 0.8659217877094972.\n"
     ]
    }
   ],
   "source": [
    "# response = await agent.achat(\"what is the auc_roc score of the trained model?\")\n",
    "response = agent_model.query(\"what is the auc_roc score of the XGBoost model?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Agent Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods for running the agents / query pipelines\n",
    "def run_agent(query: str) -> str:\n",
    "    \"\"\"Run the agent model on the query to get evaluation results from trained model.\"\"\"\n",
    "    response = agent_model.query(query)\n",
    "    return str(response)\n",
    "\n",
    "def run_query_pipeline(query: str) -> str:\n",
    "    \"\"\"Run the query pipeline to analyze dataset for the given query.\"\"\"\n",
    "    response = qp_table.run(\n",
    "        query_str=query,\n",
    "    )\n",
    "    return str(response.message.content)\n",
    "\n",
    "# create tools\n",
    "run_agent_tool = FunctionTool.from_defaults(name=\"run_agent\", fn=run_agent)\n",
    "run_query_pipeline_tool = FunctionTool.from_defaults(name=\"run_query_pipeline\", fn=run_query_pipeline)\n",
    "agent_tools = [run_agent_tool, run_query_pipeline_tool]\n",
    "\n",
    "top_level_agent_prompt = \"\"\"\n",
    "                You are designed to help with a variety of tasks, from answering questions \\\n",
    "                to providing summaries to other types of analyses.\n",
    "\n",
    "                ## Tools\n",
    "                You have access to a wide variety of tools. You are responsible for using\n",
    "                the tools in any sequence you deem appropriate to complete the task at hand.\n",
    "                This may require breaking the task into subtasks and using different tools\n",
    "                to complete each subtask.\n",
    "\n",
    "                You have access to the following tools:\n",
    "                {tool_desc}\n",
    "\n",
    "                ## Output Format\n",
    "                To answer the question, please use the following format.\n",
    "\n",
    "                ```\n",
    "                Thought: I need to use a tool to help me answer the question.\n",
    "                Action: tool name (one of {tool_names}) if using a tool.\n",
    "                Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"input\": \"hello world\", \"num_beams\": 5}})\n",
    "                ```\n",
    "\n",
    "                Please ALWAYS start with a Thought.\n",
    "\n",
    "                Please use a valid JSON format for the Action Input. Do NOT do this {{'input': 'hello world', 'num_beams': 5}}.\n",
    "\n",
    "                If this format is used, the user will respond in the following format:\n",
    "\n",
    "                ```\n",
    "                Observation: tool response\n",
    "                ```\n",
    "\n",
    "                You should keep repeating the above format until you have enough information\n",
    "                to answer the question without using any more tools. At that point, you MUST respond\n",
    "                in the one of the following two formats:\n",
    "\n",
    "                ```\n",
    "                Thought: I can answer without using any more tools.\n",
    "                Answer: [your answer here]\n",
    "                ```\n",
    "\n",
    "                ```\n",
    "                Thought: I cannot answer the question with the provided tools.\n",
    "                Answer: Sorry, I cannot answer your query.\n",
    "                ```\n",
    "\n",
    "                ## Additional Rules\n",
    "                - You MUST obey the function signature of each tool. Do NOT pass in no arguments if the function expects arguments.\n",
    "                - For queries that clearly involve data retrieval or manipulation (like 'analyze sales data', 'show trends in data'), use 'run_query_pipeline'.\n",
    "                - For queries that directly relate to model performance or evaluation (like 'what is the AUC_ROC score', 'evaluate the prediction accuracy'), use 'run_agent'.\n",
    "\n",
    "                ## Current Conversation\n",
    "                Below is the current conversation consisting of interleaving human and assistant messages.\n",
    "                \"\"\"\n",
    "top_level_agent_prompt = PromptTemplate(top_level_agent_prompt)\n",
    "agent = ReActAgent.from_tools(tools=agent_tools, \n",
    "                                    llm=llm, \n",
    "                                    verbose=True)\n",
    "agent.update_prompts({\"agent_worker:system_prompt\": top_level_agent_prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: The user is asking for a specific count of positive cases, which involves data retrieval. I should use the 'run_query_pipeline' tool to answer this question.\n",
      "Action: run_query_pipeline\n",
      "Action Input: {'query': 'how many positive cases'}\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
      "query_str: how many positive cases\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_prompt with input: \n",
      "query_str: how many positive cases\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm1 with input: \n",
      "messages: You are working with a pandas dataframe in Python.\n",
      "The name of the dataframe is `df`.\n",
      "This is the result of `print(df.head())`:\n",
      "  case_id  Target  age  max_activity_count  duration_since_reg  crp  \\\n",
      "0...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_output_parser with input: \n",
      "input: assistant: df['Target'].sum()\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module response_synthesis_prompt with input: \n",
      "query_str: how many positive cases\n",
      "pandas_instructions: assistant: df['Target'].sum()\n",
      "pandas_output: 98\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm2 with input: \n",
      "messages: Given an input question, synthesize a response from the query results.\n",
      "Query: how many positive cases\n",
      "\n",
      "Pandas Instructions (optional):\n",
      "df['Target'].sum()\n",
      "\n",
      "Pandas Output: 98\n",
      "\n",
      "Response: \n",
      "\n",
      "\u001b[0m\u001b[1;3;34mObservation: Based on the data provided, there are 98 positive cases.\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: I have the answer to the user's question, so I can respond without using any more tools.\n",
      "Answer: There are 98 positive cases.\n",
      "\u001b[0mThere are 98 positive cases.\n"
     ]
    }
   ],
   "source": [
    "# response = await agent.achat(\"what is the auc_roc score of the trained model?\")\n",
    "# response = agent.query(\"what is the auc_roc score of the trained XGBoost model?\")\n",
    "response = agent.query(\"how many positive cases?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: agent_worker:system_prompt\n",
      "\n",
      "Value: \n",
      "                You are designed to help with a variety of tasks, from answering questions                 to providing summaries to other types of analyses.\n",
      "\n",
      "                ## Tools\n",
      "                You have access to a wide variety of tools. You are responsible for using\n",
      "                the tools in any sequence you deem appropriate to complete the task at hand.\n",
      "                This may require breaking the task into subtasks and using different tools\n",
      "                to complete each subtask.\n",
      "\n",
      "                You have access to the following tools:\n",
      "                {tool_desc}\n",
      "\n",
      "                ## Output Format\n",
      "                To answer the question, please use the following format.\n",
      "\n",
      "                ```\n",
      "                Thought: I need to use a tool to help me answer the question.\n",
      "                Action: tool name (one of {tool_names}) if using a tool.\n",
      "                Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"input\": \"hello world\", \"num_beams\": 5}})\n",
      "                ```\n",
      "\n",
      "                Please ALWAYS start with a Thought.\n",
      "\n",
      "                Please use a valid JSON format for the Action Input. Do NOT do this {{'input': 'hello world', 'num_beams': 5}}.\n",
      "\n",
      "                If this format is used, the user will respond in the following format:\n",
      "\n",
      "                ```\n",
      "                Observation: tool response\n",
      "                ```\n",
      "\n",
      "                You should keep repeating the above format until you have enough information\n",
      "                to answer the question without using any more tools. At that point, you MUST respond\n",
      "                in the one of the following two formats:\n",
      "\n",
      "                ```\n",
      "                Thought: I can answer without using any more tools.\n",
      "                Answer: [your answer here]\n",
      "                ```\n",
      "\n",
      "                ```\n",
      "                Thought: I cannot answer the question with the provided tools.\n",
      "                Answer: Sorry, I cannot answer your query.\n",
      "                ```\n",
      "\n",
      "                ## Additional Rules\n",
      "                - You MUST obey the function signature of each tool. Do NOT pass in no arguments if the function expects arguments.\n",
      "                - For queries that clearly involve data retrieval or manipulation (like 'analyze sales data', 'show trends in data'), use 'run_query_pipeline'.\n",
      "                - For queries that directly relate to model performance or evaluation (like 'what is the AUC_ROC score', 'evaluate the prediction accuracy'), use 'run_agent'.\n",
      "\n",
      "                ## Current Conversation\n",
      "                Below is the current conversation consisting of interleaving human and assistant messages.\n",
      "                \n"
     ]
    }
   ],
   "source": [
    "prompt_dict = agent.get_prompts()\n",
    "for k, v in prompt_dict.items():\n",
    "    print(f\"Prompt: {k}\\n\\nValue: {v.template}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Agent run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: The user is asking for a count of positive cases, which involves data retrieval. I should use the 'run_query_pipeline' tool to answer this question.\n",
      "Action: run_query_pipeline\n",
      "Action Input: {'query': 'count positive cases'}\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
      "query_str: count positive cases\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_prompt with input: \n",
      "query_str: count positive cases\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm1 with input: \n",
      "messages: You are working with a pandas dataframe in Python.\n",
      "The name of the dataframe is `df`.\n",
      "This is the result of `print(df.head())`:\n",
      "  case_id  Target  age  max_activity_count  duration_since_reg  crp  \\\n",
      "0...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_output_parser with input: \n",
      "input: assistant: df['Target'].sum()\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module response_synthesis_prompt with input: \n",
      "query_str: count positive cases\n",
      "pandas_instructions: assistant: df['Target'].sum()\n",
      "pandas_output: 98\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm2 with input: \n",
      "messages: Given an input question, synthesize a response from the query results.\n",
      "Query: count positive cases\n",
      "\n",
      "Pandas Instructions (optional):\n",
      "df['Target'].sum()\n",
      "\n",
      "Pandas Output: 98\n",
      "\n",
      "Response: \n",
      "\n",
      "\u001b[0m\u001b[1;3;34mObservation: Based on the data provided, the total count of positive cases is 98.\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: I have the answer to the question, so I can provide it without using any more tools.\n",
      "Answer: There are 98 positive cases according to the data provided.\n",
      "\u001b[0m===========================final response============================\n",
      "There are 98 positive cases according to the data provided.\n"
     ]
    }
   ],
   "source": [
    "# response = await agent.achat(\"what is the auc_roc score of the trained model?\")\n",
    "response = agent.query(\"how many positive cases?\")\n",
    "print(f'===========================final response============================\\n{str(response)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: This question involves model performance evaluation, specifically asking for the AUC_ROC score of the trained XGBoost model. I should use the 'run_agent' tool to answer this.\n",
      "Action: run_agent\n",
      "Action Input: {'query': 'auc_roc_score_xgboost'}\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: The current language of the user is: English. The user is asking about the AUC ROC score for the XGBoost model. I need to use the 'evaluate_model' tool to help me answer this question.\n",
      "Action: evaluate_model\n",
      "Action Input: {'model_name': 'xgboost'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: {'roc_auc': 0.8659217877094972, 'average_precision': 0.7481294952146433, 'mcc': 0.7002038117109098, 'f1_macro': 0.8472989564149784, 'confusion_matrix': array([[176,   3],\n",
      "       [  7,  13]], dtype=int64)}\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: The current language of the user is: English. I have the results of the XGBoost model evaluation. The AUC ROC score is 0.8659217877094972.\n",
      "Answer: The AUC ROC score for the XGBoost model is 0.8659217877094972.\n",
      "\u001b[0m\u001b[1;3;34mObservation: The AUC ROC score for the XGBoost model is 0.8659217877094972.\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: I have received the AUC_ROC score for the XGBoost model from the 'run_agent' tool.\n",
      "Answer: The AUC_ROC score for the XGBoost model is 0.8659217877094972.\n",
      "\u001b[0m===========================final response============================\n",
      "The AUC_ROC score for the XGBoost model is 0.8659217877094972.\n"
     ]
    }
   ],
   "source": [
    "# response = await agent.achat(\"what is the auc_roc score of the trained model?\")\n",
    "response = agent.query(\"what is the auc_roc score of the trained XGBoost model?\")\n",
    "print(f'===========================final response============================\\n{str(response)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: This question involves data retrieval and manipulation, specifically calculating the average time patients spend in the hospital. I should use the 'run_query_pipeline' tool to analyze the dataset and find this information.\n",
      "Action: run_query_pipeline\n",
      "Action Input: {'query': 'average time patients spend in the hospital'}\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
      "query_str: average time patients spend in the hospital\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_prompt with input: \n",
      "query_str: average time patients spend in the hospital\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm1 with input: \n",
      "messages: You are working with a pandas dataframe in Python.\n",
      "The name of the dataframe is `df`.\n",
      "This is the result of `print(df.head())`:\n",
      "  case_id  Target  age  max_activity_count  duration_since_reg  crp  \\\n",
      "0...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_output_parser with input: \n",
      "input: assistant: df['duration_since_reg'].mean()\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module response_synthesis_prompt with input: \n",
      "query_str: average time patients spend in the hospital\n",
      "pandas_instructions: assistant: df['duration_since_reg'].mean()\n",
      "pandas_output: 11.568710217755443\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm2 with input: \n",
      "messages: Given an input question, synthesize a response from the query results.\n",
      "Query: average time patients spend in the hospital\n",
      "\n",
      "Pandas Instructions (optional):\n",
      "df['duration_since_reg'].mean()\n",
      "\n",
      "Pandas Outpu...\n",
      "\n",
      "\u001b[0m\u001b[1;3;34mObservation: Based on the data provided, the average time patients spend in the hospital is approximately 11.57 days.\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: I have received the result from the 'run_query_pipeline' tool, which calculated the average time patients spend in the hospital. Now I can answer the question without using any more tools.\n",
      "Answer: The average time patients spend in the hospital is approximately 11.57 days.\n",
      "\u001b[0m===========================final response============================\n",
      "The average time patients spend in the hospital is approximately 11.57 days.\n"
     ]
    }
   ],
   "source": [
    "# response = await agent.achat(\"what is the auc_roc score of the trained model?\")\n",
    "response = agent.query(\"what is the average time patients spend in the hospital?\")\n",
    "print(f'===========================final response============================\\n{str(response)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
