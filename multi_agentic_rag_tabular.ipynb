{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda_envs\\rag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List,Optional, Dict, Any, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# llama index imports\n",
    "import llama_index.core\n",
    "from llama_index.core import SimpleDirectoryReader,VectorStoreIndex,SummaryIndex, StorageContext, Settings, load_index_from_storage, Response\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.node_parser import SentenceSplitter, CodeSplitter, LangchainNodeParser\n",
    "from llama_index.core.tools import FunctionTool,QueryEngineTool\n",
    "from llama_index.core.vector_stores import MetadataFilters,FilterCondition\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "from llama_index.readers.file import IPYNBReader, PandasCSVReader\n",
    "from llama_index.experimental.query_engine import PandasQueryEngine\n",
    "from llama_index.experimental.query_engine.pandas import (\n",
    "    PandasInstructionParser,\n",
    ")\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.query_pipeline import (\n",
    "    QueryPipeline as QP,\n",
    "    Link,\n",
    "    InputComponent,\n",
    "    AgentFnComponent,\n",
    "    AgentInputComponent,\n",
    "    CustomQueryComponent,\n",
    "    FnComponent,\n",
    "    RouterComponent,\n",
    ")\n",
    "from llama_index.experimental.query_engine import PandasQueryEngine\n",
    "from llama_index.readers.file import IPYNBReader, PandasCSVReader\n",
    "from llama_index.experimental.query_engine import PandasQueryEngine\n",
    "from llama_index.experimental.query_engine.pandas import (\n",
    "    PandasInstructionParser,\n",
    ")\n",
    "\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "\n",
    "# llama index agent imports\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker, ReActAgent, Task, AgentChatResponse, AgentRunner, QueryPipelineAgentWorker\n",
    "\n",
    "# llama index llms and embeddings imports\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# custom package imports\n",
    "from llama_index.packs.tables.chain_of_table.base import ChainOfTableQueryEngine, serialize_table\n",
    "\n",
    "# langchain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "# uncertainty imports\n",
    "from crepes import WrapClassifier, ConformalClassifier\n",
    "from venn_abers import VennAbersCalibrator, VennAbers\n",
    "\n",
    "# tools\n",
    "import nest_asyncio # to allow running async functions in jupyter\n",
    "import chromadb # persistent storage for vectors\n",
    "# import nbconvert\n",
    "import tree_sitter\n",
    "import tree_sitter_languages\n",
    "import phoenix as px\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [phoenix.session.session] Existing running Phoenix instance detected! Shutting it down and starting a new instance...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌍 To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "📖 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "temperture = 0.0 #for deterministic results\n",
    "\n",
    "llm_model = \"mistral-large-latest\"\n",
    "# llm_model = \"codestral-latest\"\n",
    "MISTRAL_API_KEY =  \"BWdlihu9sUh5P2g3bHnzjAaHiT4anTVH\"\n",
    "os.environ[\"MISTRAL_API_KEY\"] = MISTRAL_API_KEY\n",
    "llm = MistralAI(model=llm_model, temperature=temperture)\n",
    "\n",
    "# llm_model = \"gpt-3.5-turbo\"\n",
    "# # llm_model = \"gpt-4\"\n",
    "# OPENAI_API_KEY =  \"something\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "# llm = OpenAI(model=llm_model, temperature=temperture)\n",
    "\n",
    "# llm_model = \"codellama\"\n",
    "# llm = Ollama(model=llm_model, request_timeout=1200.0, base_url=\"http://localhost:11434\", temperature=temperture)\n",
    "\n",
    "nest_asyncio.apply() # to allow running async functions in jupyter\n",
    "px.launch_app()\n",
    "llama_index.core.set_global_handler(\"arize_phoenix\")\n",
    "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading file\n",
    "file_path = \"./data_csv/Sepsis_Processed_IC.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Pipeline Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_str = (\n",
    "    \"1. Convert the query to executable Python code using Pandas.\\n\"\n",
    "    \"2. The final line of code should be a Python expression that can be called with the `eval()` function.\\n\"\n",
    "    \"3. The code should represent a solution to the query.\\n\"\n",
    "    \"4. PRINT ONLY THE EXPRESSION.\\n\"\n",
    "    \"5. Do not quote the expression.\\n\"\n",
    "    \"6. Add axis labels, legend, and title when creating a plot.\\n\"\n",
    ")\n",
    "\n",
    "pandas_prompt_str = (\n",
    "    \"You are working with a pandas dataframe in Python.\\n\"\n",
    "    \"The name of the dataframe is `df`. You should interpret the columns of the dataframe as follows: \\n\"\n",
    "    \"1) Each row represents patient data related to sepsis diagnosis.\\n\"\n",
    "    \"2) The Target column indicates whether the patient had sepsis.\\n\"\n",
    "    \"3) The duration_since_reg column describes the patient's stay after admission in days.\\n\"\n",
    "    \"4) Diagnosis-related columns detail specific diagnostic results and associated codes.\\n\"\n",
    "    \"5) The dataset includes patient demographics age, clinical measurements (crp, lacticacid, leucocytes), and diagnostic procedures (diagnosticartastrup, diagnosticblood, etc.).\\n\"\n",
    "    \"6) The dataframe also records clinical criteria for sepsis (sirscritheartrate, sirscritleucos, etc.), resource usage, and event transitions (e.g., CRP => ER Triage).\\n\"\n",
    "    \"7) Additional columns capture organ dysfunction, hypotension, hypoxia, suspected infection, and treatment details like infusions and oliguria.\\n\"\n",
    "    \"8) The dataset covers the transitions between various clinical events, highlighting the pathways in the patient's diagnostic and treatment journey.\\n\"\n",
    "    \"9) ER here refers to the emergency room.\\n\"\n",
    "    \"10) You only answer questions related to the dataframe.\\n\"\n",
    "    \"11) If you do not know the answer, then say you do not know.\\n\\n\"\n",
    "\n",
    "    \"This is the result of `print(df.head())`:\\n\"\n",
    "    \"{df_str}\\n\\n\"\n",
    "    \n",
    "    \"Follow these instructions:\\n\"\n",
    "    \"{instruction_str}\\n\"\n",
    "    \"Query: {query_str}\\n\\n\"\n",
    "    \"Expression:\"\n",
    ")\n",
    "response_synthesis_prompt_str = (\n",
    "    \"Given an input question, synthesize a response from the query results.\\n\"\n",
    "    \"Query: {query_str}\\n\\n\"\n",
    "    \"Pandas Instructions (optional):\\n{pandas_instructions}\\n\\n\"\n",
    "    \"Pandas Output: {pandas_output}\\n\\n\"\n",
    "    \"Response: \"\n",
    ")\n",
    "\n",
    "code_execution = (\n",
    "    \"Execute the code to generate the output and include explanation.\\n\"\n",
    "    \"Query: {query_str}\\n\\n\"\n",
    "    \"Pandas Output: {pandas_output}\\n\\n\"\n",
    "    \"Response: \"\n",
    ")\n",
    "\n",
    "\n",
    "class CustomPandasInstructionParser(PandasInstructionParser):\n",
    "    def parse(self, text):\n",
    "        # Prepend the necessary import statements\n",
    "        imports = \"import pandas as pd\\nimport matplotlib.pyplot as plt\\n\"\n",
    "        # Combine the imports with the code generated by the model\n",
    "        code = imports + text\n",
    "        return code\n",
    "\n",
    "\n",
    "pandas_prompt = PromptTemplate(pandas_prompt_str).partial_format(\n",
    "    instruction_str=instruction_str, df_str=df.head(5)\n",
    ")\n",
    "\n",
    "pandas_output_parser = PandasInstructionParser(df)\n",
    "# pandas_output_parser = CustomPandasInstructionParser(df)\n",
    "response_synthesis_prompt = PromptTemplate(response_synthesis_prompt_str)\n",
    "code_execution_prompt = PromptTemplate(code_execution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define query pipeline with modules\n",
    "qp_table = QP(\n",
    "    modules={\n",
    "        \"input\": InputComponent(),\n",
    "        \"pandas_prompt\": pandas_prompt,\n",
    "        \"llm1\": llm,\n",
    "        \"pandas_output_parser\": pandas_output_parser,\n",
    "        \"response_synthesis_prompt\": response_synthesis_prompt,\n",
    "        \"llm2\": llm,\n",
    "        \"code_execution\": code_execution_prompt,\n",
    "        \"llm3\": llm,\n",
    "    },\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "qp_table.add_chain([\"input\", \"pandas_prompt\", \"llm1\", \"pandas_output_parser\"])\n",
    "qp_table.add_links(\n",
    "    [\n",
    "        Link(\"input\", \"response_synthesis_prompt\", dest_key=\"query_str\"),\n",
    "        Link(\n",
    "            \"llm1\", \"response_synthesis_prompt\", dest_key=\"pandas_instructions\"\n",
    "        ),\n",
    "        Link(\n",
    "            \"pandas_output_parser\",\n",
    "            \"response_synthesis_prompt\",\n",
    "            dest_key=\"pandas_output\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "qp_table.add_link(\"response_synthesis_prompt\", \"llm2\")\n",
    "\n",
    "qp_table.add_links(\n",
    "    [\n",
    "        Link(\"input\", \"code_execution\", dest_key=\"query_str\"),\n",
    "        Link(\n",
    "            \"pandas_output_parser\",\n",
    "            \"code_execution\",\n",
    "            dest_key=\"pandas_output\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "qp_table.add_link(\"code_execution\", \"llm3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qp_table.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"qp_table.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x24c7bcbe740>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create diagram of the query pipeline\n",
    "net.from_nx(qp_table.clean_dag)\n",
    "net.show(\"qp_table.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
      "query_str: how many positive cases?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_prompt with input: \n",
      "query_str: how many positive cases?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm1 with input: \n",
      "messages: You are working with a pandas dataframe in Python.\n",
      "The name of the dataframe is `df`. You should interpret the columns of the dataframe as follows: \n",
      " 1) Each row represents patient data related to sep...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_output_parser with input: \n",
      "input: assistant: df['Target'].sum()\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module response_synthesis_prompt with input: \n",
      "query_str: how many positive cases?\n",
      "pandas_instructions: assistant: df['Target'].sum()\n",
      "pandas_output: 98\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm2 with input: \n",
      "messages: Given an input question, synthesize a response from the query results.\n",
      "Query: how many positive cases?\n",
      "\n",
      "Pandas Instructions (optional):\n",
      "df['Target'].sum()\n",
      "\n",
      "Pandas Output: 98\n",
      "\n",
      "Response: \n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# run query\n",
    "response = qp_table.run(\n",
    "    query_str=\"how many positive cases?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the data provided, there are 98 positive cases.\n"
     ]
    }
   ],
   "source": [
    "# query result\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
      "query_str: what is the average time patients spend in the hospital?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_prompt with input: \n",
      "query_str: what is the average time patients spend in the hospital?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm1 with input: \n",
      "messages: You are working with a pandas dataframe in Python.\n",
      "The name of the dataframe is `df`.\n",
      "This is the result of `print(df.head())`:\n",
      "  case_id  Target  age  max_activity_count  duration_since_reg  crp  \\\n",
      "0...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_output_parser with input: \n",
      "input: assistant: df['duration_since_reg'].mean()\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module response_synthesis_prompt with input: \n",
      "query_str: what is the average time patients spend in the hospital?\n",
      "pandas_instructions: assistant: df['duration_since_reg'].mean()\n",
      "pandas_output: 11.568710217755443\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm2 with input: \n",
      "messages: Given an input question, synthesize a response from the query results.\n",
      "Query: what is the average time patients spend in the hospital?\n",
      "\n",
      "Pandas Instructions (optional):\n",
      "df['duration_since_reg'].mean()\n",
      "...\n",
      "\n",
      "\u001b[0mfinal response: ================================================================================================================================================================================================\n",
      "Based on the data provided, the average time patients spend in the hospital is approximately 11.57 days.\n"
     ]
    }
   ],
   "source": [
    "# run query\n",
    "response = qp_table.run(\n",
    "    query_str=\"what is the average time patients spend in the hospital?\",\n",
    ")\n",
    "# query result\n",
    "print(\"final response: ================================================================================================================================================================================================\")\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
      "query_str: visualize the age distribution?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_prompt with input: \n",
      "query_str: visualize the age distribution?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm1 with input: \n",
      "messages: You are working with a pandas dataframe in Python.\n",
      "The name of the dataframe is `df`. You should interpret the columns of the dataframe as follows: \n",
      "1) Each row represents patient data related to seps...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_output_parser with input: \n",
      "input: assistant: df['age'].plot(kind='hist', bins=20, color='skyblue', edgecolor='black', title='Age Distribution', xlabel='Age', ylabel='Frequency')\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module response_synthesis_prompt with input: \n",
      "query_str: visualize the age distribution?\n",
      "pandas_instructions: assistant: df['age'].plot(kind='hist', bins=20, color='skyblue', edgecolor='black', title='Age Distribution', xlabel='Age', ylabel='Frequency')\n",
      "pandas_output: Axes(0.125,0.11;0.775x0.77)\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module code_execution with input: \n",
      "query_str: visualize the age distribution?\n",
      "pandas_output: Axes(0.125,0.11;0.775x0.77)\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm2 with input: \n",
      "messages: Given an input question, synthesize a response from the query results.\n",
      "Query: visualize the age distribution?\n",
      "\n",
      "Pandas Instructions (optional):\n",
      "df['age'].plot(kind='hist', bins=20, color='skyblue', edg...\n",
      "\n",
      "\u001b[0mfinal response: ================================================================================================================================================================================================\n",
      "The age distribution can be visualized using a histogram plot with 20 bins. The plot will show the frequency of different age groups, allowing for a clear understanding of the distribution.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run query\n",
    "response = qp_table.run(\n",
    "    query_str=\"visualize the age distribution?\",\n",
    ")\n",
    "# query result\n",
    "print(\"final response: ================================================================================================================================================================================================\")\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Agent Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "def evaluate_model(model_name:str, threshold:int = -1) -> dict:\n",
    "    \"\"\"Load the trained model, evaluation data and evaluate the loaded model.\"\"\"\n",
    "    from sklearn.metrics import roc_auc_score, roc_auc_score, average_precision_score, confusion_matrix, f1_score, matthews_corrcoef\n",
    "    from joblib import load\n",
    "    \n",
    "    model_save_path = 'models'\n",
    "    model = load(f'./{model_save_path}/{model_name}.joblib')\n",
    "    X_test = pd.read_csv('./data_python/Sepsis_X_test.csv')\n",
    "    y_test = pd.read_csv('./data_python/Sepsis_y_test.csv')\n",
    "\n",
    "    pred_prob = model.predict_proba(X_test) # get the prediction probabilities for the test set\n",
    "    predictions = model.predict(X_test) # get the predictions for the test set\n",
    "\n",
    "    roc_auc = roc_auc_score(y_test, pred_prob[:,1]) # calculate the roc auc score\n",
    "    average_precision = average_precision_score(y_test, pred_prob[:,1]) # calculate the\n",
    "    best_threshold = model.best_threshold_\n",
    "\n",
    "    if threshold > 0:\n",
    "        predictions = np.where(pred_prob[:,1] > threshold, 1, 0)\n",
    "        mcc = matthews_corrcoef(y_test, predictions)\n",
    "        f1_macro = f1_score(y_test, predictions, average='macro')\n",
    "        cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "    else:\n",
    "        mcc =  matthews_corrcoef(y_test, predictions)\n",
    "        f1_macro = f1_score(y_test, predictions, average='macro')\n",
    "        cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "    return {\"roc_auc\":roc_auc, \"average_precision\":average_precision, \"mcc\":mcc, \"f1_macro\":f1_macro, \"confusion_matrix\":cm, \"best_threshold\":best_threshold}\n",
    "\n",
    "# uncertainty quantification\n",
    "def conformal_prediction(model_name:str, alpha:int = -1) -> dict:\n",
    "    \"\"\"Load the trained model, do uncertainty quantification on the loaded model and return the coverage and average width of the prediction sets.\"\"\"\n",
    "    from joblib import load\n",
    "    from crepes import WrapClassifier\n",
    "    \n",
    "    \"\"\" loading the model and data\"\"\"\n",
    "    model_save_path = 'models'\n",
    "    model = load(f'./{model_save_path}/{model_name}.joblib')\n",
    "    X_cal = pd.read_csv('./data_python/Sepsis_X_cal.csv')\n",
    "    y_cal = pd.read_csv('./data_python/Sepsis_y_cal.csv').to_numpy().reshape(-1)\n",
    "    X_test = pd.read_csv('./data_python/Sepsis_X_test.csv')\n",
    "    y_test = pd.read_csv('./data_python/Sepsis_y_test.csv').to_numpy().reshape(-1)\n",
    "\n",
    "    \"\"\"calibrating the model\"\"\"\n",
    "    wrapped_clf = WrapClassifier(model) \n",
    "    wrapped_clf.calibrate(X_cal, y_cal)\n",
    "    \n",
    "    \"\"\" uncertainty quantification - coverage and average width of the prediction sets\"\"\"\n",
    "    if alpha > 0:\n",
    "        prediction_sets = wrapped_clf.predict_set(X_test, confidence=(1-alpha))\n",
    "        coverage = np.mean([prediction_sets[i][y_test[i]] for i in range(len(prediction_sets))])\n",
    "        widths = [np.sum(pred) for pred in prediction_sets] \n",
    "        average_width = np.mean(widths)\n",
    "    else:\n",
    "        alpha = 0.1\n",
    "        prediction_sets = wrapped_clf.predict_set(X_test, confidence=(1-alpha))\n",
    "        coverage = np.mean([prediction_sets[i][y_test[i]] for i in range(len(prediction_sets))])\n",
    "        widths = [np.sum(pred) for pred in prediction_sets] \n",
    "        average_width = np.mean(widths)\n",
    "    \n",
    "    return {\"coverage\":coverage, \"average_width\":average_width}\n",
    "\n",
    "# venn abers\n",
    "def venn_abers_calibration(model_name:str) -> dict:\n",
    "    \"\"\" Load the trained model, do uncertainty quantification using Venn-Abers calibration and generate the prediction intervals for the test set.\"\"\"\n",
    "\n",
    "    # load the trained model\n",
    "    from venn_abers import VennAbersCalibrator, VennAbers\n",
    "    from joblib import load\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    model_save_path = 'models'\n",
    "    plot_save_path = 'static/plots'\n",
    "\n",
    "    va = VennAbersCalibrator() # initialize the Venn-Abers calibrator\n",
    "\n",
    "    # load the model and data\n",
    "    model = load(f'./{model_save_path}/{model_name}.joblib')\n",
    "    X_cal = pd.read_csv('./data_python/Sepsis_X_cal.csv')\n",
    "    y_cal = pd.read_csv('./data_python/Sepsis_y_cal.csv').to_numpy().reshape(-1)\n",
    "    X_test = pd.read_csv('./data_python/Sepsis_X_test.csv')\n",
    "    y_test = pd.read_csv('./data_python/Sepsis_y_test.csv').to_numpy().reshape(-1)\n",
    "\n",
    "    # model results\n",
    "    prediction_prob_cal = model.predict_proba(X_cal)\n",
    "    prediction_prob_test = model.predict_proba(X_test)\n",
    "\n",
    "    # get calibrated prediction probabilities and predicted class labels\n",
    "    p_prime = va.predict_proba(p_cal=prediction_prob_cal, y_cal=y_cal, p_test=prediction_prob_test, p0_p1_output=True) # probability intervals for class 1\n",
    "    y_pred = np.argmax(va.predict(p_cal=prediction_prob_cal, y_cal=y_cal, p_test=prediction_prob_test), axis=1) # predicted class labels\n",
    "\n",
    "    # get the prediction probabilities and intervals for class 1\n",
    "    y_pred_interval_p1 = p_prime[1] # intervals for class 1\n",
    "    y_pred_p1 = p_prime[0][:, 1] # predicted probability of class 1\n",
    "\n",
    "    # create dataframe using the prediction probabilities and intervals for class 1\n",
    "    df = pd.DataFrame({'p0': y_pred_interval_p1[:,0], 'p1': y_pred_interval_p1[:,1], 'p of class_1': y_pred_p1, 'y': y_test})\n",
    "    display(df.head(10))\n",
    "\n",
    "\n",
    "    # sort the predictions based on the predicted probability of class 1\n",
    "    sorted_indices = np.argsort(y_pred_p1) # sort the predicted probabilities of class 1\n",
    "    y_pred_interval_p1 = y_pred_interval_p1[sorted_indices]\n",
    "    y_pred_p1 = y_pred_p1[sorted_indices]\n",
    "    y_test_sorted = y_test[sorted_indices]\n",
    "    y_pred_sorted = y_pred[sorted_indices]\n",
    "\n",
    "    # calculate the lower and upper bounds of the intervals for class 1\n",
    "    lower_bound = y_pred_p1 - y_pred_interval_p1[:, 0] # calculate lower bound by subtracting the lower interval from the predicted probability of class 1\n",
    "    upper_bound = y_pred_interval_p1[:, 1] - y_pred_p1 # calculate upper bound by subtracting the predicted probability of class 1 from the upper interval \n",
    "    bounds = [lower_bound, upper_bound]\n",
    "\n",
    "    # plot the predicted probability of class 1 with intervals\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.errorbar(np.arange(len(y_pred_p1)), y_pred_p1, yerr=bounds, fmt='o', ecolor='tab:red', capsize=5, label='Predicted Probability of Class 1')\n",
    "    # plt.scatter(np.arange(len(y_pred_p1)), y_test_sorted, color='tab:blue', label='Actual Label')\n",
    "    plt.axhline(y=0.5, color='black', linestyle='--', linewidth=1)\n",
    "    plt.ylabel('Probability')\n",
    "    plt.xlabel('Test Sample')\n",
    "    plt.title('Predicted Probability of Class 1 with intervals')\n",
    "    plt.savefig(f'./{plot_save_path}/predictionIntervals.png')\n",
    "    plt.close()\n",
    "\n",
    "    return_path = f'./{plot_save_path}/predictionIntervals.png'\n",
    "    return {\"interval_plot\": \"the intervals plot has been saved to the static folder\", \"plot_path\": return_path}\n",
    "\n",
    "# feature importance\n",
    "def feature_importance(model_name:str, shap_values_set:str = \"empty\") -> dict:\n",
    "    \"\"\"do feature importance analysis for the trained model using stored shap values, generate plots and return the path to the plots.\n",
    "    model_name: str: the name of the trained model\n",
    "    shap_values_set: str: the name of the set of shap values to use for the analysis - if None or not specified then full set, otherwise specify the set to use - zero, one or two\"\"\"\n",
    "    \n",
    "    import shap\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plot_save_path = 'static/plots'\n",
    "    data_save_path = 'data_python'\n",
    "\n",
    "    # load the shap values and the test set\n",
    "    if shap_values_set == \"empty\":\n",
    "        shap_values_set = \"full\"\n",
    "        shap_values = np.load(f'./{data_save_path}/{model_name}_shap_values_{shap_values_set}.npy', allow_pickle=True)\n",
    "    else:\n",
    "        shap_values = np.load(f'./{data_save_path}/{model_name}_shap_values_{shap_values_set}.npy', allow_pickle=True)\n",
    "    X_test = pd.read_csv(f'./{data_save_path}/Sepsis_X_test.csv')\n",
    "    y_test = pd.read_csv(f'./{data_save_path}/Sepsis_y_test.csv')\n",
    "\n",
    "    # generate the summary plot\n",
    "    shap.summary_plot(shap_values, X_test, show=False) # beeswarm plot\n",
    "    plt.title('SHAP values for the full test set')\n",
    "    plt.savefig(f'./{plot_save_path}/featureImportanceYY{shap_values_set}.png')\n",
    "    plt.close()\n",
    "\n",
    "    return_path = f'./{plot_save_path}/featureImportanceYY{shap_values_set}.png'\n",
    "    return {\"feature_importance_plot\": \"the feature importance plot using shap values has been saved to the static folder\", \"plot_path\": return_path}\n",
    "\n",
    "# statistical significance analysis\n",
    "def statistical_significance(model_name:str, feature_name: str) -> dict:\n",
    "    \"\"\"do statistical significance test for shap values of the specified feature for different prediction set widths and return the statistical significance results.\n",
    "    model_name: str: the name of the trained model\n",
    "    feature_name: str: the name of the feature for which to do the statistical significance test for the shap values\"\"\"\n",
    "    \n",
    "    from scipy.stats import mannwhitneyu\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import numpy as np\n",
    "\n",
    "    data_save_path = 'data_python'\n",
    "    plot_save_path = 'static/plots'\n",
    "\n",
    "    # load the data\n",
    "    X_test = pd.read_csv(f'./{data_save_path}/Sepsis_X_test.csv')\n",
    "\n",
    "    # load the shap values\n",
    "    try:\n",
    "        shap_values_zero = np.load(f'./{data_save_path}/{model_name}_shap_values_zero.npy', allow_pickle=True)\n",
    "        shap_values_zero_width_df = pd.DataFrame(shap_values_zero, columns=X_test.columns)\n",
    "        shap_values_zero_width_feature = shap_values_zero_width_df[feature_name].values # get the SHAP values for the specific feature\n",
    "    except:\n",
    "        shap_values_zero = None\n",
    "    try:\n",
    "        shap_values_one = np.load(f'./{data_save_path}/{model_name}_shap_values_one.npy', allow_pickle=True)\n",
    "        shap_values_one_width_df = pd.DataFrame(shap_values_one, columns=X_test.columns)\n",
    "        shap_values_one_width_feature = shap_values_one_width_df[feature_name].values # get the SHAP values for the specific feature\n",
    "    except:\n",
    "        shap_values_one = None\n",
    "    try:\n",
    "        shap_values_two = np.load(f'./{data_save_path}/{model_name}_shap_values_two.npy', allow_pickle=True)\n",
    "        shap_values_two_width_df = pd.DataFrame(shap_values_two, columns=X_test.columns)\n",
    "        shap_values_two_width_feature = shap_values_two_width_df[feature_name].values # get the SHAP values for the specific feature\n",
    "    except:\n",
    "        shap_values_two = None\n",
    "\n",
    "    # plot the distribution of SHAP values for the specific feature where shap_values_set is not None\n",
    "    if shap_values_zero is None:\n",
    "        shap_values_A = shap_values_one_width_feature\n",
    "        shap_values_B = shap_values_two_width_feature\n",
    "        label_A = 'Width 1'\n",
    "        label_B = 'Width 2'\n",
    "    elif shap_values_one is None:\n",
    "        shap_values_A = shap_values_zero_width_feature\n",
    "        shap_values_B = shap_values_two_width_feature\n",
    "        label_A = 'Width 0'\n",
    "        label_B = 'Width 2'\n",
    "    elif shap_values_two is None:\n",
    "        shap_values_A = shap_values_zero_width_feature\n",
    "        shap_values_B = shap_values_one_width_feature\n",
    "        label_A = 'Width 0'\n",
    "        label_B = 'Width 1'\n",
    "\n",
    "    # plot the distribution of SHAP values for the specific feature where the shap values are available\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.violinplot(data=[shap_values_A, shap_values_B], inner='box', palette='tab10')\n",
    "    plt.xticks([0, 1], [label_A, label_B])\n",
    "    plt.ylabel('Shapley values')\n",
    "    plt.xlabel('Prediction Set Width')\n",
    "    plt.title(f'Distribution of Shapley values for the feature: {feature_name}')\n",
    "    # annotation\n",
    "    plt.text(0, np.median(shap_values_A), f'n = {len(shap_values_A)}\\nmedian = {np.median(shap_values_A):.2f}', horizontalalignment='center', size='small', weight='semibold')\n",
    "    plt.text(1, np.median(shap_values_B), f'n = {len(shap_values_B)}\\nmedian = {np.median(shap_values_B):.2f}', horizontalalignment='center', size='small', weight='semibold')\n",
    "    plt.savefig(f'./{plot_save_path}/shapValuesDistribution.png')\n",
    "    plt.close()\n",
    "\n",
    "    return_path = f'./{plot_save_path}/shapValuesDistribution.png'       \n",
    "\n",
    "    # statistical test - Mann-Whitney U rank test\n",
    "    stat, p = mannwhitneyu(shap_values_A, shap_values_B, alternative='two-sided') # perform the Mann-Whitney U test\n",
    "    \n",
    "    alpha = 0.05 # significance level\n",
    "    if p < alpha:\n",
    "        significance_result = f'Reject the null hypothesis: There is a significant difference between the SHAP values for the feature {feature_name} in the test sets between {label_A} and {label_B}'\n",
    "    else:\n",
    "        statistical_significance = f'Fail to reject the null hypothesis: There is no significant difference between the SHAP values for the feature {feature_name} in the test sets between {label_A} and {label_B}'\n",
    "\n",
    "    return {\"statistical_significance\":significance_result, \"p_value\":p, \"plot_path\": return_path}\n",
    "\n",
    "# create tools\n",
    "evaluate_model_tool = FunctionTool.from_defaults(name=\"evaluate_model\", fn=evaluate_model)\n",
    "conformal_prediction_tool = FunctionTool.from_defaults(name=\"conformal_prediction\", fn=conformal_prediction)\n",
    "venn_abers_calibration_tool = FunctionTool.from_defaults(name=\"venn_abers_calibration\", fn=venn_abers_calibration)\n",
    "feature_importance_tool = FunctionTool.from_defaults(name=\"feature_importance\", fn=feature_importance)\n",
    "statistical_significance_tool = FunctionTool.from_defaults(name=\"statistical_significance\", fn=statistical_significance)\n",
    "tools = [conformal_prediction_tool, evaluate_model_tool, venn_abers_calibration_tool, feature_importance_tool, statistical_significance_tool]\n",
    "\n",
    "top_level_agent_prompt = \"\"\"\n",
    "                You are designed to help with a variety of tasks, from answering questions \\\n",
    "                to providing summaries to other types of analyses.\n",
    "\n",
    "                ## Tools\n",
    "                You have access to a wide variety of tools. You are responsible for using\n",
    "                the tools in any sequence you deem appropriate to complete the task at hand.\n",
    "                This may require breaking the task into subtasks and using different tools\n",
    "                to complete each subtask.\n",
    "\n",
    "                You have access to the following tools:\n",
    "                {tool_desc}\n",
    "\n",
    "                ## Output Format\n",
    "                To answer the question, please use the following format.\n",
    "\n",
    "                ```\n",
    "                Thought: I need to use a tool to help me answer the question.\n",
    "                Action: tool name (one of {tool_names}) if using a tool.\n",
    "                Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"input\": \"hello world\", \"num_beams\": 5}})\n",
    "                ```\n",
    "\n",
    "                Please ALWAYS start with a Thought.\n",
    "\n",
    "                Please use a valid JSON format for the Action Input. Do NOT do this {{'input': 'hello world', 'num_beams': 5}}.\n",
    "\n",
    "                If this format is used, the user will respond in the following format:\n",
    "\n",
    "                ```\n",
    "                Observation: tool response\n",
    "                ```\n",
    "\n",
    "                You should keep repeating the above format until you have enough information\n",
    "                to answer the question without using any more tools. At that point, you MUST respond\n",
    "                in the one of the following two formats:\n",
    "\n",
    "                ```\n",
    "                Thought: I can answer without using any more tools.\n",
    "                Answer: [your answer here]\n",
    "                ```\n",
    "\n",
    "                ```\n",
    "                Thought: I cannot answer the question with the provided tools.\n",
    "                Answer: Sorry, I cannot answer your query.\n",
    "                ```\n",
    "\n",
    "                ## Additional Rules\n",
    "                - You MUST obey the function signature of each tool. Do NOT pass in no arguments if the function expects arguments.\n",
    "                - For queries that require uncertainty quantification (like 'what is the coverage and average width of the prediction sets'), use 'conformal_prediction'.\n",
    "                - For queries that requires to evaluate the model (like 'what is the f1 score of the model'), use 'evaluate_model'.\n",
    "                - For queries that require Venn-Abers calibration (like 'generate the prediction intervals for the test set'), use 'venn_abers_calibration'.\n",
    "                - Answer only the questions asked.\n",
    "\n",
    "                ## Current Conversation\n",
    "                Below is the current conversation consisting of interleaving human and assistant messages.\n",
    "                \"\"\"\n",
    "model_agent_prompt = PromptTemplate(top_level_agent_prompt)\n",
    "agent_model = ReActAgent.from_tools(tools=tools, \n",
    "                                     llm=llm,\n",
    "                                     verbose=True)\n",
    "agent_model.update_prompts({\"agent_worker:system_prompt\": model_agent_prompt})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Agent run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Feature and SHAP matrices must have the same number of rows!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfeature_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mXGBoost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzero\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 151\u001b[0m, in \u001b[0;36mfeature_importance\u001b[1;34m(model_name, shap_values_set)\u001b[0m\n\u001b[0;32m    148\u001b[0m y_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_save_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/Sepsis_y_test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# generate the summary plot\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshap_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# beeswarm plot\u001b[39;00m\n\u001b[0;32m    152\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSHAP values for the full test set\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    153\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplot_save_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/featureImportanceYY\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshap_values_set\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\conda_envs\\rag\\lib\\site-packages\\shap\\plots\\_beeswarm.py:700\u001b[0m, in \u001b[0;36msummary_legacy\u001b[1;34m(shap_values, features, feature_names, max_display, plot_type, color, axis_color, title, alpha, show, sort, color_bar, plot_size, layered_violin_max_num_bins, class_names, class_inds, color_bar_label, cmap, show_values_in_legend, use_log_scale)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vmin \u001b[38;5;241m>\u001b[39m vmax: \u001b[38;5;66;03m# fixes rare numerical precision issues\u001b[39;00m\n\u001b[0;32m    698\u001b[0m     vmin \u001b[38;5;241m=\u001b[39m vmax\n\u001b[1;32m--> 700\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(shaps), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature and SHAP matrices must have the same number of rows!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# plot the nan values in the interaction feature as grey\u001b[39;00m\n\u001b[0;32m    703\u001b[0m nan_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(values)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Feature and SHAP matrices must have the same number of rows!"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x950 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importance(\"XGBoost\", \"zero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: I need to use a tool to help me answer the question.\n",
      "Action: feature_importance\n",
      "Action Input: {'model_name': 'XGBoost', 'shap_values_set': 'one'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Error: Feature and SHAP matrices must have the same number of rows!\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: It seems there was an error in the previous attempt to generate the SHAP values distribution. I should try again and specify the correct SHAP values set.\n",
      "Action: feature_importance\n",
      "Action Input: {'model_name': 'XGBoost', 'shap_values_set': 'one'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Error: Feature and SHAP matrices must have the same number of rows!\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: It seems there was an error in the previous attempt to generate the SHAP values distribution. I should try again and specify the correct SHAP values set.\n",
      "Action: feature\n",
      "Action Input: {}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Error: No such tool named `feature`.\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: It seems there was an error in the previous attempt to generate the SHAP values distribution. I should try again and specify the correct SHAP values set.\n",
      "Action: feature\n",
      "Action Input: {}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Error: No such tool named `feature`.\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: It seems there was an error in the previous attempt to generate the SHAP values distribution. I should try again and specify the correct SHAP values set.\n",
      "Action: feature\n",
      "Action Input: {}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Error: No such tool named `feature`.\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Reached max iterations.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# response = await agent.achat(\"what is the auc_roc score of the trained model?\")\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# response = agent_model.query(\"what is the auc_roc score of the XGBoost model?\")\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# response = agent_model.query(\"what is the uncertainty quantification of the HGBoost model?\")\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# response = agent_model.query(\"what is the prediction intervals for XGBoost model?\")\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43magent_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhat is the shap values distribution for the XGBoost model for set 1?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(response))\n",
      "File \u001b[1;32md:\\conda_envs\\rag\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    228\u001b[0m )\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32md:\\conda_envs\\rag\\lib\\site-packages\\llama_index\\core\\base\\base_query_engine.py:52\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[1;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     51\u001b[0m         str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[1;32m---> 52\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m     54\u001b[0m     QueryEndEvent(query\u001b[38;5;241m=\u001b[39mstr_or_query_bundle, response\u001b[38;5;241m=\u001b[39mquery_result)\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "File \u001b[1;32md:\\conda_envs\\rag\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    228\u001b[0m )\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32md:\\conda_envs\\rag\\lib\\site-packages\\llama_index\\core\\callbacks\\utils.py:41\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m cast(CallbackManager, callback_manager)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(trace_id):\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\conda_envs\\rag\\lib\\site-packages\\llama_index\\core\\base\\agent\\types.py:44\u001b[0m, in \u001b[0;36mBaseAgent._query\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;129m@trace_method\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RESPONSE_TYPE:\n\u001b[1;32m---> 44\u001b[0m     agent_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m     49\u001b[0m         response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(agent_response), source_nodes\u001b[38;5;241m=\u001b[39magent_response\u001b[38;5;241m.\u001b[39msource_nodes\n\u001b[0;32m     50\u001b[0m     )\n",
      "File \u001b[1;32md:\\conda_envs\\rag\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    228\u001b[0m )\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32md:\\conda_envs\\rag\\lib\\site-packages\\llama_index\\core\\callbacks\\utils.py:41\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m cast(CallbackManager, callback_manager)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(trace_id):\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\conda_envs\\rag\\lib\\site-packages\\llama_index\\core\\agent\\runner\\base.py:640\u001b[0m, in \u001b[0;36mAgentRunner.chat\u001b[1;34m(self, message, chat_history, tool_choice)\u001b[0m\n\u001b[0;32m    635\u001b[0m     tool_choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_tool_choice\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    637\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mAGENT_STEP,\n\u001b[0;32m    638\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mMESSAGES: [message]},\n\u001b[0;32m    639\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 640\u001b[0m     chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatResponseMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWAIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chat_response, AgentChatResponse)\n\u001b[0;32m    647\u001b[0m     e\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: chat_response})\n",
      "File \u001b[1;32md:\\conda_envs\\rag\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    228\u001b[0m )\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32md:\\conda_envs\\rag\\lib\\site-packages\\llama_index\\core\\agent\\runner\\base.py:572\u001b[0m, in \u001b[0;36mAgentRunner._chat\u001b[1;34m(self, message, chat_history, tool_choice, mode)\u001b[0m\n\u001b[0;32m    569\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(AgentChatWithStepStartEvent(user_msg\u001b[38;5;241m=\u001b[39mmessage))\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# pass step queue in as argument, assume step executor is stateless\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     cur_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cur_step_output\u001b[38;5;241m.\u001b[39mis_last:\n\u001b[0;32m    577\u001b[0m         result_output \u001b[38;5;241m=\u001b[39m cur_step_output\n",
      "File \u001b[1;32md:\\conda_envs\\rag\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    228\u001b[0m )\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32md:\\conda_envs\\rag\\lib\\site-packages\\llama_index\\core\\agent\\runner\\base.py:411\u001b[0m, in \u001b[0;36mAgentRunner._run_step\u001b[1;34m(self, task_id, step, input, mode, **kwargs)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;66;03m# TODO: figure out if you can dynamically swap in different step executors\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# not clear when you would do that by theoretically possible\u001b[39;00m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ChatResponseMode\u001b[38;5;241m.\u001b[39mWAIT:\n\u001b[1;32m--> 411\u001b[0m     cur_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_worker\u001b[38;5;241m.\u001b[39mrun_step(step, task, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ChatResponseMode\u001b[38;5;241m.\u001b[39mSTREAM:\n\u001b[0;32m    413\u001b[0m     cur_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_worker\u001b[38;5;241m.\u001b[39mstream_step(step, task, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\conda_envs\\rag\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    228\u001b[0m )\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32md:\\conda_envs\\rag\\lib\\site-packages\\llama_index\\core\\callbacks\\utils.py:41\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m cast(CallbackManager, callback_manager)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(trace_id):\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\conda_envs\\rag\\lib\\site-packages\\llama_index\\core\\agent\\react\\step.py:746\u001b[0m, in \u001b[0;36mReActAgentWorker.run_step\u001b[1;34m(self, step, task, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;129m@trace_method\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_step\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    744\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, step: TaskStep, task: Task, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TaskStepOutput:\n\u001b[0;32m    745\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run step.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 746\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\conda_envs\\rag\\lib\\site-packages\\llama_index\\core\\agent\\react\\step.py:544\u001b[0m, in \u001b[0;36mReActAgentWorker._run_step\u001b[1;34m(self, step, task)\u001b[0m\n\u001b[0;32m    540\u001b[0m reasoning_steps, is_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_actions(\n\u001b[0;32m    541\u001b[0m     task, tools, output\u001b[38;5;241m=\u001b[39mchat_response\n\u001b[0;32m    542\u001b[0m )\n\u001b[0;32m    543\u001b[0m task\u001b[38;5;241m.\u001b[39mextra_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_reasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mextend(reasoning_steps)\n\u001b[1;32m--> 544\u001b[0m agent_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextra_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcurrent_reasoning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextra_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msources\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_done:\n\u001b[0;32m    548\u001b[0m     task\u001b[38;5;241m.\u001b[39mextra_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_memory\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mput(\n\u001b[0;32m    549\u001b[0m         ChatMessage(content\u001b[38;5;241m=\u001b[39magent_response\u001b[38;5;241m.\u001b[39mresponse, role\u001b[38;5;241m=\u001b[39mMessageRole\u001b[38;5;241m.\u001b[39mASSISTANT)\n\u001b[0;32m    550\u001b[0m     )\n",
      "File \u001b[1;32md:\\conda_envs\\rag\\lib\\site-packages\\llama_index\\core\\agent\\react\\step.py:416\u001b[0m, in \u001b[0;36mReActAgentWorker._get_response\u001b[1;34m(self, current_reasoning, sources)\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo reasoning steps were taken.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(current_reasoning) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_iterations:\n\u001b[1;32m--> 416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReached max iterations.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(current_reasoning[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], ResponseReasoningStep):\n\u001b[0;32m    419\u001b[0m     response_step \u001b[38;5;241m=\u001b[39m cast(ResponseReasoningStep, current_reasoning[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mValueError\u001b[0m: Reached max iterations."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x950 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# response = await agent.achat(\"what is the auc_roc score of the trained model?\")\n",
    "# response = agent_model.query(\"what is the auc_roc score of the XGBoost model?\")\n",
    "# response = agent_model.query(\"what is the uncertainty quantification of the HGBoost model?\")\n",
    "# response = agent_model.query(\"what is the prediction intervals for XGBoost model?\")\n",
    "response = agent_model.query(\"what is the shap values distribution for the XGBoost model for set 1?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Agent Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods for running the agents / query pipelines\n",
    "def run_agent(query: str) -> str:\n",
    "    \"\"\"Run the agent model on the query to get evaluation results from trained model.\"\"\"\n",
    "    response = agent_model.query(query)\n",
    "    return str(response)\n",
    "\n",
    "def run_query_pipeline(query: str) -> str:\n",
    "    \"\"\"Run the query pipeline to analyze dataset for the given query.\"\"\"\n",
    "    response = qp_table.run(\n",
    "        query_str=query,\n",
    "    )\n",
    "    return str(response.message.content)\n",
    "\n",
    "# create tools\n",
    "run_agent_tool = FunctionTool.from_defaults(name=\"run_agent\", fn=run_agent)\n",
    "run_query_pipeline_tool = FunctionTool.from_defaults(name=\"run_query_pipeline\", fn=run_query_pipeline)\n",
    "agent_tools = [run_agent_tool, run_query_pipeline_tool]\n",
    "\n",
    "top_level_agent_prompt = \"\"\"\n",
    "                You are designed to help with a variety of tasks, from answering questions \\\n",
    "                to providing summaries to other types of analyses.\n",
    "\n",
    "                ## Tools\n",
    "                You have access to a wide variety of tools. You are responsible for using\n",
    "                the tools in any sequence you deem appropriate to complete the task at hand.\n",
    "                This may require breaking the task into subtasks and using different tools\n",
    "                to complete each subtask.\n",
    "\n",
    "                You have access to the following tools:\n",
    "                {tool_desc}\n",
    "\n",
    "                ## Output Format\n",
    "                To answer the question, please use the following format.\n",
    "\n",
    "                ```\n",
    "                Thought: I need to use a tool to help me answer the question.\n",
    "                Action: tool name (one of {tool_names}) if using a tool.\n",
    "                Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"input\": \"hello world\", \"num_beams\": 5}})\n",
    "                ```\n",
    "\n",
    "                Please ALWAYS start with a Thought.\n",
    "\n",
    "                Please use a valid JSON format for the Action Input. Do NOT do this {{'input': 'hello world', 'num_beams': 5}}.\n",
    "\n",
    "                If this format is used, the user will respond in the following format:\n",
    "\n",
    "                ```\n",
    "                Observation: tool response\n",
    "                ```\n",
    "\n",
    "                You should keep repeating the above format until you have enough information\n",
    "                to answer the question without using any more tools. At that point, you MUST respond\n",
    "                in the one of the following two formats:\n",
    "\n",
    "                ```\n",
    "                Thought: I can answer without using any more tools.\n",
    "                Answer: [your answer here]\n",
    "                ```\n",
    "\n",
    "                ```\n",
    "                Thought: I cannot answer the question with the provided tools.\n",
    "                Answer: Sorry, I cannot answer your query.\n",
    "                ```\n",
    "\n",
    "                ## Additional Rules\n",
    "                - You MUST obey the function signature of each tool. Do NOT pass in no arguments if the function expects arguments.\n",
    "                - For queries that clearly involve data retrieval or manipulation (like 'analyze sales data', 'show trends in data'), use 'run_query_pipeline'.\n",
    "                - For queries that directly relate to model performance or evaluation (like 'what is the AUC_ROC score', 'evaluate the prediction accuracy'), use 'run_agent'.\n",
    "\n",
    "                ## Current Conversation\n",
    "                Below is the current conversation consisting of interleaving human and assistant messages.\n",
    "                \"\"\"\n",
    "top_level_agent_prompt = PromptTemplate(top_level_agent_prompt)\n",
    "agent = ReActAgent.from_tools(tools=agent_tools, \n",
    "                                    llm=llm, \n",
    "                                    verbose=True)\n",
    "agent.update_prompts({\"agent_worker:system_prompt\": top_level_agent_prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: The user is asking for a specific count of positive cases, which involves data retrieval. I should use the 'run_query_pipeline' tool to answer this question.\n",
      "Action: run_query_pipeline\n",
      "Action Input: {'query': 'how many positive cases'}\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
      "query_str: how many positive cases\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_prompt with input: \n",
      "query_str: how many positive cases\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm1 with input: \n",
      "messages: You are working with a pandas dataframe in Python.\n",
      "The name of the dataframe is `df`.\n",
      "This is the result of `print(df.head())`:\n",
      "  case_id  Target  age  max_activity_count  duration_since_reg  crp  \\\n",
      "0...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_output_parser with input: \n",
      "input: assistant: df['Target'].sum()\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module response_synthesis_prompt with input: \n",
      "query_str: how many positive cases\n",
      "pandas_instructions: assistant: df['Target'].sum()\n",
      "pandas_output: 98\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm2 with input: \n",
      "messages: Given an input question, synthesize a response from the query results.\n",
      "Query: how many positive cases\n",
      "\n",
      "Pandas Instructions (optional):\n",
      "df['Target'].sum()\n",
      "\n",
      "Pandas Output: 98\n",
      "\n",
      "Response: \n",
      "\n",
      "\u001b[0m\u001b[1;3;34mObservation: Based on the data provided, there are 98 positive cases.\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: I have the answer to the user's question, so I can respond without using any more tools.\n",
      "Answer: There are 98 positive cases.\n",
      "\u001b[0mThere are 98 positive cases.\n"
     ]
    }
   ],
   "source": [
    "# response = await agent.achat(\"what is the auc_roc score of the trained model?\")\n",
    "# response = agent.query(\"what is the auc_roc score of the trained XGBoost model?\")\n",
    "response = agent.query(\"how many positive cases?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: agent_worker:system_prompt\n",
      "\n",
      "Value: \n",
      "                You are designed to help with a variety of tasks, from answering questions                 to providing summaries to other types of analyses.\n",
      "\n",
      "                ## Tools\n",
      "                You have access to a wide variety of tools. You are responsible for using\n",
      "                the tools in any sequence you deem appropriate to complete the task at hand.\n",
      "                This may require breaking the task into subtasks and using different tools\n",
      "                to complete each subtask.\n",
      "\n",
      "                You have access to the following tools:\n",
      "                {tool_desc}\n",
      "\n",
      "                ## Output Format\n",
      "                To answer the question, please use the following format.\n",
      "\n",
      "                ```\n",
      "                Thought: I need to use a tool to help me answer the question.\n",
      "                Action: tool name (one of {tool_names}) if using a tool.\n",
      "                Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"input\": \"hello world\", \"num_beams\": 5}})\n",
      "                ```\n",
      "\n",
      "                Please ALWAYS start with a Thought.\n",
      "\n",
      "                Please use a valid JSON format for the Action Input. Do NOT do this {{'input': 'hello world', 'num_beams': 5}}.\n",
      "\n",
      "                If this format is used, the user will respond in the following format:\n",
      "\n",
      "                ```\n",
      "                Observation: tool response\n",
      "                ```\n",
      "\n",
      "                You should keep repeating the above format until you have enough information\n",
      "                to answer the question without using any more tools. At that point, you MUST respond\n",
      "                in the one of the following two formats:\n",
      "\n",
      "                ```\n",
      "                Thought: I can answer without using any more tools.\n",
      "                Answer: [your answer here]\n",
      "                ```\n",
      "\n",
      "                ```\n",
      "                Thought: I cannot answer the question with the provided tools.\n",
      "                Answer: Sorry, I cannot answer your query.\n",
      "                ```\n",
      "\n",
      "                ## Additional Rules\n",
      "                - You MUST obey the function signature of each tool. Do NOT pass in no arguments if the function expects arguments.\n",
      "                - For queries that clearly involve data retrieval or manipulation (like 'analyze sales data', 'show trends in data'), use 'run_query_pipeline'.\n",
      "                - For queries that directly relate to model performance or evaluation (like 'what is the AUC_ROC score', 'evaluate the prediction accuracy'), use 'run_agent'.\n",
      "\n",
      "                ## Current Conversation\n",
      "                Below is the current conversation consisting of interleaving human and assistant messages.\n",
      "                \n"
     ]
    }
   ],
   "source": [
    "prompt_dict = agent.get_prompts()\n",
    "for k, v in prompt_dict.items():\n",
    "    print(f\"Prompt: {k}\\n\\nValue: {v.template}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Agent run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: The user is asking for a count of positive cases, which involves data retrieval. I should use the 'run_query_pipeline' tool to answer this question.\n",
      "Action: run_query_pipeline\n",
      "Action Input: {'query': 'count positive cases'}\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
      "query_str: count positive cases\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_prompt with input: \n",
      "query_str: count positive cases\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm1 with input: \n",
      "messages: You are working with a pandas dataframe in Python.\n",
      "The name of the dataframe is `df`.\n",
      "This is the result of `print(df.head())`:\n",
      "  case_id  Target  age  max_activity_count  duration_since_reg  crp  \\\n",
      "0...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_output_parser with input: \n",
      "input: assistant: df['Target'].sum()\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module response_synthesis_prompt with input: \n",
      "query_str: count positive cases\n",
      "pandas_instructions: assistant: df['Target'].sum()\n",
      "pandas_output: 98\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm2 with input: \n",
      "messages: Given an input question, synthesize a response from the query results.\n",
      "Query: count positive cases\n",
      "\n",
      "Pandas Instructions (optional):\n",
      "df['Target'].sum()\n",
      "\n",
      "Pandas Output: 98\n",
      "\n",
      "Response: \n",
      "\n",
      "\u001b[0m\u001b[1;3;34mObservation: Based on the data provided, the total count of positive cases is 98.\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: I have the answer to the question, so I can provide it without using any more tools.\n",
      "Answer: There are 98 positive cases according to the data provided.\n",
      "\u001b[0m===========================final response============================\n",
      "There are 98 positive cases according to the data provided.\n"
     ]
    }
   ],
   "source": [
    "# response = await agent.achat(\"what is the auc_roc score of the trained model?\")\n",
    "response = agent.query(\"how many positive cases?\")\n",
    "print(f'===========================final response============================\\n{str(response)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: This question involves model performance evaluation, specifically asking for the AUC_ROC score of the trained XGBoost model. I should use the 'run_agent' tool to answer this.\n",
      "Action: run_agent\n",
      "Action Input: {'query': 'auc_roc_score_xgboost'}\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: The current language of the user is: English. The user is asking about the AUC ROC score for the XGBoost model. I need to use the 'evaluate_model' tool to help me answer this question.\n",
      "Action: evaluate_model\n",
      "Action Input: {'model_name': 'xgboost'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: {'roc_auc': 0.8659217877094972, 'average_precision': 0.7481294952146433, 'mcc': 0.7002038117109098, 'f1_macro': 0.8472989564149784, 'confusion_matrix': array([[176,   3],\n",
      "       [  7,  13]], dtype=int64)}\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: The current language of the user is: English. I have the results of the XGBoost model evaluation. The AUC ROC score is 0.8659217877094972.\n",
      "Answer: The AUC ROC score for the XGBoost model is 0.8659217877094972.\n",
      "\u001b[0m\u001b[1;3;34mObservation: The AUC ROC score for the XGBoost model is 0.8659217877094972.\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: I have received the AUC_ROC score for the XGBoost model from the 'run_agent' tool.\n",
      "Answer: The AUC_ROC score for the XGBoost model is 0.8659217877094972.\n",
      "\u001b[0m===========================final response============================\n",
      "The AUC_ROC score for the XGBoost model is 0.8659217877094972.\n"
     ]
    }
   ],
   "source": [
    "# response = await agent.achat(\"what is the auc_roc score of the trained model?\")\n",
    "response = agent.query(\"what is the auc_roc score of the trained XGBoost model?\")\n",
    "print(f'===========================final response============================\\n{str(response)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: This question involves data retrieval and manipulation, specifically calculating the average time patients spend in the hospital. I should use the 'run_query_pipeline' tool to analyze the dataset and find this information.\n",
      "Action: run_query_pipeline\n",
      "Action Input: {'query': 'average time patients spend in the hospital'}\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
      "query_str: average time patients spend in the hospital\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_prompt with input: \n",
      "query_str: average time patients spend in the hospital\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm1 with input: \n",
      "messages: You are working with a pandas dataframe in Python.\n",
      "The name of the dataframe is `df`.\n",
      "This is the result of `print(df.head())`:\n",
      "  case_id  Target  age  max_activity_count  duration_since_reg  crp  \\\n",
      "0...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_output_parser with input: \n",
      "input: assistant: df['duration_since_reg'].mean()\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module response_synthesis_prompt with input: \n",
      "query_str: average time patients spend in the hospital\n",
      "pandas_instructions: assistant: df['duration_since_reg'].mean()\n",
      "pandas_output: 11.568710217755443\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm2 with input: \n",
      "messages: Given an input question, synthesize a response from the query results.\n",
      "Query: average time patients spend in the hospital\n",
      "\n",
      "Pandas Instructions (optional):\n",
      "df['duration_since_reg'].mean()\n",
      "\n",
      "Pandas Outpu...\n",
      "\n",
      "\u001b[0m\u001b[1;3;34mObservation: Based on the data provided, the average time patients spend in the hospital is approximately 11.57 days.\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: I have received the result from the 'run_query_pipeline' tool, which calculated the average time patients spend in the hospital. Now I can answer the question without using any more tools.\n",
      "Answer: The average time patients spend in the hospital is approximately 11.57 days.\n",
      "\u001b[0m===========================final response============================\n",
      "The average time patients spend in the hospital is approximately 11.57 days.\n"
     ]
    }
   ],
   "source": [
    "# response = await agent.achat(\"what is the auc_roc score of the trained model?\")\n",
    "response = agent.query(\"what is the average time patients spend in the hospital?\")\n",
    "print(f'===========================final response============================\\n{str(response)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
