{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader,VectorStoreIndex,SummaryIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.tools import FunctionTool,QueryEngineTool\n",
    "from llama_index.core.vector_stores import MetadataFilters,FilterCondition\n",
    "from typing import List,Optional\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "userdata = {\n",
    "    \"MISTRAL_API_KEY\": \"BWdlihu9sUh5P2g3bHnzjAaHiT4anTVH\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "Document Metadata: {'page_label': '1', 'file_name': 'bert_pre_train.pdf', 'file_path': 'data\\\\bert_pre_train.pdf', 'file_type': 'application/pdf', 'file_size': 775166, 'creation_date': '2024-06-17', 'last_modified_date': '2024-06-17'}\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(input_files = ['./data/bert_pre_train.pdf']).load_data()\n",
    "print(len(documents))\n",
    "print(f\"Document Metadata: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of nodes : 28\n",
      "get the content for node 0 :page_label: 1\n",
      "file_name: bert_pre_train.pdf\n",
      "file_path: data\\bert_pre_train.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 775166\n",
      "creation_date: 2024-06-17\n",
      "last_modified_date: 2024-06-17\n",
      "\n",
      "BERT: Pre-training of Deep Bidirectional Transformers for\n",
      "Language Understanding\n",
      "Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\n",
      "Google AI Language\n",
      "{jacobdevlin,mingweichang,kentonl,kristout }@google.com\n",
      "Abstract\n",
      "We introduce a new language representa-\n",
      "tion model called BERT , which stands for\n",
      "Bidirectional Encoder Representations from\n",
      "Transformers. Unlike recent language repre-\n",
      "sentation models (Peters et al., 2018a; Rad-\n",
      "ford et al., 2018), BERT is designed to pre-\n",
      "train deep bidirectional representations from\n",
      "unlabeled text by jointly conditioning on both\n",
      "left and right context in all layers. As a re-\n",
      "sult, the pre-trained BERT model can be ﬁne-\n",
      "tuned with just one additional output layer\n",
      "to create state-of-the-art models for a wide\n",
      "range of tasks, such as question answering and\n",
      "language inference, without substantial task-\n",
      "speciﬁc architecture modiﬁcations.\n",
      "BERT is conceptually simple and empirically\n",
      "powerful. It obtains new state-of-the-art re-\n",
      "sults on eleven natural language processing\n",
      "tasks, including pushing the GLUE score to\n",
      "80.5% (7.7% point absolute improvement),\n",
      "MultiNLI accuracy to 86.7% (4.6% absolute\n",
      "improvement), SQuAD v1.1 question answer-\n",
      "ing Test F1 to 93.2 (1.5 point absolute im-\n",
      "provement) and SQuAD v2.0 Test F1 to 83.1\n",
      "(5.1 point absolute improvement).\n",
      "1 Introduction\n",
      "Language model pre-training has been shown to\n",
      "be effective for improving many natural language\n",
      "processing tasks (Dai and Le, 2015; Peters et al.,\n",
      "2018a; Radford et al., 2018; Howard and Ruder,\n",
      "2018). These include sentence-level tasks such as\n",
      "natural language inference (Bowman et al., 2015;\n",
      "Williams et al., 2018) and paraphrasing (Dolan\n",
      "and Brockett, 2005), which aim to predict the re-\n",
      "lationships between sentences by analyzing them\n",
      "holistically, as well as token-level tasks such as\n",
      "named entity recognition and question answering,\n",
      "where models are required to produce ﬁne-grained\n",
      "output at the token level (Tjong Kim Sang and\n",
      "De Meulder, 2003; Rajpurkar et al., 2016).There are two existing strategies for apply-\n",
      "ing pre-trained language representations to down-\n",
      "stream tasks: feature-based andﬁne-tuning . The\n",
      "feature-based approach, such as ELMo (Peters\n",
      "et al., 2018a), uses task-speciﬁc architectures that\n",
      "include the pre-trained representations as addi-\n",
      "tional features. The ﬁne-tuning approach, such as\n",
      "the Generative Pre-trained Transformer (OpenAI\n",
      "GPT) (Radford et al., 2018), introduces minimal\n",
      "task-speciﬁc parameters, and is trained on the\n",
      "downstream tasks by simply ﬁne-tuning allpre-\n",
      "trained parameters. The two approaches share the\n",
      "same objective function during pre-training, where\n",
      "they use unidirectional language models to learn\n",
      "general language representations.\n",
      "We argue that current techniques restrict the\n",
      "power of the pre-trained representations, espe-\n",
      "cially for the ﬁne-tuning approaches. The ma-\n",
      "jor limitation is that standard language models are\n",
      "unidirectional, and this limits the choice of archi-\n",
      "tectures that can be used during pre-training. For\n",
      "example, in OpenAI GPT, the authors use a left-to-\n",
      "right architecture, where every token can only at-\n",
      "tend to previous tokens in the self-attention layers\n",
      "of the Transformer (Vaswani et al., 2017). Such re-\n",
      "strictions are sub-optimal for sentence-level tasks,\n",
      "and could be very harmful when applying ﬁne-\n",
      "tuning based approaches to token-level tasks such\n",
      "as question answering, where it is crucial to incor-\n",
      "porate context from both directions.\n",
      "In this paper, we improve the ﬁne-tuning based\n",
      "approaches by proposing BERT: Bidirectional\n",
      "Encoder Representations from Transformers.\n",
      "BERT alleviates the previously mentioned unidi-\n",
      "rectionality constraint by using a “masked lan-\n",
      "guage model” (MLM) pre-training objective, in-\n",
      "spired by the Cloze task (Taylor, 1953). The\n",
      "masked language model randomly masks some of\n",
      "the tokens from the input, and the objective is to\n",
      "predict the original vocabulary id of the maskedarXiv:1810.04805v2  [cs.CL]  24 May 2019\n"
     ]
    }
   ],
   "source": [
    "splitter = SentenceSplitter(chunk_size=1024,chunk_overlap=100)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "print(f\"Length of nodes : {len(nodes)}\")\n",
    "print(f\"get the content for node 0 :{nodes[0].get_content(metadata_mode='all')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector store\n",
    "import chromadb\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db_mistral\")\n",
    "chroma_collection = db.get_or_create_collection(\"multidocument-agent\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda_envs\\rag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# embedding model\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from llama_index.core import Settings\n",
    "embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the llama index\n",
    "os.environ[\"MISTRAL_API_KEY\"] = userdata.get(\"MISTRAL_API_KEY\")\n",
    "llm = MistralAI(model=\"mistral-large-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: ba9060d7-db5c-4210-89d0-6c9d3d240528\n",
      "Add of existing embedding ID: 6cab1fd3-cc91-479c-8178-2073efc87d80\n",
      "Add of existing embedding ID: 43f8d46c-ee3a-4ef0-b49f-d5bfcfd0fb64\n",
      "Add of existing embedding ID: 9ec80a9f-c646-4fa9-a578-c83e574bde26\n",
      "Add of existing embedding ID: 11bb8a99-daa8-4431-a50f-c0dc711192a2\n",
      "Add of existing embedding ID: b4a6f619-2e9c-460e-a44d-6e502adf7b6c\n",
      "Add of existing embedding ID: 7bf5eb3a-81e0-490d-a95d-2b29f9cf1faf\n",
      "Add of existing embedding ID: df7b81d8-12d5-4b44-a754-4adbabd5544d\n",
      "Add of existing embedding ID: 1b80e0d2-5a87-4866-b847-77139222f729\n",
      "Add of existing embedding ID: 79c6678f-ad13-4954-9ca8-82d50acfe864\n",
      "Add of existing embedding ID: 9a61f1d8-39fb-4cb0-93bd-40dceb868c94\n",
      "Add of existing embedding ID: c0d152d8-21f8-432c-817a-5322766dbe52\n",
      "Add of existing embedding ID: 54ae2325-885e-45f7-838f-654dbc426c28\n",
      "Add of existing embedding ID: d8e6f904-f30a-429e-9fa4-e987be5d4968\n",
      "Add of existing embedding ID: c28e20b5-be86-4187-9c4c-48fb799fadf6\n",
      "Add of existing embedding ID: 4ec11899-0f56-4028-8055-e90e970ca20e\n",
      "Add of existing embedding ID: 912435a4-30f9-401e-901e-3ba75f86e968\n",
      "Add of existing embedding ID: 2b429c19-669f-448a-b5d7-f3a037e1164e\n",
      "Add of existing embedding ID: f8e286fd-8968-473b-b798-ebcc017219f6\n",
      "Add of existing embedding ID: ea1c1c8d-0c79-4ffb-9b2e-6c2ac70e01b1\n",
      "Add of existing embedding ID: 0ff6768c-7e57-4a73-a5bb-dbef9bc4f742\n",
      "Add of existing embedding ID: 73f8f26b-01aa-4cb0-9d18-a182971245a8\n",
      "Add of existing embedding ID: 15ef43b4-18cc-4c62-bf2f-988fb7b930b4\n",
      "Add of existing embedding ID: a7287e84-ef69-432f-8db6-be55bf6cfd0a\n",
      "Add of existing embedding ID: 9ecee273-4039-4319-b628-eea31d9265a6\n",
      "Add of existing embedding ID: 3ac26fdc-be1f-4b44-81ce-b3a1c074b949\n",
      "Add of existing embedding ID: 853c470c-351f-45bf-ba14-87cc6c8ff13d\n",
      "Add of existing embedding ID: bce79361-3283-435e-bf1d-efeb483556b4\n",
      "Add of existing embedding ID: 63a6e80b-f0b5-4805-bcaa-916e7dbbd630\n",
      "Add of existing embedding ID: a088ce4d-0720-4228-a1e5-cb4b843e8efb\n",
      "Add of existing embedding ID: ba9060d7-db5c-4210-89d0-6c9d3d240528\n",
      "Add of existing embedding ID: 6cab1fd3-cc91-479c-8178-2073efc87d80\n",
      "Add of existing embedding ID: 43f8d46c-ee3a-4ef0-b49f-d5bfcfd0fb64\n",
      "Add of existing embedding ID: 9ec80a9f-c646-4fa9-a578-c83e574bde26\n",
      "Add of existing embedding ID: 11bb8a99-daa8-4431-a50f-c0dc711192a2\n",
      "Add of existing embedding ID: b4a6f619-2e9c-460e-a44d-6e502adf7b6c\n",
      "Add of existing embedding ID: 7bf5eb3a-81e0-490d-a95d-2b29f9cf1faf\n",
      "Add of existing embedding ID: df7b81d8-12d5-4b44-a754-4adbabd5544d\n",
      "Add of existing embedding ID: 1b80e0d2-5a87-4866-b847-77139222f729\n",
      "Add of existing embedding ID: 79c6678f-ad13-4954-9ca8-82d50acfe864\n",
      "Add of existing embedding ID: 9a61f1d8-39fb-4cb0-93bd-40dceb868c94\n",
      "Add of existing embedding ID: c0d152d8-21f8-432c-817a-5322766dbe52\n",
      "Add of existing embedding ID: 54ae2325-885e-45f7-838f-654dbc426c28\n",
      "Add of existing embedding ID: d8e6f904-f30a-429e-9fa4-e987be5d4968\n",
      "Add of existing embedding ID: c28e20b5-be86-4187-9c4c-48fb799fadf6\n",
      "Add of existing embedding ID: 4ec11899-0f56-4028-8055-e90e970ca20e\n",
      "Add of existing embedding ID: 912435a4-30f9-401e-901e-3ba75f86e968\n",
      "Add of existing embedding ID: 2b429c19-669f-448a-b5d7-f3a037e1164e\n",
      "Add of existing embedding ID: f8e286fd-8968-473b-b798-ebcc017219f6\n",
      "Add of existing embedding ID: ea1c1c8d-0c79-4ffb-9b2e-6c2ac70e01b1\n",
      "Add of existing embedding ID: 0ff6768c-7e57-4a73-a5bb-dbef9bc4f742\n",
      "Add of existing embedding ID: 73f8f26b-01aa-4cb0-9d18-a182971245a8\n",
      "Add of existing embedding ID: 15ef43b4-18cc-4c62-bf2f-988fb7b930b4\n",
      "Add of existing embedding ID: a7287e84-ef69-432f-8db6-be55bf6cfd0a\n",
      "Add of existing embedding ID: 9ecee273-4039-4319-b628-eea31d9265a6\n",
      "Add of existing embedding ID: 3ac26fdc-be1f-4b44-81ce-b3a1c074b949\n",
      "Add of existing embedding ID: 853c470c-351f-45bf-ba14-87cc6c8ff13d\n",
      "Add of existing embedding ID: bce79361-3283-435e-bf1d-efeb483556b4\n",
      "Add of existing embedding ID: 63a6e80b-f0b5-4805-bcaa-916e7dbbd630\n",
      "Add of existing embedding ID: a088ce4d-0720-4228-a1e5-cb4b843e8efb\n",
      "Add of existing embedding ID: ba9060d7-db5c-4210-89d0-6c9d3d240528\n",
      "Add of existing embedding ID: 6cab1fd3-cc91-479c-8178-2073efc87d80\n",
      "Add of existing embedding ID: 43f8d46c-ee3a-4ef0-b49f-d5bfcfd0fb64\n",
      "Add of existing embedding ID: 9ec80a9f-c646-4fa9-a578-c83e574bde26\n",
      "Add of existing embedding ID: 11bb8a99-daa8-4431-a50f-c0dc711192a2\n",
      "Add of existing embedding ID: b4a6f619-2e9c-460e-a44d-6e502adf7b6c\n",
      "Add of existing embedding ID: 7bf5eb3a-81e0-490d-a95d-2b29f9cf1faf\n",
      "Add of existing embedding ID: df7b81d8-12d5-4b44-a754-4adbabd5544d\n",
      "Add of existing embedding ID: 1b80e0d2-5a87-4866-b847-77139222f729\n",
      "Add of existing embedding ID: 79c6678f-ad13-4954-9ca8-82d50acfe864\n",
      "Add of existing embedding ID: 9a61f1d8-39fb-4cb0-93bd-40dceb868c94\n",
      "Add of existing embedding ID: c0d152d8-21f8-432c-817a-5322766dbe52\n",
      "Add of existing embedding ID: 54ae2325-885e-45f7-838f-654dbc426c28\n",
      "Add of existing embedding ID: d8e6f904-f30a-429e-9fa4-e987be5d4968\n",
      "Add of existing embedding ID: c28e20b5-be86-4187-9c4c-48fb799fadf6\n",
      "Add of existing embedding ID: 4ec11899-0f56-4028-8055-e90e970ca20e\n",
      "Add of existing embedding ID: 912435a4-30f9-401e-901e-3ba75f86e968\n",
      "Add of existing embedding ID: 2b429c19-669f-448a-b5d7-f3a037e1164e\n",
      "Add of existing embedding ID: f8e286fd-8968-473b-b798-ebcc017219f6\n",
      "Add of existing embedding ID: ea1c1c8d-0c79-4ffb-9b2e-6c2ac70e01b1\n",
      "Add of existing embedding ID: 0ff6768c-7e57-4a73-a5bb-dbef9bc4f742\n",
      "Add of existing embedding ID: 73f8f26b-01aa-4cb0-9d18-a182971245a8\n",
      "Add of existing embedding ID: 15ef43b4-18cc-4c62-bf2f-988fb7b930b4\n",
      "Add of existing embedding ID: a7287e84-ef69-432f-8db6-be55bf6cfd0a\n",
      "Add of existing embedding ID: 9ecee273-4039-4319-b628-eea31d9265a6\n",
      "Add of existing embedding ID: 3ac26fdc-be1f-4b44-81ce-b3a1c074b949\n",
      "Add of existing embedding ID: 853c470c-351f-45bf-ba14-87cc6c8ff13d\n",
      "Add of existing embedding ID: bce79361-3283-435e-bf1d-efeb483556b4\n",
      "Add of existing embedding ID: 63a6e80b-f0b5-4805-bcaa-916e7dbbd630\n",
      "Add of existing embedding ID: a088ce4d-0720-4228-a1e5-cb4b843e8efb\n",
      "Add of existing embedding ID: ba9060d7-db5c-4210-89d0-6c9d3d240528\n",
      "Add of existing embedding ID: 6cab1fd3-cc91-479c-8178-2073efc87d80\n",
      "Add of existing embedding ID: 43f8d46c-ee3a-4ef0-b49f-d5bfcfd0fb64\n",
      "Add of existing embedding ID: 9ec80a9f-c646-4fa9-a578-c83e574bde26\n",
      "Add of existing embedding ID: 11bb8a99-daa8-4431-a50f-c0dc711192a2\n",
      "Add of existing embedding ID: b4a6f619-2e9c-460e-a44d-6e502adf7b6c\n",
      "Add of existing embedding ID: 7bf5eb3a-81e0-490d-a95d-2b29f9cf1faf\n",
      "Add of existing embedding ID: df7b81d8-12d5-4b44-a754-4adbabd5544d\n",
      "Add of existing embedding ID: 1b80e0d2-5a87-4866-b847-77139222f729\n",
      "Add of existing embedding ID: 79c6678f-ad13-4954-9ca8-82d50acfe864\n",
      "Add of existing embedding ID: 9a61f1d8-39fb-4cb0-93bd-40dceb868c94\n",
      "Add of existing embedding ID: c0d152d8-21f8-432c-817a-5322766dbe52\n",
      "Add of existing embedding ID: 54ae2325-885e-45f7-838f-654dbc426c28\n",
      "Add of existing embedding ID: d8e6f904-f30a-429e-9fa4-e987be5d4968\n",
      "Add of existing embedding ID: c28e20b5-be86-4187-9c4c-48fb799fadf6\n",
      "Add of existing embedding ID: 4ec11899-0f56-4028-8055-e90e970ca20e\n",
      "Add of existing embedding ID: 912435a4-30f9-401e-901e-3ba75f86e968\n",
      "Add of existing embedding ID: 2b429c19-669f-448a-b5d7-f3a037e1164e\n",
      "Add of existing embedding ID: f8e286fd-8968-473b-b798-ebcc017219f6\n",
      "Add of existing embedding ID: ea1c1c8d-0c79-4ffb-9b2e-6c2ac70e01b1\n",
      "Add of existing embedding ID: 0ff6768c-7e57-4a73-a5bb-dbef9bc4f742\n",
      "Add of existing embedding ID: 73f8f26b-01aa-4cb0-9d18-a182971245a8\n",
      "Add of existing embedding ID: 15ef43b4-18cc-4c62-bf2f-988fb7b930b4\n",
      "Add of existing embedding ID: a7287e84-ef69-432f-8db6-be55bf6cfd0a\n",
      "Add of existing embedding ID: 9ecee273-4039-4319-b628-eea31d9265a6\n",
      "Add of existing embedding ID: 3ac26fdc-be1f-4b44-81ce-b3a1c074b949\n",
      "Add of existing embedding ID: 853c470c-351f-45bf-ba14-87cc6c8ff13d\n",
      "Add of existing embedding ID: bce79361-3283-435e-bf1d-efeb483556b4\n",
      "Add of existing embedding ID: 63a6e80b-f0b5-4805-bcaa-916e7dbbd630\n",
      "Add of existing embedding ID: a088ce4d-0720-4228-a1e5-cb4b843e8efb\n",
      "Add of existing embedding ID: ba9060d7-db5c-4210-89d0-6c9d3d240528\n",
      "Add of existing embedding ID: 6cab1fd3-cc91-479c-8178-2073efc87d80\n",
      "Add of existing embedding ID: 43f8d46c-ee3a-4ef0-b49f-d5bfcfd0fb64\n",
      "Add of existing embedding ID: 9ec80a9f-c646-4fa9-a578-c83e574bde26\n",
      "Add of existing embedding ID: 11bb8a99-daa8-4431-a50f-c0dc711192a2\n",
      "Add of existing embedding ID: b4a6f619-2e9c-460e-a44d-6e502adf7b6c\n",
      "Add of existing embedding ID: 7bf5eb3a-81e0-490d-a95d-2b29f9cf1faf\n",
      "Add of existing embedding ID: df7b81d8-12d5-4b44-a754-4adbabd5544d\n",
      "Add of existing embedding ID: 1b80e0d2-5a87-4866-b847-77139222f729\n",
      "Add of existing embedding ID: 79c6678f-ad13-4954-9ca8-82d50acfe864\n",
      "Add of existing embedding ID: 9a61f1d8-39fb-4cb0-93bd-40dceb868c94\n",
      "Add of existing embedding ID: c0d152d8-21f8-432c-817a-5322766dbe52\n",
      "Add of existing embedding ID: 54ae2325-885e-45f7-838f-654dbc426c28\n",
      "Add of existing embedding ID: d8e6f904-f30a-429e-9fa4-e987be5d4968\n",
      "Add of existing embedding ID: c28e20b5-be86-4187-9c4c-48fb799fadf6\n",
      "Add of existing embedding ID: 4ec11899-0f56-4028-8055-e90e970ca20e\n",
      "Add of existing embedding ID: 912435a4-30f9-401e-901e-3ba75f86e968\n",
      "Add of existing embedding ID: 2b429c19-669f-448a-b5d7-f3a037e1164e\n",
      "Add of existing embedding ID: f8e286fd-8968-473b-b798-ebcc017219f6\n",
      "Add of existing embedding ID: ea1c1c8d-0c79-4ffb-9b2e-6c2ac70e01b1\n",
      "Add of existing embedding ID: 0ff6768c-7e57-4a73-a5bb-dbef9bc4f742\n",
      "Add of existing embedding ID: 73f8f26b-01aa-4cb0-9d18-a182971245a8\n",
      "Add of existing embedding ID: 15ef43b4-18cc-4c62-bf2f-988fb7b930b4\n",
      "Add of existing embedding ID: a7287e84-ef69-432f-8db6-be55bf6cfd0a\n",
      "Add of existing embedding ID: 9ecee273-4039-4319-b628-eea31d9265a6\n",
      "Add of existing embedding ID: 3ac26fdc-be1f-4b44-81ce-b3a1c074b949\n",
      "Add of existing embedding ID: 853c470c-351f-45bf-ba14-87cc6c8ff13d\n",
      "Add of existing embedding ID: bce79361-3283-435e-bf1d-efeb483556b4\n",
      "Add of existing embedding ID: 63a6e80b-f0b5-4805-bcaa-916e7dbbd630\n",
      "Add of existing embedding ID: a088ce4d-0720-4228-a1e5-cb4b843e8efb\n",
      "Add of existing embedding ID: 447b4e20-62cd-413c-9108-9bfffa1d995e\n",
      "Add of existing embedding ID: 7b15d1ce-ad9e-42f8-a958-d735e1a29885\n",
      "Add of existing embedding ID: 241e9b75-e4a7-47dd-92fc-b938294bfbd6\n",
      "Add of existing embedding ID: 1c4da214-09b0-4d54-898d-290c9f9644f7\n",
      "Add of existing embedding ID: 88a96c21-416d-4067-9a5f-77c09be0bfe3\n",
      "Add of existing embedding ID: cb40f0c6-2536-42e2-928e-08b8fa80651d\n",
      "Add of existing embedding ID: 0bd361f4-7ef0-4d78-8d61-6c069898d78f\n",
      "Add of existing embedding ID: 8cb88170-95cb-4f37-b3f3-1176ec1b7871\n",
      "Add of existing embedding ID: b7e68d91-ba42-4909-b3be-8fa3e781230a\n",
      "Add of existing embedding ID: 54bd364a-24f6-4dc6-9650-78caa7005724\n",
      "Add of existing embedding ID: b1b3a9b2-f2f4-440a-923f-fb1139f07cd9\n",
      "Add of existing embedding ID: 9ce3ccfd-0b97-4872-b7ed-314920dc3c7c\n",
      "Add of existing embedding ID: 5eb9eddb-cf02-45d6-b888-8580cb7fc17e\n",
      "Add of existing embedding ID: 0d4017a1-0ce2-4d79-a7c6-9acc920d2591\n",
      "Add of existing embedding ID: 01112d5d-5ce5-41d2-ad91-b1a5a95fee5f\n",
      "Add of existing embedding ID: cd093aa6-c9e8-47a3-99eb-e22e6a4df9db\n",
      "Add of existing embedding ID: cf5cc6d5-e6dd-4b00-a114-287d7846111a\n",
      "Add of existing embedding ID: 2e19e035-6d67-4f6d-9712-9ea1c7fa07d1\n",
      "Add of existing embedding ID: 0526af49-ebd9-419b-8098-69348433ba51\n",
      "Add of existing embedding ID: d3d14c24-b3a8-432b-86bf-c7a851d4acac\n",
      "Add of existing embedding ID: c7295928-3627-4d55-984d-811f6396b8ee\n",
      "Add of existing embedding ID: 2a3bf5b3-13ed-4ff2-a5e7-e49180655aa8\n",
      "Add of existing embedding ID: 56a9e061-bb6d-4ba1-814d-7f567a260483\n",
      "Add of existing embedding ID: 6d0e3328-5934-4a4c-9d58-43b098605745\n",
      "Add of existing embedding ID: 8622a557-67b7-4d46-bc45-64a8c022acbb\n",
      "Add of existing embedding ID: 6da2635f-0a6a-40c1-83cc-4959ef155132\n",
      "Add of existing embedding ID: da212291-5149-4658-8f71-ad3222bc11d5\n",
      "Add of existing embedding ID: 67583f86-66cc-4f21-ba46-58b2ed1e8177\n"
     ]
    }
   ],
   "source": [
    "#instantiate Vectorstore\n",
    "name = \"BERT_arxiv\"\n",
    "vector_index = VectorStoreIndex(nodes,storage_context=storage_context)\n",
    "vector_index.storage_context.vector_store.persist(persist_path=\"/content/chroma_db\")\n",
    "\n",
    "# Define Vectorstore Autoretrieval tool\n",
    "def vector_query(query:str,page_numbers:Optional[List[str]]=None)->str:\n",
    "  page_numbers = page_numbers or []\n",
    "  metadata_dict = [{\"key\":'page_label',\"value\":p} for p in page_numbers]\n",
    "  \n",
    "  query_engine = vector_index.as_query_engine(similarity_top_k =2, filters = MetadataFilters.from_dicts(metadata_dict, condition=FilterCondition.OR), llm=llm)\n",
    "  \n",
    "  response = query_engine.query(query)\n",
    "  return response\n",
    "\n",
    "#llamiondex FunctionTool wraps any python function we feed it\n",
    "vector_query_tool = FunctionTool.from_defaults(name=f\"vector_tool_{name}\", fn=vector_query)\n",
    "# Prepare Summary Tool\n",
    "summary_index = SummaryIndex(nodes)\n",
    "summary_query_engine = summary_index.as_query_engine(response_mode=\"tree_summarize\", se_async=True, llm=llm)\n",
    "summary_query_tool = QueryEngineTool.from_defaults(name=f\"summary_tool_{name}\", query_engine=summary_query_engine, description=(\"Use ONLY IF you want to get a holistic summary of the documents.\" \"DO NOT USE if you have specified questions over the documents.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_tool_BERT_arxiv with args: {\"query\": \"Summarize the content in page number 2\", \"page_numbers\": [\"2\"]}\n",
      "=== Function Output ===\n",
      "The page discusses the experimentation and results of a study on knowledge-intensive generation using MS-MARCO and Jeopardy question generation. The models used in the study generate responses that are more factual, specific, and diverse than a BART baseline. For FEVER fact verification, the results are within 4.3% of state-of-the-art pipeline models that use strong retrieval supervision. The study also demonstrates the ability to update the models' knowledge as the world changes by replacing the non-parametric memory. The methods explored in the study are called RAG models, which use an input sequence to retrieve text documents and use them as additional context when generating the target sequence. The models consist of two components: a retriever that returns distributions over text passages given a query, and a generator that is parametrized. The code to run experiments with RAG has been open-sourced as part of the HuggingFace Transformers Library, and an interactive demo of RAG models is available online.\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call([vector_query_tool], \"Summarize the content in page number 2\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_tools(file_path:str,name:str)->str:\n",
    "  #load documents\n",
    "  documents = SimpleDirectoryReader(input_files = [file_path]).load_data()\n",
    "  print(f\"length of nodes\")\n",
    "  splitter = SentenceSplitter(chunk_size=1024,chunk_overlap=100)\n",
    "  nodes = splitter.get_nodes_from_documents(documents)\n",
    "  print(f\"Length of nodes : {len(nodes)}\")\n",
    "  \n",
    "  #instantiate Vectorstore\n",
    "  vector_index = VectorStoreIndex(nodes,storage_context=storage_context)\n",
    "  vector_index.storage_context.vector_store.persist(persist_path=\"/content/chroma_db\")\n",
    "  \n",
    "  # Define Vectorstore Autoretrieval tool\n",
    "  def vector_query(query:str,page_numbers:Optional[List[str]]=None)->str:\n",
    "    page_numbers = page_numbers or []\n",
    "    metadata_dict = [{\"key\":'page_label',\"value\":p} for p in page_numbers]\n",
    "    \n",
    "    query_engine = vector_index.as_query_engine(similarity_top_k =2, filters = MetadataFilters.from_dicts(metadata_dict, condition=FilterCondition.OR), llm=llm)\n",
    "    \n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "  \n",
    "  #llamiondex FunctionTool wraps any python function we feed it\n",
    "  vector_query_tool = FunctionTool.from_defaults(name=f\"vector_tool_{name}\", fn=vector_query)\n",
    "  \n",
    "  # Prepare Summary Tool\n",
    "  summary_index = SummaryIndex(nodes)\n",
    "  summary_query_engine = summary_index.as_query_engine(response_mode=\"tree_summarize\", se_async=True, llm=llm)\n",
    "  summary_query_tool = QueryEngineTool.from_defaults(name=f\"summary_tool_{name}\",query_engine=summary_query_engine, description=(\"Use ONLY IF you want to get a holistic summary of the documents.\" \"DO NOT USE if you have specified questions over the documents.\"))\n",
    "  return vector_query_tool,summary_query_tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bert_pre_train', 'rag_nlp']\n",
      "['./data\\\\bert_pre_train.pdf', './data\\\\rag_nlp.pdf']\n"
     ]
    }
   ],
   "source": [
    "root_path = \"./data\"\n",
    "file_name = []\n",
    "file_path = []\n",
    "for file in os.listdir(root_path):\n",
    "  if file.endswith(\".pdf\"):\n",
    "    file_name.append(file.split(\".\")[0])\n",
    "    file_path.append(os.path.join(root_path,file))\n",
    "#\n",
    "print(file_name)\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of nodes\n",
      "Length of nodes : 28\n",
      "length of nodes\n",
      "Length of nodes : 30\n"
     ]
    }
   ],
   "source": [
    "papers_to_tools_dict = {}\n",
    "for name,filename in zip(file_name,file_path):\n",
    "  vector_query_tool,summary_query_tool = get_doc_tools(filename,name)\n",
    "  papers_to_tools_dict[name] = [vector_query_tool,summary_query_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<llama_index.core.tools.function_tool.FunctionTool at 0x17535525060>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x175355370d0>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x1753a5b95a0>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x1753a514a30>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tools = [t for f in file_name for t in papers_to_tools_dict[f]]\n",
    "initial_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_index = ObjectIndex.from_objects(initial_tools,index_cls=VectorStoreIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolMetadata(description='vector_tool_rag_nlp(query: str, page_numbers: Optional[List[str]] = None) -> str\\nNone', name='vector_tool_rag_nlp', fn_schema=<class 'pydantic.v1.main.vector_tool_rag_nlp'>, return_direct=False)\n",
      "ToolMetadata(description='Use ONLY IF you want to get a holistic summary of the documents.DO NOT USE if you have specified questions over the documents.', name='summary_tool_rag_nlp', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)\n"
     ]
    }
   ],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=2)\n",
    "tools = obj_retriever.retrieve(\"compare and contrast the papers self rag and corrective rag\")\n",
    "\n",
    "print(tools[0].metadata)\n",
    "print(tools[1].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the agent\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(tool_retriever=obj_retriever,\n",
    "                                                     llm=llm,\n",
    "                                                     system_prompt=\"\"\"You are an agent designed to answer queries over a set of given papers.\n",
    "                                                     Please always use the tools provided to answer a question.Do not rely on prior knowledge.\"\"\",\n",
    "                                                     verbose=True)\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: summarize rag for nlp\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_rag_nlp with args: {\"input\": \"rag for nlp\"}\n",
      "=== Function Output ===\n",
      "Retrieval-Augmented Generation (RAG) for NLP tasks is a method that combines pre-trained parametric and non-parametric memory for language generation. The parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. RAG models condition on the same retrieved passages across the whole generated sequence or use different passages per token. They are fine-tuned and evaluated on a wide range of knowledge-intensive NLP tasks, setting the state of the art on three open domain QA tasks and outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, RAG models generate more specific, diverse, and factual language than a state-of-the-art parametric-only seq2seq baseline.\n",
      "=== LLM Response ===\n",
      "Retrieval-Augmented Generation (RAG) for NLP tasks is a method that combines pre-trained parametric and non-parametric memory for language generation. The parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. RAG models condition on the same retrieved passages across the whole generated sequence or use different passages per token. They have been found to outperform parametric seq2seq models and task-specific retrieve-and-extract architectures on a variety of knowledge-intensive NLP tasks, setting the state of the art on three open domain QA tasks. Additionally, RAG models generate more specific, diverse, and factual language than a state-of-the-art parametric-only seq2seq baseline for language generation tasks.\n",
      "Retrieval-Augmented Generation (RAG) for NLP tasks is a method that combines pre-trained parametric and non-parametric memory for language generation. The parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. RAG models condition on the same retrieved passages across the whole generated sequence or use different passages per token. They have been found to outperform parametric seq2seq models and task-specific retrieve-and-extract architectures on a variety of knowledge-intensive NLP tasks, setting the state of the art on three open domain QA tasks. Additionally, RAG models generate more specific, diverse, and factual language than a state-of-the-art parametric-only seq2seq baseline for language generation tasks.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"summarize rag for nlp\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: what is a bidirectional transformer?\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_bert_pre_train with args: {\"query\": \"what is a bidirectional transformer?\"}\n",
      "=== Function Output ===\n",
      "A bidirectional transformer is a type of model that uses self-attention mechanisms to process input data in both directions. This is in contrast to other transformer models, like the one used in GPT, where each token can only attend to the context to its left. The bidirectional approach allows the model to consider the full context of a word, both before and after it, to better understand its meaning.\n",
      "=== LLM Response ===\n",
      "A bidirectional transformer is a type of model that uses self-attention mechanisms to process input data in both directions. This is in contrast to other transformer models, like the one used in GPT, where each token can only attend to the context to its left. The bidirectional approach allows the model to consider the full context of a word, both before and after it, to better understand its meaning.\n",
      "A bidirectional transformer is a type of model that uses self-attention mechanisms to process input data in both directions. This is in contrast to other transformer models, like the one used in GPT, where each token can only attend to the context to its left. The bidirectional approach allows the model to consider the full context of a word, both before and after it, to better understand its meaning.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"what is a bidirectional transformer?\")\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
