{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader,VectorStoreIndex,SummaryIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.tools import FunctionTool,QueryEngineTool\n",
    "from llama_index.core.vector_stores import MetadataFilters,FilterCondition\n",
    "from typing import List,Optional\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "userdata = {\n",
    "    \"MISTRAL_API_KEY\": \"BWdlihu9sUh5P2g3bHnzjAaHiT4anTVH\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "Document Metadata: {'page_label': '1', 'file_name': 'bert_pre_train.pdf', 'file_path': 'data\\\\bert_pre_train.pdf', 'file_type': 'application/pdf', 'file_size': 775166, 'creation_date': '2024-06-17', 'last_modified_date': '2024-06-17'}\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(input_files = ['./data/bert_pre_train.pdf']).load_data()\n",
    "print(len(documents))\n",
    "print(f\"Document Metadata: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of nodes : 28\n",
      "get the content for node 0 :page_label: 1\n",
      "file_name: bert_pre_train.pdf\n",
      "file_path: data\\bert_pre_train.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 775166\n",
      "creation_date: 2024-06-17\n",
      "last_modified_date: 2024-06-17\n",
      "\n",
      "BERT: Pre-training of Deep Bidirectional Transformers for\n",
      "Language Understanding\n",
      "Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\n",
      "Google AI Language\n",
      "{jacobdevlin,mingweichang,kentonl,kristout }@google.com\n",
      "Abstract\n",
      "We introduce a new language representa-\n",
      "tion model called BERT , which stands for\n",
      "Bidirectional Encoder Representations from\n",
      "Transformers. Unlike recent language repre-\n",
      "sentation models (Peters et al., 2018a; Rad-\n",
      "ford et al., 2018), BERT is designed to pre-\n",
      "train deep bidirectional representations from\n",
      "unlabeled text by jointly conditioning on both\n",
      "left and right context in all layers. As a re-\n",
      "sult, the pre-trained BERT model can be ﬁne-\n",
      "tuned with just one additional output layer\n",
      "to create state-of-the-art models for a wide\n",
      "range of tasks, such as question answering and\n",
      "language inference, without substantial task-\n",
      "speciﬁc architecture modiﬁcations.\n",
      "BERT is conceptually simple and empirically\n",
      "powerful. It obtains new state-of-the-art re-\n",
      "sults on eleven natural language processing\n",
      "tasks, including pushing the GLUE score to\n",
      "80.5% (7.7% point absolute improvement),\n",
      "MultiNLI accuracy to 86.7% (4.6% absolute\n",
      "improvement), SQuAD v1.1 question answer-\n",
      "ing Test F1 to 93.2 (1.5 point absolute im-\n",
      "provement) and SQuAD v2.0 Test F1 to 83.1\n",
      "(5.1 point absolute improvement).\n",
      "1 Introduction\n",
      "Language model pre-training has been shown to\n",
      "be effective for improving many natural language\n",
      "processing tasks (Dai and Le, 2015; Peters et al.,\n",
      "2018a; Radford et al., 2018; Howard and Ruder,\n",
      "2018). These include sentence-level tasks such as\n",
      "natural language inference (Bowman et al., 2015;\n",
      "Williams et al., 2018) and paraphrasing (Dolan\n",
      "and Brockett, 2005), which aim to predict the re-\n",
      "lationships between sentences by analyzing them\n",
      "holistically, as well as token-level tasks such as\n",
      "named entity recognition and question answering,\n",
      "where models are required to produce ﬁne-grained\n",
      "output at the token level (Tjong Kim Sang and\n",
      "De Meulder, 2003; Rajpurkar et al., 2016).There are two existing strategies for apply-\n",
      "ing pre-trained language representations to down-\n",
      "stream tasks: feature-based andﬁne-tuning . The\n",
      "feature-based approach, such as ELMo (Peters\n",
      "et al., 2018a), uses task-speciﬁc architectures that\n",
      "include the pre-trained representations as addi-\n",
      "tional features. The ﬁne-tuning approach, such as\n",
      "the Generative Pre-trained Transformer (OpenAI\n",
      "GPT) (Radford et al., 2018), introduces minimal\n",
      "task-speciﬁc parameters, and is trained on the\n",
      "downstream tasks by simply ﬁne-tuning allpre-\n",
      "trained parameters. The two approaches share the\n",
      "same objective function during pre-training, where\n",
      "they use unidirectional language models to learn\n",
      "general language representations.\n",
      "We argue that current techniques restrict the\n",
      "power of the pre-trained representations, espe-\n",
      "cially for the ﬁne-tuning approaches. The ma-\n",
      "jor limitation is that standard language models are\n",
      "unidirectional, and this limits the choice of archi-\n",
      "tectures that can be used during pre-training. For\n",
      "example, in OpenAI GPT, the authors use a left-to-\n",
      "right architecture, where every token can only at-\n",
      "tend to previous tokens in the self-attention layers\n",
      "of the Transformer (Vaswani et al., 2017). Such re-\n",
      "strictions are sub-optimal for sentence-level tasks,\n",
      "and could be very harmful when applying ﬁne-\n",
      "tuning based approaches to token-level tasks such\n",
      "as question answering, where it is crucial to incor-\n",
      "porate context from both directions.\n",
      "In this paper, we improve the ﬁne-tuning based\n",
      "approaches by proposing BERT: Bidirectional\n",
      "Encoder Representations from Transformers.\n",
      "BERT alleviates the previously mentioned unidi-\n",
      "rectionality constraint by using a “masked lan-\n",
      "guage model” (MLM) pre-training objective, in-\n",
      "spired by the Cloze task (Taylor, 1953). The\n",
      "masked language model randomly masks some of\n",
      "the tokens from the input, and the objective is to\n",
      "predict the original vocabulary id of the maskedarXiv:1810.04805v2  [cs.CL]  24 May 2019\n"
     ]
    }
   ],
   "source": [
    "splitter = SentenceSplitter(chunk_size=1024,chunk_overlap=100)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "print(f\"Length of nodes : {len(nodes)}\")\n",
    "print(f\"get the content for node 0 :{nodes[0].get_content(metadata_mode='all')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector store\n",
    "import chromadb\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db_mistral\")\n",
    "chroma_collection = db.get_or_create_collection(\"multidocument-agent\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda_envs\\rag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# embedding model\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from llama_index.core import Settings\n",
    "embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the llama index\n",
    "os.environ[\"MISTRAL_API_KEY\"] = userdata.get(\"MISTRAL_API_KEY\")\n",
    "llm = MistralAI(model=\"mistral-large-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: e6c4a4e7-c5fe-4d99-a678-19405604679c\n",
      "Insert of existing embedding ID: 9df84515-3c77-469e-a0c5-690dc8337528\n",
      "Insert of existing embedding ID: a2a1e1e1-8971-45d4-8176-31823e5b28e7\n",
      "Insert of existing embedding ID: 6da5e357-baad-48f1-a8a0-8d296780f93a\n",
      "Insert of existing embedding ID: 91338de9-b89c-48e8-8a82-97fea20eaa74\n",
      "Insert of existing embedding ID: 4c71ef69-cf5f-4585-b0ac-59d352c59169\n",
      "Insert of existing embedding ID: 54b3be6a-c298-47de-9a0b-ec7e61636cc7\n",
      "Insert of existing embedding ID: 41d64240-b3a6-4287-884e-d4776b583c78\n",
      "Insert of existing embedding ID: 6dfe39c5-323e-4ce9-902a-d4319dd45e1e\n",
      "Insert of existing embedding ID: 00f6489a-db31-4852-9b3a-fcc6d8140696\n",
      "Insert of existing embedding ID: 69059688-2592-4001-b7c2-54177fd17c36\n",
      "Insert of existing embedding ID: 9d6b83dd-710b-4236-b157-dcf1d74d1766\n",
      "Insert of existing embedding ID: e703498a-a48d-4811-9fcf-097880d9dafa\n",
      "Insert of existing embedding ID: 476d1cbb-c5b8-43e2-864e-6cbdc73aca30\n",
      "Insert of existing embedding ID: aeaec200-c7c6-48f8-8f51-b8fd91acc29a\n",
      "Insert of existing embedding ID: cb4523ab-a47f-4b27-9cff-47ae7062dcef\n",
      "Insert of existing embedding ID: eab62e4b-0d08-4a52-945f-d5fc063bb3c9\n",
      "Insert of existing embedding ID: a92646c8-19c7-45e6-9b06-8e876dfde009\n",
      "Insert of existing embedding ID: b81d54bf-5ee0-459c-865a-970aeb7cb582\n",
      "Insert of existing embedding ID: 36ee07fd-c7f5-4341-9bca-5dea1c30a661\n",
      "Insert of existing embedding ID: 47824c56-71e9-48f3-8c61-3ef033e33ec3\n",
      "Insert of existing embedding ID: 4f89e7b4-2165-45b0-bbc8-42a0a6ea6a0b\n",
      "Insert of existing embedding ID: df8fbbe2-9c67-4dfb-b106-f10530d6f0ef\n",
      "Insert of existing embedding ID: b04fe45c-1d24-4cd2-a648-58438ec29565\n",
      "Insert of existing embedding ID: 7e464bd9-cbd3-4017-8e69-f5fbcf8a0e37\n",
      "Insert of existing embedding ID: 93f39fb2-6652-4ae2-bdc9-6228abe8ba02\n",
      "Insert of existing embedding ID: 6020a48e-f159-4125-ac5d-dc186837e845\n",
      "Insert of existing embedding ID: 8a162516-0e25-4952-a4f2-90fbeb51cb7f\n",
      "Add of existing embedding ID: e6c4a4e7-c5fe-4d99-a678-19405604679c\n",
      "Add of existing embedding ID: 9df84515-3c77-469e-a0c5-690dc8337528\n",
      "Add of existing embedding ID: a2a1e1e1-8971-45d4-8176-31823e5b28e7\n",
      "Add of existing embedding ID: 6da5e357-baad-48f1-a8a0-8d296780f93a\n",
      "Add of existing embedding ID: 91338de9-b89c-48e8-8a82-97fea20eaa74\n",
      "Add of existing embedding ID: 4c71ef69-cf5f-4585-b0ac-59d352c59169\n",
      "Add of existing embedding ID: 54b3be6a-c298-47de-9a0b-ec7e61636cc7\n",
      "Add of existing embedding ID: 41d64240-b3a6-4287-884e-d4776b583c78\n",
      "Add of existing embedding ID: 6dfe39c5-323e-4ce9-902a-d4319dd45e1e\n",
      "Add of existing embedding ID: 00f6489a-db31-4852-9b3a-fcc6d8140696\n",
      "Add of existing embedding ID: 69059688-2592-4001-b7c2-54177fd17c36\n",
      "Add of existing embedding ID: 9d6b83dd-710b-4236-b157-dcf1d74d1766\n",
      "Add of existing embedding ID: e703498a-a48d-4811-9fcf-097880d9dafa\n",
      "Add of existing embedding ID: 476d1cbb-c5b8-43e2-864e-6cbdc73aca30\n",
      "Add of existing embedding ID: aeaec200-c7c6-48f8-8f51-b8fd91acc29a\n",
      "Add of existing embedding ID: cb4523ab-a47f-4b27-9cff-47ae7062dcef\n",
      "Add of existing embedding ID: eab62e4b-0d08-4a52-945f-d5fc063bb3c9\n",
      "Add of existing embedding ID: a92646c8-19c7-45e6-9b06-8e876dfde009\n",
      "Add of existing embedding ID: b81d54bf-5ee0-459c-865a-970aeb7cb582\n",
      "Add of existing embedding ID: 36ee07fd-c7f5-4341-9bca-5dea1c30a661\n",
      "Add of existing embedding ID: 47824c56-71e9-48f3-8c61-3ef033e33ec3\n",
      "Add of existing embedding ID: 4f89e7b4-2165-45b0-bbc8-42a0a6ea6a0b\n",
      "Add of existing embedding ID: df8fbbe2-9c67-4dfb-b106-f10530d6f0ef\n",
      "Add of existing embedding ID: b04fe45c-1d24-4cd2-a648-58438ec29565\n",
      "Add of existing embedding ID: 7e464bd9-cbd3-4017-8e69-f5fbcf8a0e37\n",
      "Add of existing embedding ID: 93f39fb2-6652-4ae2-bdc9-6228abe8ba02\n",
      "Add of existing embedding ID: 6020a48e-f159-4125-ac5d-dc186837e845\n",
      "Add of existing embedding ID: 8a162516-0e25-4952-a4f2-90fbeb51cb7f\n"
     ]
    }
   ],
   "source": [
    "#instantiate Vectorstore\n",
    "name = \"BERT\"\n",
    "vector_index = VectorStoreIndex(nodes,storage_context=storage_context)\n",
    "vector_index.storage_context.vector_store.persist(persist_path=\"/content/chroma_db\")\n",
    "\n",
    "# Define Vectorstore Autoretrieval tool\n",
    "def vector_query(query:str,page_numbers:Optional[List[str]]=None)->str:\n",
    "  '''\n",
    "  perform vector search over index on\n",
    "  query(str): query string needs to be embedded\n",
    "  page_numbers(List[str]): list of page numbers to be retrieved,\n",
    "  leave blank if we want to perform a vector search over all pages\n",
    "  '''\n",
    "  page_numbers = page_numbers or []\n",
    "  metadata_dict = [{\"key\":'page_label',\"value\":p} for p in page_numbers]\n",
    "  \n",
    "  query_engine = vector_index.as_query_engine(similarity_top_k =2, filters = MetadataFilters.from_dicts(metadata_dict, condition=FilterCondition.OR), llm=llm)\n",
    "  \n",
    "  response = query_engine.query(query)\n",
    "  return response\n",
    "\n",
    "#llamiondex FunctionTool wraps any python function we feed it\n",
    "vector_query_tool = FunctionTool.from_defaults(name=f\"vector_tool_{name}\", fn=vector_query)\n",
    "# Prepare Summary Tool\n",
    "summary_index = SummaryIndex(nodes)\n",
    "summary_query_engine = summary_index.as_query_engine(response_mode=\"tree_summarize\", se_async=True, llm=llm)\n",
    "summary_query_tool = QueryEngineTool.from_defaults(name=f\"summary_tool_{name}\", query_engine=summary_query_engine, description=(\"Use ONLY IF you want to get a holistic summary of the documents.\" \"DO NOT USE if you have specified questions over the documents.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_tool_BERT with args: {\"query\": \"summarize the content\", \"page_numbers\": [\"2\"]}\n",
      "=== Function Output ===\n",
      "The content discusses experiments conducted for knowledge-intensive generation using MS-MARCO and Jeopardy question generation. The results show that the models used in these experiments generate responses that are more factual, specific, and diverse compared to a BART baseline. For FEVER fact verification, the outcomes are within 4.3% of state-of-the-art pipeline models that use strong retrieval supervision. The text also mentions the ability to update the models' knowledge as the world changes by replacing the non-parametric memory.\n",
      "\n",
      "The methodology explores RAG models that utilize an input sequence to retrieve text documents and use them as additional context when generating the target sequence. These models consist of two main components: a retriever that returns distributions over text passages given a query, and a generator that is parametrized to generate output based on the query and retrieved documents. The code for running experiments with RAG has been open-sourced as part of the HuggingFace Transformers Library, and an interactive demo of RAG models is available online.\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call([vector_query_tool], \"Summarize the content in page number 2\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for setting vector and summary tool\n",
    "def get_doc_tools(file_path:str,name:str)->str:\n",
    "  '''\n",
    "  get vector query and sumnmary query tools from a document\n",
    "  '''\n",
    "  #load documents\n",
    "  documents = SimpleDirectoryReader(input_files = [file_path]).load_data()\n",
    "  print(f\"length of nodes\")\n",
    "  splitter = SentenceSplitter(chunk_size=1024,chunk_overlap=100)\n",
    "  nodes = splitter.get_nodes_from_documents(documents)\n",
    "  print(f\"Length of nodes : {len(nodes)}\")\n",
    "  \n",
    "  #instantiate Vectorstore\n",
    "  vector_index = VectorStoreIndex(nodes,storage_context=storage_context)\n",
    "  vector_index.storage_context.vector_store.persist(persist_path=\"/content/chroma_db\")\n",
    "  \n",
    "  # Define Vectorstore Autoretrieval tool\n",
    "  def vector_query(query:str,page_numbers:Optional[List[str]]=None)->str:\n",
    "    '''\n",
    "    perform vector search over index on\n",
    "    query(str): query string needs to be embedded\n",
    "    page_numbers(List[str]): list of page numbers to be retrieved,\n",
    "    leave blank if we want to perform a vector search over all pages\n",
    "    '''\n",
    "    page_numbers = page_numbers or []\n",
    "    metadata_dict = [{\"key\":'page_label',\"value\":p} for p in page_numbers]\n",
    "    \n",
    "    query_engine = vector_index.as_query_engine(similarity_top_k =2, filters = MetadataFilters.from_dicts(metadata_dict, condition=FilterCondition.OR), llm=llm)\n",
    "    \n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "  \n",
    "  #llamiondex FunctionTool wraps any python function we feed it\n",
    "  vector_query_tool = FunctionTool.from_defaults(name=f\"vector_tool_{name}\", fn=vector_query)\n",
    "  \n",
    "  # Prepare Summary Tool\n",
    "  summary_index = SummaryIndex(nodes)\n",
    "  summary_query_engine = summary_index.as_query_engine(response_mode=\"tree_summarize\", se_async=True, llm=llm)\n",
    "  summary_query_tool = QueryEngineTool.from_defaults(name=f\"summary_tool_{name}\",query_engine=summary_query_engine, description=(\"Use ONLY IF you want to get a holistic summary of the documents.\" \"DO NOT USE if you have specified questions over the documents.\"))\n",
    "  return vector_query_tool,summary_query_tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bert_pre_train', 'corrective_rag', 'rag_nlp', 'self_rag']\n",
      "['./data\\\\bert_pre_train.pdf', './data\\\\corrective_rag.pdf', './data\\\\rag_nlp.pdf', './data\\\\self_rag.pdf']\n"
     ]
    }
   ],
   "source": [
    "root_path = \"./data\"\n",
    "file_name = []\n",
    "file_path = []\n",
    "for file in os.listdir(root_path):\n",
    "  if file.endswith(\".pdf\"):\n",
    "    file_name.append(file.split(\".\")[0])\n",
    "    file_path.append(os.path.join(root_path,file))\n",
    "#\n",
    "print(file_name)\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of nodes\n",
      "Length of nodes : 28\n",
      "length of nodes\n",
      "Length of nodes : 22\n",
      "length of nodes\n",
      "Length of nodes : 30\n",
      "length of nodes\n",
      "Length of nodes : 43\n"
     ]
    }
   ],
   "source": [
    "papers_to_tools_dict = {}\n",
    "for name,filename in zip(file_name,file_path):\n",
    "  vector_query_tool,summary_query_tool = get_doc_tools(filename,name)\n",
    "  papers_to_tools_dict[name] = [vector_query_tool,summary_query_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<llama_index.core.tools.function_tool.FunctionTool at 0x25316002d40>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x25316003400>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x25315b3fe80>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x25315b3eaa0>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x2531bd0a1a0>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x2531be98550>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x2531bd0a890>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x253178de2c0>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tools = [t for f in file_name for t in papers_to_tools_dict[f]]\n",
    "initial_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_index = ObjectIndex.from_objects(initial_tools,index_cls=VectorStoreIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolMetadata(description='vector_tool_corrective_rag(query: str, page_numbers: Optional[List[str]] = None) -> str\\n\\n    perform vector search over index on\\n    query(str): query string needs to be embedded\\n    page_numbers(List[str]): list of page numbers to be retrieved,\\n    leave blank if we want to perform a vector search over all pages\\n    ', name='vector_tool_corrective_rag', fn_schema=<class 'pydantic.v1.main.vector_tool_corrective_rag'>, return_direct=False)\n",
      "ToolMetadata(description='Use ONLY IF you want to get a holistic summary of the documents.DO NOT USE if you have specified questions over the documents.', name='summary_tool_corrective_rag', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)\n"
     ]
    }
   ],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=2)\n",
    "tools = obj_retriever.retrieve(\"compare and contrast the papers self rag and corrective rag\")\n",
    "\n",
    "print(tools[0].metadata)\n",
    "print(tools[1].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the agent\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(tool_retriever=obj_retriever,\n",
    "                                                     llm=llm,\n",
    "                                                     system_prompt=\"\"\"You are an agent designed to answer queries over a set of given papers.\n",
    "                                                     Please always use the tools provided to answer a question.Do not rely on prior knowledge.\"\"\",\n",
    "                                                     verbose=True)\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: summarize rag for nlp\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_rag_nlp with args: {\"input\": \"rag for nlp\"}\n",
      "=== Function Output ===\n",
      "Retrieval-Augmented Generation (RAG) for Knowledge-Intensive NLP Tasks is a method that combines pre-trained parametric and non-parametric memory for language generation. The parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. RAG models are fine-tuned and evaluated on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.\n",
      "=== LLM Response ===\n",
      "Retrieval-Augmented Generation (RAG) is a method used for knowledge-intensive NLP tasks. It combines pre-trained parametric and non-parametric memory for language generation. The parametric memory is a pre-trained seq2seq model, while the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. RAG models have been fine-tuned and evaluated on various knowledge-intensive NLP tasks, setting the state of the art on three open domain QA tasks. They outperform parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, RAG models generate more specific, diverse, and factual language than a state-of-the-art parametric-only seq2seq baseline.\n",
      "Retrieval-Augmented Generation (RAG) is a method used for knowledge-intensive NLP tasks. It combines pre-trained parametric and non-parametric memory for language generation. The parametric memory is a pre-trained seq2seq model, while the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. RAG models have been fine-tuned and evaluated on various knowledge-intensive NLP tasks, setting the state of the art on three open domain QA tasks. They outperform parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, RAG models generate more specific, diverse, and factual language than a state-of-the-art parametric-only seq2seq baseline.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"summarize rag for nlp\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: what is a bidirectional transformer?how are they trained for language understanding?\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_bert_pre_train with args: {\"query\": \"bidirectional transformer\", \"page_numbers\": []}\n",
      "=== Function Output ===\n",
      "The bidirectional transformer is a model used in the BERT (Bidirectional Encoder Representations from Transformers) system. Unlike other models such as OpenAI GPT, which use a left-to-right transformer or a constrained self-attention where every token can only attend to context to its left, the bidirectional transformer allows every token to attend to context from both left and right. This makes the BERT model unique as its representations are jointly conditioned on both left and right context in all layers.\n",
      "=== LLM Response ===\n",
      "A bidirectional transformer is a model used in the BERT (Bidirectional Encoder Representations from Transformers) system. Unlike other models such as OpenAI GPT, which use a left-to-right transformer or a constrained self-attention where every token can only attend to context to its left, the bidirectional transformer allows every token to attend to context from both left and right. This makes the BERT model unique as its representations are jointly conditioned on both left and right context in all layers.\n",
      "\n",
      "As for how they are trained for language understanding, BERT models are trained using two strategies: Masked Language Model (MLM) and Next Sentence Prediction (NSP). In MLM, some tokens in the input are randomly masked and the model is trained to predict those masked tokens. This helps the model to understand the context and semantics of the words. In NSP, the model is given two sentences and it has to predict whether the second sentence follows the first one in the actual corpus. This helps the model to understand the relationship between sentences.\n",
      "A bidirectional transformer is a model used in the BERT (Bidirectional Encoder Representations from Transformers) system. Unlike other models such as OpenAI GPT, which use a left-to-right transformer or a constrained self-attention where every token can only attend to context to its left, the bidirectional transformer allows every token to attend to context from both left and right. This makes the BERT model unique as its representations are jointly conditioned on both left and right context in all layers.\n",
      "\n",
      "As for how they are trained for language understanding, BERT models are trained using two strategies: Masked Language Model (MLM) and Next Sentence Prediction (NSP). In MLM, some tokens in the input are randomly masked and the model is trained to predict those masked tokens. This helps the model to understand the context and semantics of the words. In NSP, the model is given two sentences and it has to predict whether the second sentence follows the first one in the actual corpus. This helps the model to understand the relationship between sentences.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"what is a bidirectional transformer?\"\n",
    "                       \"how are they trained for language understanding?\")\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
