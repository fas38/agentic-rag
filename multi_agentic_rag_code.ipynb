{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda_envs\\rag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List,Optional\n",
    "\n",
    "# llama index imports\n",
    "from llama_index.core import SimpleDirectoryReader,VectorStoreIndex,SummaryIndex, StorageContext, Settings, load_index_from_storage\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.node_parser import SentenceSplitter, CodeSplitter, LangchainNodeParser\n",
    "from llama_index.core.tools import FunctionTool,QueryEngineTool\n",
    "from llama_index.core.vector_stores import MetadataFilters,FilterCondition\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "from llama_index.readers.file import IPYNBReader\n",
    "\n",
    "# llama index agent imports\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "# llama index llms and embeddings imports\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "# tools\n",
    "import nest_asyncio # to allow running async functions in jupyter\n",
    "import chromadb # persistent storage for vectors\n",
    "# import nbconvert\n",
    "import tree_sitter\n",
    "import tree_sitter_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply() # to allow running async functions in jupyter\n",
    "\n",
    "# setting flags\n",
    "create_index = True\n",
    "\n",
    "# configuration\n",
    "MISTRAL_API_KEY =  \"BWdlihu9sUh5P2g3bHnzjAaHiT4anTVH\"\n",
    "embedding = \"BAAI/bge-small-en-v1.5\"\n",
    "# embedding = \"Qdrant/bm42-all-minilm-l6-v2-attentions\"\n",
    "# embedding = \"mistral-embed\"\n",
    "# embedding = OllamaEmbedding(\n",
    "#     model_name=\"llama2\",\n",
    "#     base_url=\"http://localhost:11434\",\n",
    "#     ollama_additional_kwargs={\"mirostat\": 0} \n",
    "# )\n",
    "# embedding = \"Salesforce/codet5p-110m-embedding\"\n",
    "llm_model = \"mistral-large-latest\"\n",
    "# llm_model = \"codellama\"\n",
    "chunk_size = 2000 # number of lines\n",
    "chunk_overlap = 200 # number of lines to overlap between chunks\n",
    "language = \"python\"\n",
    "data_path = \"./data_python\"\n",
    "\n",
    "# setup the llm and embedding\n",
    "embed_model = FastEmbedEmbedding(model_name=embedding)\n",
    "# embed_model = MistralAIEmbedding(model_name=embedding, api_key=MISTRAL_API_KEY)\n",
    "# embed_model =  HuggingFaceEmbedding(model_name=embedding)  \n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = chunk_size\n",
    "Settings.chunk_overlap = chunk_overlap\n",
    "os.environ[\"MISTRAL_API_KEY\"] = MISTRAL_API_KEY\n",
    "llm = MistralAI(model=llm_model)\n",
    "# llm = Ollama(model=llm_model, request_timeout=60.0)\n",
    "Settings.llm = llm\n",
    "\n",
    "# setup the persistent storage for vector store\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db_mistral_python\")\n",
    "chroma_collection = db.get_or_create_collection(\"code-agent\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for investigating node outputs\n",
    "\n",
    "# #load documents\n",
    "# file_path = \"./data_python/tabular_classification_binary.ipynb\"\n",
    "# documents = SimpleDirectoryReader(input_files = [file_path]).load_data()\n",
    "# print(f\"length of nodes\")\n",
    "# # splitter = CodeSplitter(language=\"python\", chunk_lines=chunk_size, chunk_lines_overlap=chunk_overlap, max_chars=max_chars)\n",
    "# splitter = LangchainNodeParser(RecursiveCharacterTextSplitter().from_language(Language.PYTHON, chunk_size=2000, chunk_overlap=0))\n",
    "# nodes = splitter.get_nodes_from_documents(documents)\n",
    "# print(f\"Length of nodes : {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for setting vector and summary tool from a document by creating new vector and summary index\n",
    "def get_doc_tools(file_path:str, name:str) -> str:\n",
    "  \"\"\"Get vector query and summary query tools from a jupyter notebook.\"\"\"\n",
    "  \n",
    "  #load documents\n",
    "  documents = SimpleDirectoryReader(input_files = [file_path]).load_data()\n",
    "  print(f\"length of nodes\")\n",
    "  # splitter = CodeSplitter(language=\"python\", chunk_lines=chunk_size, chunk_lines_overlap=chunk_overlap, max_chars=max_chars)\n",
    "  splitter = LangchainNodeParser(RecursiveCharacterTextSplitter().from_language(Language.PYTHON, chunk_size=chunk_size, chunk_overlap=chunk_overlap))\n",
    "  nodes = splitter.get_nodes_from_documents(documents)\n",
    "  print(f\"Length of nodes : {len(nodes)}\")\n",
    "  \n",
    "  #instantiate Vector store\n",
    "  vector_index = VectorStoreIndex(nodes,storage_context=storage_context)\n",
    "  vector_index.storage_context.vector_store.persist(persist_path=\"/content/chroma_db\")\n",
    "  \n",
    "  # Vector store Auto retrieval query engine method\n",
    "  def vector_query(query:str) -> str:\n",
    "    \"\"\"\n",
    "    query (str): the string query to be embedded\n",
    "    \"\"\"\n",
    "    query_engine = vector_index.as_query_engine(similarity_top_k =2) # set vector query engine with similarity as top 2 results\n",
    "    \n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "  \n",
    "  # Prepare Vector Tool\n",
    "  vector_query_tool = FunctionTool.from_defaults(name=f\"vector_tool_{name}\", fn=vector_query)\n",
    "  \n",
    "  # Prepare Summary Tool\n",
    "  summary_index = SummaryIndex(nodes)\n",
    "  summary_index.storage_context.persist(persist_dir=\"./db_mistral_python\") # save the summary index to disk\n",
    "  summary_query_engine = summary_index.as_query_engine(response_mode=\"tree_summarize\", use_async=True) # set summary query engine with tree summarization\n",
    "  summary_query_tool = QueryEngineTool.from_defaults(name=f\"summary_tool_{name}\",query_engine=summary_query_engine, description=(\"Use ONLY IF you want to get a holistic approach for full implementation\" \"DO NOT USE if you have question for specific implementation.\")) # set summary query tool with prompt\n",
    "  return vector_query_tool,summary_query_tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for setting vector and summary tool from a document by loading vector and summary index from storage\n",
    "def get_doc_tools_from_storage(file_path:str, name:str) -> str:\n",
    "  \"\"\"Get vector query and summary query tools from a document.\"\"\"\n",
    "  \n",
    "  #load vector store\n",
    "  vector_index = VectorStoreIndex.from_vector_store(vector_store=vector_store, storage_context=storage_context)\n",
    "  \n",
    "  # Vector store Auto retrieval query engine method\n",
    "  def vector_query(query:str, page_numbers:Optional[List[str]] = None) -> str:\n",
    "    \"\"\"\n",
    "    query (str): the string query to be embedded\n",
    "    page_numbers Optional[List[str]]: List of page numbers to be retrieved.\n",
    "    Leave as NONE if we want to perform a vector search over all pages. \n",
    "    Otherwise, filter by the set of specified pages.\n",
    "    Always leave page_numbers as None UNLESS there is a specific page you want to search for.\n",
    "    \"\"\"\n",
    "    page_numbers = page_numbers or []\n",
    "    metadata_dict = [{\"key\":'page_label', \"value\":p} for p in page_numbers]\n",
    "    \n",
    "    query_engine = vector_index.as_query_engine(similarity_top_k =2, filters = MetadataFilters.from_dicts(metadata_dict, condition=FilterCondition.OR)) # set vector query engine with similarity as top 2 results\n",
    "    \n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "  \n",
    "  # Prepare Vector Tool\n",
    "  vector_query_tool = FunctionTool.from_defaults(name=f\"vector_tool_{name}\", fn=vector_query)\n",
    "  \n",
    "  # Prepare Summary Tool\n",
    "  storage_context_all = StorageContext.from_defaults(persist_dir=\"./db_mistral_python\") # set storage context for summary index\n",
    "  summary_index = load_index_from_storage(storage_context=storage_context_all) # load summary index from storage\n",
    "  summary_query_engine = summary_index.as_query_engine(response_mode=\"tree_summarize\", use_async=True) # set summary query engine with tree summarization\n",
    "  summary_query_tool = QueryEngineTool.from_defaults(name=f\"summary_tool_{name}\",query_engine=summary_query_engine, description=(\"Use ONLY IF you want to get a holistic summary of the documents.\" \"DO NOT USE if you have specified questions over the documents.\")) # set summary query tool with prompt\n",
    "  return vector_query_tool,summary_query_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of vector and summary tools for all documents in the path\n",
    "def get_doc_tools_from_path(path:str) -> list:\n",
    "  file_name = []\n",
    "  file_path = []\n",
    "  for file in os.listdir(path):\n",
    "    if file.endswith(\".ipynb\"):\n",
    "      file_name.append(file.split(\".\")[0])\n",
    "      file_path.append(os.path.join(path,file))\n",
    "\n",
    "  papers_to_tools_dict = {}\n",
    "  for name,filename in zip(file_name,file_path):\n",
    "    if create_index:\n",
    "      vector_query_tool,summary_query_tool = get_doc_tools(filename,name)\n",
    "    else:\n",
    "      vector_query_tool,summary_query_tool = get_doc_tools_from_storage(filename,name)\n",
    "    papers_to_tools_dict[name] = [vector_query_tool,summary_query_tool]\n",
    "\n",
    "  initial_tools = [t for f in file_name for t in papers_to_tools_dict[f]]\n",
    "  return initial_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of nodes\n",
      "Length of nodes : 132\n"
     ]
    }
   ],
   "source": [
    "# create object index from the list of tools\n",
    "initial_tools_for_data = get_doc_tools_from_path(data_path)\n",
    "obj_index = ObjectIndex.from_objects(initial_tools_for_data, index_cls=VectorStoreIndex)\n",
    "obj_retriever = obj_index.as_retriever(similarity_top_k=2) # set object retriever with similarity as top 2 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup single agent\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(tool_retriever=obj_retriever, \n",
    "                                                     llm=llm, \n",
    "                                                     system_prompt=\"\"\"You are an agent designed to answer queries over a given jupyter notebook. Please always use the tools provided to answer a question.Do not rely on prior knowledge.\"\"\", \n",
    "                                                     verbose=True) \n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: how to train xgboost model for sepsis dataset?\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_tabular_classification_binary with args: {\"input\": \"how to train xgboost model for sepsis dataset?\"}\n",
      "=== Function Output ===\n",
      "To train an XGBoost model for the Sepsis dataset, you would first need to load and preprocess your data. This might involve handling missing values, one-hot encoding categorical variables, and scaling numerical features. Once your data is prepared, you can split it into training and testing sets.\n",
      "\n",
      "Next, you would initialize the XGBoost classifier. In the provided context, the classifier is initialized with the following parameters: `iterations=100, learning_rate=0.01, depth=7, verbose=1`.\n",
      "\n",
      "After initializing the classifier, you can fit the model to your training data using the `fit` method: `clf.fit(X_train_proper, y_train_proper)`.\n",
      "\n",
      "Once the model is trained, you can make predictions on your test data using the `predict` method: `predictions = clf.predict(X_test)`.\n",
      "\n",
      "Finally, you can evaluate the performance of your model using appropriate metrics such as ROC AUC and Average Precision. In the provided context, these are calculated using the `roc_auc_score` and `average_precision_score` functions from the `sklearn.metrics` module.\n",
      "\n",
      "Remember, the specific parameters and preprocessing steps might vary depending on the specific characteristics of your dataset and the problem at hand. It's always a good idea to experiment with different parameters and evaluate your model's performance to ensure you're getting the best results possible.\n",
      "\n",
      "Here is a simplified example:\n",
      "\n",
      "```python\n",
      "# Load and preprocess data\n",
      "# ...\n",
      "\n",
      "# Split data into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
      "\n",
      "# Initialize XGBoost classifier\n",
      "clf = XGBClassifier(iterations=100, learning_rate=0.01, depth=7, verbose=1)\n",
      "\n",
      "# Train the model\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "# Make predictions\n",
      "predictions = clf.predict(X_test)\n",
      "\n",
      "# Evaluate the model\n",
      "roc_auc = roc_auc_score(y_test,\n",
      "=== LLM Response ===\n",
      "To train an XGBoost model for the Sepsis dataset, you would first need to load and preprocess your data. This might involve handling missing values, one-hot encoding categorical variables, and scaling numerical features. Once your data is prepared, you can split it into training and testing sets.\n",
      "\n",
      "Next, you would initialize the XGBoost classifier. In the provided context, the classifier is initialized with the following parameters: `iterations=100, learning_rate=0.01, depth=7, verbose=1`.\n",
      "\n",
      "After initializing the classifier, you can fit the model to your training data using the `fit` method: `clf.fit(X_train_proper, y_train_proper)`.\n",
      "\n",
      "Once the model is trained, you can make predictions on your test data using the `predict` method: `predictions = clf.predict(X_test)`.\n",
      "\n",
      "Finally, you can evaluate the performance of your model using appropriate metrics such as ROC AUC and Average Precision. In the provided context, these are calculated using the `roc_auc_score` and `average_precision_score` functions from the `sklearn.metrics` module.\n",
      "\n",
      "Remember, the specific parameters and preprocessing steps might vary depending on the specific characteristics of your dataset and the problem at hand. It's always a good idea to experiment with different parameters and evaluate your model's performance to ensure you're getting the best results possible.\n",
      "\n",
      "Here is a simplified example:\n",
      "\n",
      "```python\n",
      "# Load and preprocess data\n",
      "# ...\n",
      "\n",
      "# Split data into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
      "\n",
      "# Initialize XGBoost classifier\n",
      "clf = XGBClassifier(iterations=100, learning_rate=0.01, depth=7, verbose=1)\n",
      "\n",
      "# Train the model\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "# Make predictions\n",
      "predictions = clf.predict(X_test)\n",
      "\n",
      "# Evaluate the model\n",
      "roc_auc = roc_auc_score(y_test,\n",
      "To train an XGBoost model for the Sepsis dataset, you would first need to load and preprocess your data. This might involve handling missing values, one-hot encoding categorical variables, and scaling numerical features. Once your data is prepared, you can split it into training and testing sets.\n",
      "\n",
      "Next, you would initialize the XGBoost classifier. In the provided context, the classifier is initialized with the following parameters: `iterations=100, learning_rate=0.01, depth=7, verbose=1`.\n",
      "\n",
      "After initializing the classifier, you can fit the model to your training data using the `fit` method: `clf.fit(X_train_proper, y_train_proper)`.\n",
      "\n",
      "Once the model is trained, you can make predictions on your test data using the `predict` method: `predictions = clf.predict(X_test)`.\n",
      "\n",
      "Finally, you can evaluate the performance of your model using appropriate metrics such as ROC AUC and Average Precision. In the provided context, these are calculated using the `roc_auc_score` and `average_precision_score` functions from the `sklearn.metrics` module.\n",
      "\n",
      "Remember, the specific parameters and preprocessing steps might vary depending on the specific characteristics of your dataset and the problem at hand. It's always a good idea to experiment with different parameters and evaluate your model's performance to ensure you're getting the best results possible.\n",
      "\n",
      "Here is a simplified example:\n",
      "\n",
      "```python\n",
      "# Load and preprocess data\n",
      "# ...\n",
      "\n",
      "# Split data into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
      "\n",
      "# Initialize XGBoost classifier\n",
      "clf = XGBClassifier(iterations=100, learning_rate=0.01, depth=7, verbose=1)\n",
      "\n",
      "# Train the model\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "# Make predictions\n",
      "predictions = clf.predict(X_test)\n",
      "\n",
      "# Evaluate the model\n",
      "roc_auc = roc_auc_score(y_test,\n"
     ]
    }
   ],
   "source": [
    "# response = agent.query(\"give me sample code for getting conformal prediction using crepes library from a dataframe\")\n",
    "response = agent.chat(\"how to train xgboost model for sepsis dataset?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To train an XGBoost model for the Sepsis dataset, you would first need to load and preprocess your data. This might involve handling missing values, one-hot encoding categorical variables, and scaling numerical features. Once your data is prepared, you can split it into training and testing sets.\n",
      "\n",
      "Next, you would initialize the XGBoost classifier. In the provided context, the classifier is initialized with the following parameters: `iterations=100, learning_rate=0.01, depth=7, verbose=1`.\n",
      "\n",
      "After initializing the classifier, you can fit the model to your training data using the `fit` method: `clf.fit(X_train_proper, y_train_proper)`.\n",
      "\n",
      "Once the model is trained, you can make predictions on your test data using the `predict` method: `predictions = clf.predict(X_test)`.\n",
      "\n",
      "Finally, you can evaluate the performance of your model using appropriate metrics such as ROC AUC and Average Precision. In the provided context, these are calculated using the `roc_auc_score` and `average_precision_score` functions from the `sklearn.metrics` module.\n",
      "\n",
      "Remember, the specific parameters and preprocessing steps might vary depending on the specific characteristics of your dataset and the problem at hand. It's always a good idea to experiment with different parameters and evaluate your model's performance to ensure you're getting the best results possible.\n",
      "\n",
      "Here is a simplified example:\n",
      "\n",
      "```python\n",
      "# Load and preprocess data\n",
      "# ...\n",
      "\n",
      "# Split data into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
      "\n",
      "# Initialize XGBoost classifier\n",
      "clf = XGBClassifier(iterations=100, learning_rate=0.01, depth=7, verbose=1)\n",
      "\n",
      "# Train the model\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "# Make predictions\n",
      "predictions = clf.predict(X_test)\n",
      "\n",
      "# Evaluate the model\n",
      "roc_auc = roc_auc_score(y_test,\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: how to then compute uncertainty like conformal manually?\n",
      "=== LLM Response ===\n",
      "To compute uncertainty like conformal manually, you can follow these steps:\n",
      "\n",
      "1. Train your model on the proper training set.\n",
      "2. Calculate non-conformity scores for the calibration set. Non-conformity scores measure how different a new example is from the training data. In the context of binary classification, a common choice for non-conformity score is the absolute difference between the predicted probability and the true label.\n",
      "3. Sort the non-conformity scores in increasing order and store them in a list.\n",
      "4. For a new test example, calculate its non-conformity score.\n",
      "5. To compute the p-value, calculate the fraction of calibration examples that have a non-conformity score greater than the test example.\n",
      "6. The uncertainty of the prediction can be computed as 1 - p-value.\n",
      "\n",
      "Here is a simplified example:\n",
      "\n",
      "```python\n",
      "# Train the model\n",
      "clf.fit(X_train_proper, y_train_proper)\n",
      "\n",
      "# Calculate non-conformity scores for the calibration set\n",
      "calibration_scores = []\n",
      "for i in range(len(X_calib)):\n",
      "    pred_prob = clf.predict_proba(X_calib[i].reshape(1, -1))[:, 1]\n",
      "    calibration_scores.append(abs(pred_prob - y_calib[i]))\n",
      "\n",
      "# Sort the non-conformity scores in increasing order\n",
      "calibration_scores.sort()\n",
      "\n",
      "# For a new test example, calculate its non-conformity score\n",
      "test_score = abs(clf.predict_proba(X_test[0].reshape(1, -1))[:, 1] - y_test[0])\n",
      "\n",
      "# Compute the p-value\n",
      "p_value = sum(score > test_score for score in calibration_scores) / len(calibration_scores)\n",
      "\n",
      "# Compute the uncertainty\n",
      "uncertainty = 1 - p_value\n",
      "```\n",
      "\n",
      "Remember, the specific implementation might vary depending on the specific characteristics of your dataset and the problem at hand. It's always a good idea to experiment with different methods and evaluate your model's performance to ensure you're getting the best results possible.\n",
      "\n",
      "To get\n",
      "To compute uncertainty like conformal manually, you can follow these steps:\n",
      "\n",
      "1. Train your model on the proper training set.\n",
      "2. Calculate non-conformity scores for the calibration set. Non-conformity scores measure how different a new example is from the training data. In the context of binary classification, a common choice for non-conformity score is the absolute difference between the predicted probability and the true label.\n",
      "3. Sort the non-conformity scores in increasing order and store them in a list.\n",
      "4. For a new test example, calculate its non-conformity score.\n",
      "5. To compute the p-value, calculate the fraction of calibration examples that have a non-conformity score greater than the test example.\n",
      "6. The uncertainty of the prediction can be computed as 1 - p-value.\n",
      "\n",
      "Here is a simplified example:\n",
      "\n",
      "```python\n",
      "# Train the model\n",
      "clf.fit(X_train_proper, y_train_proper)\n",
      "\n",
      "# Calculate non-conformity scores for the calibration set\n",
      "calibration_scores = []\n",
      "for i in range(len(X_calib)):\n",
      "    pred_prob = clf.predict_proba(X_calib[i].reshape(1, -1))[:, 1]\n",
      "    calibration_scores.append(abs(pred_prob - y_calib[i]))\n",
      "\n",
      "# Sort the non-conformity scores in increasing order\n",
      "calibration_scores.sort()\n",
      "\n",
      "# For a new test example, calculate its non-conformity score\n",
      "test_score = abs(clf.predict_proba(X_test[0].reshape(1, -1))[:, 1] - y_test[0])\n",
      "\n",
      "# Compute the p-value\n",
      "p_value = sum(score > test_score for score in calibration_scores) / len(calibration_scores)\n",
      "\n",
      "# Compute the uncertainty\n",
      "uncertainty = 1 - p_value\n",
      "```\n",
      "\n",
      "Remember, the specific implementation might vary depending on the specific characteristics of your dataset and the problem at hand. It's always a good idea to experiment with different methods and evaluate your model's performance to ensure you're getting the best results possible.\n",
      "\n",
      "To get\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"how to then compute uncertainty like conformal manually?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To compute uncertainty like conformal manually, you can follow these steps:\n",
      "\n",
      "1. Train your model on the proper training set.\n",
      "2. Calculate non-conformity scores for the calibration set. Non-conformity scores measure how different a new example is from the training data. In the context of binary classification, a common choice for non-conformity score is the absolute difference between the predicted probability and the true label.\n",
      "3. Sort the non-conformity scores in increasing order and store them in a list.\n",
      "4. For a new test example, calculate its non-conformity score.\n",
      "5. To compute the p-value, calculate the fraction of calibration examples that have a non-conformity score greater than the test example.\n",
      "6. The uncertainty of the prediction can be computed as 1 - p-value.\n",
      "\n",
      "Here is a simplified example:\n",
      "\n",
      "```python\n",
      "# Train the model\n",
      "clf.fit(X_train_proper, y_train_proper)\n",
      "\n",
      "# Calculate non-conformity scores for the calibration set\n",
      "calibration_scores = []\n",
      "for i in range(len(X_calib)):\n",
      "    pred_prob = clf.predict_proba(X_calib[i].reshape(1, -1))[:, 1]\n",
      "    calibration_scores.append(abs(pred_prob - y_calib[i]))\n",
      "\n",
      "# Sort the non-conformity scores in increasing order\n",
      "calibration_scores.sort()\n",
      "\n",
      "# For a new test example, calculate its non-conformity score\n",
      "test_score = abs(clf.predict_proba(X_test[0].reshape(1, -1))[:, 1] - y_test[0])\n",
      "\n",
      "# Compute the p-value\n",
      "p_value = sum(score > test_score for score in calibration_scores) / len(calibration_scores)\n",
      "\n",
      "# Compute the uncertainty\n",
      "uncertainty = 1 - p_value\n",
      "```\n",
      "\n",
      "Remember, the specific implementation might vary depending on the specific characteristics of your dataset and the problem at hand. It's always a good idea to experiment with different methods and evaluate your model's performance to ensure you're getting the best results possible.\n",
      "\n",
      "To get\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
