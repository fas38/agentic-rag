{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda_envs\\rag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List,Optional\n",
    "\n",
    "# llama index imports\n",
    "from llama_index.core import SimpleDirectoryReader,VectorStoreIndex,SummaryIndex, StorageContext, Settings, load_index_from_storage\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.node_parser import SentenceSplitter, CodeSplitter, LangchainNodeParser\n",
    "from llama_index.core.tools import FunctionTool,QueryEngineTool\n",
    "from llama_index.core.vector_stores import MetadataFilters,FilterCondition\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "from llama_index.readers.file import IPYNBReader\n",
    "\n",
    "# llama index agent imports\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker, ReActAgent\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "# llama index llms and embeddings imports\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# langchain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "# tools\n",
    "import nest_asyncio # to allow running async functions in jupyter\n",
    "import chromadb # persistent storage for vectors\n",
    "# import nbconvert\n",
    "import tree_sitter\n",
    "import tree_sitter_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply() # to allow running async functions in jupyter\n",
    "\n",
    "# setting flags\n",
    "create_index = True\n",
    "\n",
    "# configuration\n",
    "MISTRAL_API_KEY =  \"BWdlihu9sUh5P2g3bHnzjAaHiT4anTVH\"\n",
    "embedding = \"BAAI/bge-small-en-v1.5\"\n",
    "# embedding = \"Qdrant/bm42-all-minilm-l6-v2-attentions\"\n",
    "# embedding = \"mistral-embed\"\n",
    "# embedding = OllamaEmbedding(\n",
    "#     model_name=\"llama2\",\n",
    "#     base_url=\"http://localhost:11434\",\n",
    "#     ollama_additional_kwargs={\"mirostat\": 0} \n",
    "# )\n",
    "# embedding = \"Salesforce/codet5p-110m-embedding\"\n",
    "# llm_model = \"mistral-large-latest\"\n",
    "llm_model = \"codellama\"\n",
    "chunk_size = 2000 # number of lines\n",
    "chunk_overlap = 200 # number of lines to overlap between chunks\n",
    "language = \"python\"\n",
    "data_path = \"./data_python\"\n",
    "\n",
    "# setup the llm and embedding\n",
    "embed_model = FastEmbedEmbedding(model_name=embedding)\n",
    "# embed_model = MistralAIEmbedding(model_name=embedding, api_key=MISTRAL_API_KEY)\n",
    "# embed_model =  HuggingFaceEmbedding(model_name=embedding)  \n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = chunk_size\n",
    "Settings.chunk_overlap = chunk_overlap\n",
    "# os.environ[\"MISTRAL_API_KEY\"] = MISTRAL_API_KEY\n",
    "# llm = MistralAI(model=llm_model, temperature=0.0)\n",
    "llm = Ollama(model=llm_model, request_timeout=1200.0, base_url=\"http://localhost:11434\", temperature=0.0)\n",
    "# temperture = 0.0 for deterministic results\n",
    "Settings.llm = llm\n",
    "\n",
    "# setup the persistent storage for vector store\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db_mistral_python\")\n",
    "chroma_collection = db.get_or_create_collection(\"code-agent\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for investigating node outputs\n",
    "\n",
    "# #load documents\n",
    "# file_path = \"./data_python/tabular_classification_binary.ipynb\"\n",
    "# documents = SimpleDirectoryReader(input_files = [file_path]).load_data()\n",
    "# print(f\"length of nodes\")\n",
    "# # splitter = CodeSplitter(language=\"python\", chunk_lines=chunk_size, chunk_lines_overlap=chunk_overlap, max_chars=max_chars)\n",
    "# splitter = LangchainNodeParser(RecursiveCharacterTextSplitter().from_language(Language.PYTHON, chunk_size=2000, chunk_overlap=0))\n",
    "# nodes = splitter.get_nodes_from_documents(documents)\n",
    "# print(f\"Length of nodes : {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for setting vector and summary tool from a document by creating new vector and summary index\n",
    "def get_doc_tools(file_path:str, name:str) -> str:\n",
    "  \"\"\"Get vector query and summary query tools from a jupyter notebook.\"\"\"\n",
    "  \n",
    "  #load documents\n",
    "  documents = SimpleDirectoryReader(input_files = [file_path]).load_data()\n",
    "  print(f\"length of nodes\")\n",
    "  # splitter = CodeSplitter(language=\"python\", chunk_lines=chunk_size, chunk_lines_overlap=chunk_overlap, max_chars=max_chars)\n",
    "  splitter = LangchainNodeParser(RecursiveCharacterTextSplitter().from_language(Language.PYTHON, chunk_size=chunk_size, chunk_overlap=chunk_overlap))\n",
    "  nodes = splitter.get_nodes_from_documents(documents)\n",
    "  print(f\"Length of nodes : {len(nodes)}\")\n",
    "  \n",
    "  #instantiate Vector store\n",
    "  vector_index = VectorStoreIndex(nodes,storage_context=storage_context)\n",
    "  vector_index.storage_context.vector_store.persist(persist_path=\"/content/chroma_db\")\n",
    "  \n",
    "  # Vector store Auto retrieval query engine method\n",
    "  def vector_query(query:str) -> str:\n",
    "    \"\"\"\n",
    "    query (str): the string query to be embedded\n",
    "    \"\"\"\n",
    "    query_engine = vector_index.as_query_engine(similarity_top_k =2) # set vector query engine with similarity as top 2 results\n",
    "    \n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "  \n",
    "  # Prepare Vector Tool\n",
    "  vector_query_tool = FunctionTool.from_defaults(name=f\"vector_tool_{name}\", fn=vector_query)\n",
    "  \n",
    "  # Prepare Summary Tool\n",
    "  summary_index = SummaryIndex(nodes)\n",
    "  summary_index.storage_context.persist(persist_dir=\"./db_mistral_python\") # save the summary index to disk\n",
    "  summary_query_engine = summary_index.as_query_engine(response_mode=\"tree_summarize\", use_async=True) # set summary query engine with tree summarization\n",
    "  summary_query_tool = QueryEngineTool.from_defaults(name=f\"summary_tool_{name}\",query_engine=summary_query_engine, description=(\"Use ONLY IF you want to get a holistic approach for full implementation\" \"DO NOT USE if you have question for specific implementation.\")) # set summary query tool with prompt\n",
    "  return vector_query_tool,summary_query_tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for setting vector and summary tool from a document by loading vector and summary index from storage\n",
    "def get_doc_tools_from_storage(file_path:str, name:str) -> str:\n",
    "  \"\"\"Get vector query and summary query tools from a document.\"\"\"\n",
    "  \n",
    "  #load vector store\n",
    "  vector_index = VectorStoreIndex.from_vector_store(vector_store=vector_store, storage_context=storage_context)\n",
    "  \n",
    "  # Vector store Auto retrieval query engine method\n",
    "  def vector_query(query:str, page_numbers:Optional[List[str]] = None) -> str:\n",
    "    \"\"\"\n",
    "    query (str): the string query to be embedded\n",
    "    page_numbers Optional[List[str]]: List of page numbers to be retrieved.\n",
    "    Leave as NONE if we want to perform a vector search over all pages. \n",
    "    Otherwise, filter by the set of specified pages.\n",
    "    Always leave page_numbers as None UNLESS there is a specific page you want to search for.\n",
    "    \"\"\"\n",
    "    page_numbers = page_numbers or []\n",
    "    metadata_dict = [{\"key\":'page_label', \"value\":p} for p in page_numbers]\n",
    "    \n",
    "    query_engine = vector_index.as_query_engine(similarity_top_k =2, filters = MetadataFilters.from_dicts(metadata_dict, condition=FilterCondition.OR)) # set vector query engine with similarity as top 2 results\n",
    "    \n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "  \n",
    "  # Prepare Vector Tool\n",
    "  vector_query_tool = FunctionTool.from_defaults(name=f\"vector_tool_{name}\", fn=vector_query)\n",
    "  \n",
    "  # Prepare Summary Tool\n",
    "  storage_context_all = StorageContext.from_defaults(persist_dir=\"./db_mistral_python\") # set storage context for summary index\n",
    "  summary_index = load_index_from_storage(storage_context=storage_context_all) # load summary index from storage\n",
    "  summary_query_engine = summary_index.as_query_engine(response_mode=\"tree_summarize\", use_async=True) # set summary query engine with tree summarization\n",
    "  summary_query_tool = QueryEngineTool.from_defaults(name=f\"summary_tool_{name}\",query_engine=summary_query_engine, description=(\"Use ONLY IF you want to get a holistic summary of the documents.\" \"DO NOT USE if you have specified questions over the documents.\")) # set summary query tool with prompt\n",
    "  return vector_query_tool,summary_query_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of vector and summary tools for all documents in the path\n",
    "def get_doc_tools_from_path(path:str) -> list:\n",
    "  file_name = []\n",
    "  file_path = []\n",
    "  for file in os.listdir(path):\n",
    "    if file.endswith(\".ipynb\"):\n",
    "      file_name.append(file.split(\".\")[0])\n",
    "      file_path.append(os.path.join(path,file))\n",
    "\n",
    "  papers_to_tools_dict = {}\n",
    "  for name,filename in zip(file_name,file_path):\n",
    "    if create_index:\n",
    "      vector_query_tool,summary_query_tool = get_doc_tools(filename,name)\n",
    "    else:\n",
    "      vector_query_tool,summary_query_tool = get_doc_tools_from_storage(filename,name)\n",
    "    papers_to_tools_dict[name] = [vector_query_tool,summary_query_tool]\n",
    "\n",
    "  initial_tools = [t for f in file_name for t in papers_to_tools_dict[f]]\n",
    "  return initial_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of nodes\n",
      "Length of nodes : 132\n"
     ]
    }
   ],
   "source": [
    "# create object index from the list of tools\n",
    "initial_tools_for_data = get_doc_tools_from_path(data_path)\n",
    "obj_index = ObjectIndex.from_objects(initial_tools_for_data, index_cls=VectorStoreIndex)\n",
    "obj_retriever = obj_index.as_retriever(similarity_top_k=2) # set object retriever with similarity as top 2 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # setup single agent\n",
    "# agent_worker = FunctionCallingAgentWorker.from_tools(tool_retriever=obj_retriever, \n",
    "#                                                      llm=llm, \n",
    "#                                                      system_prompt=\"\"\"You are an agent designed to answer queries over a given jupyter notebook. Please always use the tools provided to answer a question.Do not rely on prior knowledge.\"\"\", \n",
    "#                                                      verbose=True) \n",
    "# agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup ReAct agent\n",
    "agent = ReActAgent.from_tools(tool_retriever=obj_retriever, \n",
    "                                     llm=llm, \n",
    "                                     system_prompt=\"\"\"You are a proficient python developer. Respond with the syntactically correct code for the question below. Make sure you follow these rules:\n",
    "                                        1. Use context to understand the APIs and how to use them.\n",
    "                                        2. Ensure all the requirements in the question are met.\n",
    "                                        3. Ensure the output code syntax is correct.\n",
    "                                        4. All required dependencies should be imported above the code.\n",
    "                                        Question:\n",
    "                                        {question}\n",
    "                                        Context:\n",
    "                                        {context}\n",
    "                                        Helpful Response:\"\"\", \n",
    "                                     verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using CodeLlama (Local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n",
      "Action: vector_tool_tabular_classification_binary\n",
      "Action Input: {'query': 'how to train xgboost model for sepsis dataset?'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: To train an XGBoost model for the Sepsis dataset using the provided code, you can follow these steps:\n",
      "\n",
      "1. Import the necessary libraries:\n",
      "```python\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
      "from xgboost import XGBClassifier\n",
      "```\n",
      "2. Load the Sepsis dataset and split it into training and testing sets:\n",
      "```python\n",
      "# load the Sepsis dataset\n",
      "dataset = pd.read_csv(\"./dataset/Sepsis_Processed_IC.csv\")\n",
      "\n",
      "# split the data into training and testing sets\n",
      "train_data, test_data = dataset.split(test_size=0.2, random_state=42)\n",
      "```\n",
      "3. Preprocess the data:\n",
      "```python\n",
      "# labels and features separation\n",
      "X = train_data.drop(columns=['Target'], axis=1)\n",
      "y = train_data['Target']\n",
      "\n",
      "# one hot encoding the category columns\n",
      "category_columns = X.select_dtypes(include=['object']).columns\n",
      "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
      "X_encoded = one_hot_encoder.fit_transform(X[category_columns])\n",
      "X_encoded = pd.DataFrame(X_encoded, columns=one_hot_encoder.get_feature_names_out(category_columns))\n",
      "X = pd.concat([X, X_encoded], axis=1)\n",
      "X.drop(columns=category_columns, inplace=True)\n",
      "\n",
      "# convert bool to int\n",
      "bool_columns = X.select_dtypes(include=['bool']).columns\n",
      "for column in bool_columns:\n",
      "    X[column] = X[column].astype('int')\n",
      "\n",
      "# standardize the data using robust scaler\n",
      "scaler = RobustScaler()\n",
      "scaled_columns = ['age', 'max_activity_count', 'duration_since_reg', 'crp', 'lacticacid', 'hours_past_midnight', 'duration_last_event']\n",
      "X[scaled_columns] = scaler.fit_transform(X[scaled_columns])\n",
      "```\n",
      "4. Train the XGBoost model:\n",
      "```python\n",
      "# train the XGBoost model\n",
      "model = XGBClassifier()\n",
      "model.fit(X, y)\n",
      "```\n",
      "5. Evaluate the model on the testing data:\n",
      "```python\n",
      "# evaluate the model on the testing data\n",
      "y_pred = model.predict(test_data)\n",
      "accuracy = accuracy_score(test_data['Target'], y_pred)\n",
      "print(\"Accuracy:\", accuracy)\n",
      "```\n",
      "Note that this is just a basic example of how to train an XGBoost model for the Sepsis dataset using the provided code. You may need to adjust the preprocessing steps and the hyperparameters of the model based on your specific use case.\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: To train an XGBoost model for the Sepsis dataset, you can follow these steps:\n",
      "\n",
      "1. Import the necessary libraries:\n",
      "```python\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
      "from xgboost import XGBClassifier\n",
      "```\n",
      "2. Load the Sepsis dataset and split it into training and testing sets:\n",
      "```python\n",
      "# load the Sepsis dataset\n",
      "dataset = pd.read_csv(\"./dataset/Sepsis_Processed_IC.csv\")\n",
      "\n",
      "# split the data into training and testing sets\n",
      "train_data, test_data = dataset.split(test_size=0.2, random_state=42)\n",
      "```\n",
      "3. Preprocess the data:\n",
      "```python\n",
      "# labels and features separation\n",
      "X = train_data.drop(columns=['Target'], axis=1)\n",
      "y = train_data['Target']\n",
      "\n",
      "# one hot encoding the category columns\n",
      "category_columns = X.select_dtypes(include=['object']).columns\n",
      "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
      "X_encoded = one_hot_encoder.fit_transform(X[category_columns])\n",
      "X_encoded = pd.DataFrame(X_encoded, columns=one_hot_encoder.get_feature_names_out(category_columns))\n",
      "X = pd.concat([X, X_encoded], axis=1)\n",
      "X.drop(columns=category_columns, inplace=True)\n",
      "\n",
      "# convert bool to int\n",
      "bool_columns = X.select_dtypes(include=['bool']).columns\n",
      "for column in bool_columns:\n",
      "    X[column] = X[column].astype('int')\n",
      "\n",
      "# standardize the data using robust scaler\n",
      "scaler = RobustScaler()\n",
      "scaled_columns = ['age', 'max_activity_count', 'duration_since_reg', 'crp', 'lacticacid', 'hours_past_midnight', 'duration_last_event']\n",
      "X[scaled_columns] = scaler.fit_transform(X[scaled_columns])\n",
      "```\n",
      "4. Train the XGBoost model:\n",
      "```python\n",
      "# train the XGBoost model\n",
      "model = XGBClassifier()\n",
      "model.fit(X, y)\n",
      "```\n",
      "5. Evaluate the model on the testing data:\n",
      "```python\n",
      "# evaluate the model on the testing data\n",
      "y_pred = model.predict(test_data)\n",
      "accuracy = accuracy_score(test_data['Target'], y_pred)\n",
      "print(\"Accuracy:\", accuracy)\n",
      "```\n",
      "Note that this is just a basic example of how to train an XGBoost model for the Sepsis dataset using the provided code. You may need to adjust the preprocessing steps and the hyperparameters of the model based on your specific use case.\n",
      "\u001b[0mTo train an XGBoost model for the Sepsis dataset, you can follow these steps:\n",
      "\n",
      "1. Import the necessary libraries:\n",
      "```python\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
      "from xgboost import XGBClassifier\n",
      "```\n",
      "2. Load the Sepsis dataset and split it into training and testing sets:\n",
      "```python\n",
      "# load the Sepsis dataset\n",
      "dataset = pd.read_csv(\"./dataset/Sepsis_Processed_IC.csv\")\n",
      "\n",
      "# split the data into training and testing sets\n",
      "train_data, test_data = dataset.split(test_size=0.2, random_state=42)\n",
      "```\n",
      "3. Preprocess the data:\n",
      "```python\n",
      "# labels and features separation\n",
      "X = train_data.drop(columns=['Target'], axis=1)\n",
      "y = train_data['Target']\n",
      "\n",
      "# one hot encoding the category columns\n",
      "category_columns = X.select_dtypes(include=['object']).columns\n",
      "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
      "X_encoded = one_hot_encoder.fit_transform(X[category_columns])\n",
      "X_encoded = pd.DataFrame(X_encoded, columns=one_hot_encoder.get_feature_names_out(category_columns))\n",
      "X = pd.concat([X, X_encoded], axis=1)\n",
      "X.drop(columns=category_columns, inplace=True)\n",
      "\n",
      "# convert bool to int\n",
      "bool_columns = X.select_dtypes(include=['bool']).columns\n",
      "for column in bool_columns:\n",
      "    X[column] = X[column].astype('int')\n",
      "\n",
      "# standardize the data using robust scaler\n",
      "scaler = RobustScaler()\n",
      "scaled_columns = ['age', 'max_activity_count', 'duration_since_reg', 'crp', 'lacticacid', 'hours_past_midnight', 'duration_last_event']\n",
      "X[scaled_columns] = scaler.fit_transform(X[scaled_columns])\n",
      "```\n",
      "4. Train the XGBoost model:\n",
      "```python\n",
      "# train the XGBoost model\n",
      "model = XGBClassifier()\n",
      "model.fit(X, y)\n",
      "```\n",
      "5. Evaluate the model on the testing data:\n",
      "```python\n",
      "# evaluate the model on the testing data\n",
      "y_pred = model.predict(test_data)\n",
      "accuracy = accuracy_score(test_data['Target'], y_pred)\n",
      "print(\"Accuracy:\", accuracy)\n",
      "```\n",
      "Note that this is just a basic example of how to train an XGBoost model for the Sepsis dataset using the provided code. You may need to adjust the preprocessing steps and the hyperparameters of the model based on your specific use case.\n"
     ]
    }
   ],
   "source": [
    "response = await agent.achat(\"how to train xgboost model for sepsis dataset?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActionReasoningStep -> thought='The current language of the user is English. I need to use a tool to help me answer the question.' action='vector_tool_tabular_classification_binary' action_input={'query': 'how to train xgboost model for sepsis dataset?'}\n",
      "\n",
      "ObservationReasoningStep -> observation='To train an XGBoost model for the Sepsis dataset using the provided code, you can follow these steps:\\n\\n1. Import the necessary libraries:\\n```python\\nimport pandas as pd\\nfrom sklearn.preprocessing import OneHotEncoder, RobustScaler\\nfrom xgboost import XGBClassifier\\n```\\n2. Load the Sepsis dataset and split it into training and testing sets:\\n```python\\n# load the Sepsis dataset\\ndataset = pd.read_csv(\"./dataset/Sepsis_Processed_IC.csv\")\\n\\n# split the data into training and testing sets\\ntrain_data, test_data = dataset.split(test_size=0.2, random_state=42)\\n```\\n3. Preprocess the data:\\n```python\\n# labels and features separation\\nX = train_data.drop(columns=[\\'Target\\'], axis=1)\\ny = train_data[\\'Target\\']\\n\\n# one hot encoding the category columns\\ncategory_columns = X.select_dtypes(include=[\\'object\\']).columns\\none_hot_encoder = OneHotEncoder(sparse_output=False)\\nX_encoded = one_hot_encoder.fit_transform(X[category_columns])\\nX_encoded = pd.DataFrame(X_encoded, columns=one_hot_encoder.get_feature_names_out(category_columns))\\nX = pd.concat([X, X_encoded], axis=1)\\nX.drop(columns=category_columns, inplace=True)\\n\\n# convert bool to int\\nbool_columns = X.select_dtypes(include=[\\'bool\\']).columns\\nfor column in bool_columns:\\n    X[column] = X[column].astype(\\'int\\')\\n\\n# standardize the data using robust scaler\\nscaler = RobustScaler()\\nscaled_columns = [\\'age\\', \\'max_activity_count\\', \\'duration_since_reg\\', \\'crp\\', \\'lacticacid\\', \\'hours_past_midnight\\', \\'duration_last_event\\']\\nX[scaled_columns] = scaler.fit_transform(X[scaled_columns])\\n```\\n4. Train the XGBoost model:\\n```python\\n# train the XGBoost model\\nmodel = XGBClassifier()\\nmodel.fit(X, y)\\n```\\n5. Evaluate the model on the testing data:\\n```python\\n# evaluate the model on the testing data\\ny_pred = model.predict(test_data)\\naccuracy = accuracy_score(test_data[\\'Target\\'], y_pred)\\nprint(\"Accuracy:\", accuracy)\\n```\\nNote that this is just a basic example of how to train an XGBoost model for the Sepsis dataset using the provided code. You may need to adjust the preprocessing steps and the hyperparameters of the model based on your specific use case.' return_direct=False\n",
      "\n",
      "ResponseReasoningStep -> thought=\"I can answer without using any more tools. I'll use the user's language to answer.\" response='To train an XGBoost model for the Sepsis dataset, you can follow these steps:\\n\\n1. Import the necessary libraries:\\n```python\\nimport pandas as pd\\nfrom sklearn.preprocessing import OneHotEncoder, RobustScaler\\nfrom xgboost import XGBClassifier\\n```\\n2. Load the Sepsis dataset and split it into training and testing sets:\\n```python\\n# load the Sepsis dataset\\ndataset = pd.read_csv(\"./dataset/Sepsis_Processed_IC.csv\")\\n\\n# split the data into training and testing sets\\ntrain_data, test_data = dataset.split(test_size=0.2, random_state=42)\\n```\\n3. Preprocess the data:\\n```python\\n# labels and features separation\\nX = train_data.drop(columns=[\\'Target\\'], axis=1)\\ny = train_data[\\'Target\\']\\n\\n# one hot encoding the category columns\\ncategory_columns = X.select_dtypes(include=[\\'object\\']).columns\\none_hot_encoder = OneHotEncoder(sparse_output=False)\\nX_encoded = one_hot_encoder.fit_transform(X[category_columns])\\nX_encoded = pd.DataFrame(X_encoded, columns=one_hot_encoder.get_feature_names_out(category_columns))\\nX = pd.concat([X, X_encoded], axis=1)\\nX.drop(columns=category_columns, inplace=True)\\n\\n# convert bool to int\\nbool_columns = X.select_dtypes(include=[\\'bool\\']).columns\\nfor column in bool_columns:\\n    X[column] = X[column].astype(\\'int\\')\\n\\n# standardize the data using robust scaler\\nscaler = RobustScaler()\\nscaled_columns = [\\'age\\', \\'max_activity_count\\', \\'duration_since_reg\\', \\'crp\\', \\'lacticacid\\', \\'hours_past_midnight\\', \\'duration_last_event\\']\\nX[scaled_columns] = scaler.fit_transform(X[scaled_columns])\\n```\\n4. Train the XGBoost model:\\n```python\\n# train the XGBoost model\\nmodel = XGBClassifier()\\nmodel.fit(X, y)\\n```\\n5. Evaluate the model on the testing data:\\n```python\\n# evaluate the model on the testing data\\ny_pred = model.predict(test_data)\\naccuracy = accuracy_score(test_data[\\'Target\\'], y_pred)\\nprint(\"Accuracy:\", accuracy)\\n```\\nNote that this is just a basic example of how to train an XGBoost model for the Sepsis dataset using the provided code. You may need to adjust the preprocessing steps and the hyperparameters of the model based on your specific use case.' is_streaming=False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for resoning_step in agent.get_completed_tasks()[0].extra_state[\"current_reasoning\"]:\n",
    "    print(f\"{resoning_step.__class__.__name__} -> {resoning_step}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To train an XGBoost model for the Sepsis dataset, you can follow these steps:\n",
      "\n",
      "1. Import the necessary libraries:\n",
      "```python\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
      "from xgboost import XGBClassifier\n",
      "```\n",
      "2. Load the Sepsis dataset and split it into training and testing sets:\n",
      "```python\n",
      "# load the Sepsis dataset\n",
      "dataset = pd.read_csv(\"./dataset/Sepsis_Processed_IC.csv\")\n",
      "\n",
      "# split the data into training and testing sets\n",
      "train_data, test_data = dataset.split(test_size=0.2, random_state=42)\n",
      "```\n",
      "3. Preprocess the data:\n",
      "```python\n",
      "# labels and features separation\n",
      "X = train_data.drop(columns=['Target'], axis=1)\n",
      "y = train_data['Target']\n",
      "\n",
      "# one hot encoding the category columns\n",
      "category_columns = X.select_dtypes(include=['object']).columns\n",
      "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
      "X_encoded = one_hot_encoder.fit_transform(X[category_columns])\n",
      "X_encoded = pd.DataFrame(X_encoded, columns=one_hot_encoder.get_feature_names_out(category_columns))\n",
      "X = pd.concat([X, X_encoded], axis=1)\n",
      "X.drop(columns=category_columns, inplace=True)\n",
      "\n",
      "# convert bool to int\n",
      "bool_columns = X.select_dtypes(include=['bool']).columns\n",
      "for column in bool_columns:\n",
      "    X[column] = X[column].astype('int')\n",
      "\n",
      "# standardize the data using robust scaler\n",
      "scaler = RobustScaler()\n",
      "scaled_columns = ['age', 'max_activity_count', 'duration_since_reg', 'crp', 'lacticacid', 'hours_past_midnight', 'duration_last_event']\n",
      "X[scaled_columns] = scaler.fit_transform(X[scaled_columns])\n",
      "```\n",
      "4. Train the XGBoost model:\n",
      "```python\n",
      "# train the XGBoost model\n",
      "model = XGBClassifier()\n",
      "model.fit(X, y)\n",
      "```\n",
      "5. Evaluate the model on the testing data:\n",
      "```python\n",
      "# evaluate the model on the testing data\n",
      "y_pred = model.predict(test_data)\n",
      "accuracy = accuracy_score(test_data['Target'], y_pred)\n",
      "print(\"Accuracy:\", accuracy)\n",
      "```\n",
      "Note that this is just a basic example of how to train an XGBoost model for the Sepsis dataset using the provided code. You may need to adjust the preprocessing steps and the hyperparameters of the model based on your specific use case.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: (Implicit) I can answer without any more tools!\n",
      "Answer: To compute conformal predictions manually on a trained XGBoost model, you can follow these steps:\n",
      "\n",
      "1. Load the trained XGBoost model and the testing data:\n",
      "```python\n",
      "import xgboost as xgb\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "# load the trained XGBoost model\n",
      "model = xgb.XGBClassifier()\n",
      "model.load_model(\"./models/xgb_model.json\")\n",
      "\n",
      "# load the testing data\n",
      "test_data = pd.read_csv(\"./dataset/Sepsis_Processed_IC.csv\")\n",
      "```\n",
      "2. Preprocess the testing data:\n",
      "```python\n",
      "# labels and features separation\n",
      "X = test_data.drop(columns=['Target'], axis=1)\n",
      "y = test_data['Target']\n",
      "\n",
      "# one hot encoding the category columns\n",
      "category_columns = X.select_dtypes(include=['object']).columns\n",
      "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
      "X_encoded = one_hot_encoder.fit_transform(X[category_columns])\n",
      "X_encoded = pd.DataFrame(X_encoded, columns=one_hot_encoder.get_feature_names_out(category_columns))\n",
      "X = pd.concat([X, X_encoded], axis=1)\n",
      "X.drop(columns=category_columns, inplace=True)\n",
      "\n",
      "# convert bool to int\n",
      "bool_columns = X.select_dtypes(include=['bool']).columns\n",
      "for column in bool_columns:\n",
      "    X[column] = X[column].astype('int')\n",
      "\n",
      "# standardize the data using robust scaler\n",
      "scaler = RobustScaler()\n",
      "scaled_columns = ['age', 'max_activity_count', 'duration_since_reg', 'crp', 'lacticacid', 'hours_past_midnight', 'duration_last_event']\n",
      "X[scaled_columns] = scaler.fit_transform(X[scaled_columns])\n",
      "```\n",
      "3. Compute the conformal predictions:\n",
      "```python\n",
      "# compute the conformal predictions\n",
      "conformal_predictions = model.predict(X)\n",
      "confidence_scores = model.predict_proba(X)[:, 1]\n",
      "```\n",
      "4. Evaluate the conformal predictions:\n",
      "```python\n",
      "# evaluate the conformal predictions\n",
      "accuracy = accuracy_score(y, conformal_predictions)\n",
      "print(\"Accuracy:\", accuracy)\n",
      "```\n",
      "Note that this is just a basic example of how to compute conformal predictions manually on an XGBoost model using the provided code. You may need to adjust the preprocessing steps and the hyperparameters of the model based on your specific use case.\n",
      "\u001b[0mTo compute conformal predictions manually on a trained XGBoost model, you can follow these steps:\n",
      "\n",
      "1. Load the trained XGBoost model and the testing data:\n",
      "```python\n",
      "import xgboost as xgb\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "# load the trained XGBoost model\n",
      "model = xgb.XGBClassifier()\n",
      "model.load_model(\"./models/xgb_model.json\")\n",
      "\n",
      "# load the testing data\n",
      "test_data = pd.read_csv(\"./dataset/Sepsis_Processed_IC.csv\")\n",
      "```\n",
      "2. Preprocess the testing data:\n",
      "```python\n",
      "# labels and features separation\n",
      "X = test_data.drop(columns=['Target'], axis=1)\n",
      "y = test_data['Target']\n",
      "\n",
      "# one hot encoding the category columns\n",
      "category_columns = X.select_dtypes(include=['object']).columns\n",
      "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
      "X_encoded = one_hot_encoder.fit_transform(X[category_columns])\n",
      "X_encoded = pd.DataFrame(X_encoded, columns=one_hot_encoder.get_feature_names_out(category_columns))\n",
      "X = pd.concat([X, X_encoded], axis=1)\n",
      "X.drop(columns=category_columns, inplace=True)\n",
      "\n",
      "# convert bool to int\n",
      "bool_columns = X.select_dtypes(include=['bool']).columns\n",
      "for column in bool_columns:\n",
      "    X[column] = X[column].astype('int')\n",
      "\n",
      "# standardize the data using robust scaler\n",
      "scaler = RobustScaler()\n",
      "scaled_columns = ['age', 'max_activity_count', 'duration_since_reg', 'crp', 'lacticacid', 'hours_past_midnight', 'duration_last_event']\n",
      "X[scaled_columns] = scaler.fit_transform(X[scaled_columns])\n",
      "```\n",
      "3. Compute the conformal predictions:\n",
      "```python\n",
      "# compute the conformal predictions\n",
      "conformal_predictions = model.predict(X)\n",
      "confidence_scores = model.predict_proba(X)[:, 1]\n",
      "```\n",
      "4. Evaluate the conformal predictions:\n",
      "```python\n",
      "# evaluate the conformal predictions\n",
      "accuracy = accuracy_score(y, conformal_predictions)\n",
      "print(\"Accuracy:\", accuracy)\n",
      "```\n",
      "Note that this is just a basic example of how to compute conformal predictions manually on an XGBoost model using the provided code. You may need to adjust the preprocessing steps and the hyperparameters of the model based on your specific use case.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"give me the code for computing conformal prediction manually on the trained model\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To compute conformal predictions manually on a trained XGBoost model, you can follow these steps:\n",
      "\n",
      "1. Load the trained XGBoost model and the testing data:\n",
      "```python\n",
      "import xgboost as xgb\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "# load the trained XGBoost model\n",
      "model = xgb.XGBClassifier()\n",
      "model.load_model(\"./models/xgb_model.json\")\n",
      "\n",
      "# load the testing data\n",
      "test_data = pd.read_csv(\"./dataset/Sepsis_Processed_IC.csv\")\n",
      "```\n",
      "2. Preprocess the testing data:\n",
      "```python\n",
      "# labels and features separation\n",
      "X = test_data.drop(columns=['Target'], axis=1)\n",
      "y = test_data['Target']\n",
      "\n",
      "# one hot encoding the category columns\n",
      "category_columns = X.select_dtypes(include=['object']).columns\n",
      "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
      "X_encoded = one_hot_encoder.fit_transform(X[category_columns])\n",
      "X_encoded = pd.DataFrame(X_encoded, columns=one_hot_encoder.get_feature_names_out(category_columns))\n",
      "X = pd.concat([X, X_encoded], axis=1)\n",
      "X.drop(columns=category_columns, inplace=True)\n",
      "\n",
      "# convert bool to int\n",
      "bool_columns = X.select_dtypes(include=['bool']).columns\n",
      "for column in bool_columns:\n",
      "    X[column] = X[column].astype('int')\n",
      "\n",
      "# standardize the data using robust scaler\n",
      "scaler = RobustScaler()\n",
      "scaled_columns = ['age', 'max_activity_count', 'duration_since_reg', 'crp', 'lacticacid', 'hours_past_midnight', 'duration_last_event']\n",
      "X[scaled_columns] = scaler.fit_transform(X[scaled_columns])\n",
      "```\n",
      "3. Compute the conformal predictions:\n",
      "```python\n",
      "# compute the conformal predictions\n",
      "conformal_predictions = model.predict(X)\n",
      "confidence_scores = model.predict_proba(X)[:, 1]\n",
      "```\n",
      "4. Evaluate the conformal predictions:\n",
      "```python\n",
      "# evaluate the conformal predictions\n",
      "accuracy = accuracy_score(y, conformal_predictions)\n",
      "print(\"Accuracy:\", accuracy)\n",
      "```\n",
      "Note that this is just a basic example of how to compute conformal predictions manually on an XGBoost model using the provided code. You may need to adjust the preprocessing steps and the hyperparameters of the model based on your specific use case.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using Mistral API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: summary_tool_tabular_classification_binary\n",
      "Action Input: {'input': 'how to train xgboost model for sepsis dataset?'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: To train an XGBoost model for the Sepsis dataset, you would first need to preprocess your data. This includes loading the dataset, separating labels and features, handling missing values, one-hot encoding categorical columns, converting boolean columns to integers, and standardizing numerical columns.\n",
      "\n",
      "Once your data is preprocessed, you can split it into a training set and a test set using stratified sampling. Then, you can initialize the XGBoost model with specified parameters such as the number of estimators, learning rate, depth, and objective.\n",
      "\n",
      "After initializing the model, you can train it on the training set using the `fit` method. Once the model is trained, you can make predictions on the test set using the `predict` method and calculate the prediction probabilities using the `predict_proba` method.\n",
      "\n",
      "Finally, you can evaluate the model's performance by calculating metrics such as ROC AUC and average precision, and by plotting the ROC curve and precision-recall curve. If needed, you can also tune the model's threshold for optimal performance.\n",
      "\n",
      "Here is a simplified example of how you might do this:\n",
      "\n",
      "```python\n",
      "# model initialization\n",
      "clf = XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss', scale_pos_weight=(len(class_0_train) / len(class_1_train)), use_label_encoder=False, verbosity=2, objective='binary:logistic', max_depth=6, learning_rate=0.01, n_estimators=500, subsample=0.8, colsample_bytree=0.6, reg_alpha=0.5, reg_lambda=1, oob_score=True)\n",
      "\n",
      "# training and prediction\n",
      "clf.fit(X_train_proper, y_train_proper) # train the model on the small training set\n",
      "prediction_prob_test = clf.predict_proba(X_test) # get the prediction probabilities for the test set\n",
      "predictions = clf.predict(X_test) # get the predictions for the test set\n",
      "\n",
      "# evaluation\n",
      "roc_auc = roc_auc_score(y_test, prediction_prob_test[:,1]) #\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: The current language of the user is: English. I have enough information to answer the question without using any more tools.\n",
      "Answer: To train an XGBoost model for the Sepsis dataset, you would first need to preprocess your data. This includes loading the dataset, separating labels and features, handling missing values, one-hot encoding categorical columns, converting boolean columns to integers, and standardizing numerical columns.\n",
      "\n",
      "Once your data is preprocessed, you can split it into a training set and a test set using stratified sampling. Then, you can initialize the XGBoost model with specified parameters such as the number of estimators, learning rate, depth, and objective.\n",
      "\n",
      "After initializing the model, you can train it on the training set using the `fit` method. Once the model is trained, you can make predictions on the test set using the `predict` method and calculate the prediction probabilities using the `predict_proba` method.\n",
      "\n",
      "Finally, you can evaluate the model's performance by calculating metrics such as ROC AUC and average precision, and by plotting the ROC curve and precision-recall curve. If needed, you can also tune the model's threshold for optimal performance.\n",
      "\n",
      "Here is a simplified example of how you might do this:\n",
      "\n",
      "```python\n",
      "# model initialization\n",
      "clf = XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss', scale_pos_weight=(len(class_0_train) / len(class_1_train)), use_label_encoder=False, verbosity=2, objective='binary:logistic', max_depth=6, learning_rate=0.01, n_estimators=500, subsample=0.8, colsample_bytree=0.6, reg_alpha=0.5, reg_lambda=1, oob_score=True)\n",
      "\n",
      "# training and prediction\n",
      "clf.fit(X_train_proper, y_train_proper) # train the model on the small training set\n",
      "prediction_prob_test = clf.predict_proba(X_test) # get the prediction probabilities for the test set\n",
      "predictions = clf.predict(X_test) # get the predictions for the test\n",
      "\u001b[0mTo train an XGBoost model for the Sepsis dataset, you would first need to preprocess your data. This includes loading the dataset, separating labels and features, handling missing values, one-hot encoding categorical columns, converting boolean columns to integers, and standardizing numerical columns.\n",
      "\n",
      "Once your data is preprocessed, you can split it into a training set and a test set using stratified sampling. Then, you can initialize the XGBoost model with specified parameters such as the number of estimators, learning rate, depth, and objective.\n",
      "\n",
      "After initializing the model, you can train it on the training set using the `fit` method. Once the model is trained, you can make predictions on the test set using the `predict` method and calculate the prediction probabilities using the `predict_proba` method.\n",
      "\n",
      "Finally, you can evaluate the model's performance by calculating metrics such as ROC AUC and average precision, and by plotting the ROC curve and precision-recall curve. If needed, you can also tune the model's threshold for optimal performance.\n",
      "\n",
      "Here is a simplified example of how you might do this:\n",
      "\n",
      "```python\n",
      "# model initialization\n",
      "clf = XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss', scale_pos_weight=(len(class_0_train) / len(class_1_train)), use_label_encoder=False, verbosity=2, objective='binary:logistic', max_depth=6, learning_rate=0.01, n_estimators=500, subsample=0.8, colsample_bytree=0.6, reg_alpha=0.5, reg_lambda=1, oob_score=True)\n",
      "\n",
      "# training and prediction\n",
      "clf.fit(X_train_proper, y_train_proper) # train the model on the small training set\n",
      "prediction_prob_test = clf.predict_proba(X_test) # get the prediction probabilities for the test set\n",
      "predictions = clf.predict(X_test) # get the predictions for the test\n"
     ]
    }
   ],
   "source": [
    "response = await agent.achat(\"how to train xgboost model for sepsis dataset?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActionReasoningStep -> thought='The current language of the user is: English. I need to use a tool to help me answer the question.' action='summary_tool_tabular_classification_binary' action_input={'input': 'how to train xgboost model for sepsis dataset?'}\n",
      "\n",
      "ObservationReasoningStep -> observation=\"To train an XGBoost model for the Sepsis dataset, you would first need to preprocess your data. This includes loading the dataset, separating labels and features, handling missing values, one-hot encoding categorical columns, converting boolean columns to integers, and standardizing numerical columns.\\n\\nOnce your data is preprocessed, you can split it into a training set and a test set using stratified sampling. Then, you can initialize the XGBoost model with specified parameters such as the number of estimators, learning rate, depth, and objective.\\n\\nAfter initializing the model, you can train it on the training set using the `fit` method. Once the model is trained, you can make predictions on the test set using the `predict` method and calculate the prediction probabilities using the `predict_proba` method.\\n\\nFinally, you can evaluate the model's performance by calculating metrics such as ROC AUC and average precision, and by plotting the ROC curve and precision-recall curve. If needed, you can also tune the model's threshold for optimal performance.\\n\\nHere is a simplified example of how you might do this:\\n\\n```python\\n# model initialization\\nclf = XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss', scale_pos_weight=(len(class_0_train) / len(class_1_train)), use_label_encoder=False, verbosity=2, objective='binary:logistic', max_depth=6, learning_rate=0.01, n_estimators=500, subsample=0.8, colsample_bytree=0.6, reg_alpha=0.5, reg_lambda=1, oob_score=True)\\n\\n# training and prediction\\nclf.fit(X_train_proper, y_train_proper) # train the model on the small training set\\nprediction_prob_test = clf.predict_proba(X_test) # get the prediction probabilities for the test set\\npredictions = clf.predict(X_test) # get the predictions for the test set\\n\\n# evaluation\\nroc_auc = roc_auc_score(y_test, prediction_prob_test[:,1]) #\" return_direct=False\n",
      "\n",
      "ResponseReasoningStep -> thought='The current language of the user is: English. I have enough information to answer the question without using any more tools.' response=\"To train an XGBoost model for the Sepsis dataset, you would first need to preprocess your data. This includes loading the dataset, separating labels and features, handling missing values, one-hot encoding categorical columns, converting boolean columns to integers, and standardizing numerical columns.\\n\\nOnce your data is preprocessed, you can split it into a training set and a test set using stratified sampling. Then, you can initialize the XGBoost model with specified parameters such as the number of estimators, learning rate, depth, and objective.\\n\\nAfter initializing the model, you can train it on the training set using the `fit` method. Once the model is trained, you can make predictions on the test set using the `predict` method and calculate the prediction probabilities using the `predict_proba` method.\\n\\nFinally, you can evaluate the model's performance by calculating metrics such as ROC AUC and average precision, and by plotting the ROC curve and precision-recall curve. If needed, you can also tune the model's threshold for optimal performance.\\n\\nHere is a simplified example of how you might do this:\\n\\n```python\\n# model initialization\\nclf = XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss', scale_pos_weight=(len(class_0_train) / len(class_1_train)), use_label_encoder=False, verbosity=2, objective='binary:logistic', max_depth=6, learning_rate=0.01, n_estimators=500, subsample=0.8, colsample_bytree=0.6, reg_alpha=0.5, reg_lambda=1, oob_score=True)\\n\\n# training and prediction\\nclf.fit(X_train_proper, y_train_proper) # train the model on the small training set\\nprediction_prob_test = clf.predict_proba(X_test) # get the prediction probabilities for the test set\\npredictions = clf.predict(X_test) # get the predictions for the test\" is_streaming=False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for resoning_step in agent.get_completed_tasks()[0].extra_state[\"current_reasoning\"]:\n",
    "    print(f\"{resoning_step.__class__.__name__} -> {resoning_step}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To train an XGBoost model for the Sepsis dataset, you would first need to preprocess your data. This includes loading the dataset, separating labels and features, handling missing values, one-hot encoding categorical columns, converting boolean columns to integers, and standardizing numerical columns.\n",
      "\n",
      "Once your data is preprocessed, you can split it into a training set and a test set using stratified sampling. Then, you can initialize the XGBoost model with specified parameters such as the number of estimators, learning rate, depth, and objective.\n",
      "\n",
      "After initializing the model, you can train it on the training set using the `fit` method. Once the model is trained, you can make predictions on the test set using the `predict` method and calculate the prediction probabilities using the `predict_proba` method.\n",
      "\n",
      "Finally, you can evaluate the model's performance by calculating metrics such as ROC AUC and average precision, and by plotting the ROC curve and precision-recall curve. If needed, you can also tune the model's threshold for optimal performance.\n",
      "\n",
      "Here is a simplified example of how you might do this:\n",
      "\n",
      "```python\n",
      "# model initialization\n",
      "clf = XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss', scale_pos_weight=(len(class_0_train) / len(class_1_train)), use_label_encoder=False, verbosity=2, objective='binary:logistic', max_depth=6, learning_rate=0.01, n_estimators=500, subsample=0.8, colsample_bytree=0.6, reg_alpha=0.5, reg_lambda=1, oob_score=True)\n",
      "\n",
      "# training and prediction\n",
      "clf.fit(X_train_proper, y_train_proper) # train the model on the small training set\n",
      "prediction_prob_test = clf.predict_proba(X_test) # get the prediction probabilities for the test set\n",
      "predictions = clf.predict(X_test) # get the predictions for the test\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: The current language of the user is English. I need to provide the code for computing conformal prediction manually on a trained model. However, I don't have the necessary tools to generate the code directly. I will explain the process in a way that the user can write the code themselves.\n",
      "Answer: To compute conformal prediction manually on a trained model, you can follow these steps:\n",
      "\n",
      "1. Fit your model on the training data.\n",
      "2. Compute non-conformity scores for the calibration set. Non-conformity scores measure how different a new example is from the examples used to train the model. In the case of classification, a common choice for non-conformity score is the difference between the maximum prediction probability and the probability assigned to the true class.\n",
      "3. Sort the non-conformity scores computed in step 2 and store them in an array.\n",
      "4. For a new test example, compute its non-conformity score using the same method as in step 2.\n",
      "5. To compute the p-value for the test example, calculate the fraction of calibration examples that have a non-conformity score greater than or equal to the test example's non-conformity score.\n",
      "6. To obtain a prediction set for the test example, include all classes whose p-values are greater than a significance level alpha.\n",
      "\n",
      "Here is a simplified example of how you might do this in Python:\n",
      "\n",
      "```python\n",
      "# assume you have a trained model `clf` and calibration data `X_calib`, `y_calib`\n",
      "\n",
      "# compute non-conformity scores for calibration data\n",
      "scores_calib = []\n",
      "for i in range(len(X_calib)):\n",
      "    pred_proba = clf.predict_proba(X_calib[i].reshape(1, -1))\n",
      "    scores_calib.append(1 - pred_proba[0][y_calib[i]])\n",
      "\n",
      "# sort the scores\n",
      "scores_calib.sort()\n",
      "\n",
      "# compute non-conformity score for test example\n",
      "test_example = X_test[0]\n",
      "pred_proba_test = clf.predict_proba(test_example.reshape(1, -1))\n",
      "score_test = 1 - pred_proba\n",
      "\u001b[0mTo compute conformal prediction manually on a trained model, you can follow these steps:\n",
      "\n",
      "1. Fit your model on the training data.\n",
      "2. Compute non-conformity scores for the calibration set. Non-conformity scores measure how different a new example is from the examples used to train the model. In the case of classification, a common choice for non-conformity score is the difference between the maximum prediction probability and the probability assigned to the true class.\n",
      "3. Sort the non-conformity scores computed in step 2 and store them in an array.\n",
      "4. For a new test example, compute its non-conformity score using the same method as in step 2.\n",
      "5. To compute the p-value for the test example, calculate the fraction of calibration examples that have a non-conformity score greater than or equal to the test example's non-conformity score.\n",
      "6. To obtain a prediction set for the test example, include all classes whose p-values are greater than a significance level alpha.\n",
      "\n",
      "Here is a simplified example of how you might do this in Python:\n",
      "\n",
      "```python\n",
      "# assume you have a trained model `clf` and calibration data `X_calib`, `y_calib`\n",
      "\n",
      "# compute non-conformity scores for calibration data\n",
      "scores_calib = []\n",
      "for i in range(len(X_calib)):\n",
      "    pred_proba = clf.predict_proba(X_calib[i].reshape(1, -1))\n",
      "    scores_calib.append(1 - pred_proba[0][y_calib[i]])\n",
      "\n",
      "# sort the scores\n",
      "scores_calib.sort()\n",
      "\n",
      "# compute non-conformity score for test example\n",
      "test_example = X_test[0]\n",
      "pred_proba_test = clf.predict_proba(test_example.reshape(1, -1))\n",
      "score_test = 1 - pred_proba\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"give me the code for computing conformal prediction manually on the trained model\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To compute conformal prediction manually on a trained model, you can follow these steps:\n",
      "\n",
      "1. Fit your model on the training data.\n",
      "2. Compute non-conformity scores for the calibration set. Non-conformity scores measure how different a new example is from the examples used to train the model. In the case of classification, a common choice for non-conformity score is the difference between the maximum prediction probability and the probability assigned to the true class.\n",
      "3. Sort the non-conformity scores computed in step 2 and store them in an array.\n",
      "4. For a new test example, compute its non-conformity score using the same method as in step 2.\n",
      "5. To compute the p-value for the test example, calculate the fraction of calibration examples that have a non-conformity score greater than or equal to the test example's non-conformity score.\n",
      "6. To obtain a prediction set for the test example, include all classes whose p-values are greater than a significance level alpha.\n",
      "\n",
      "Here is a simplified example of how you might do this in Python:\n",
      "\n",
      "```python\n",
      "# assume you have a trained model `clf` and calibration data `X_calib`, `y_calib`\n",
      "\n",
      "# compute non-conformity scores for calibration data\n",
      "scores_calib = []\n",
      "for i in range(len(X_calib)):\n",
      "    pred_proba = clf.predict_proba(X_calib[i].reshape(1, -1))\n",
      "    scores_calib.append(1 - pred_proba[0][y_calib[i]])\n",
      "\n",
      "# sort the scores\n",
      "scores_calib.sort()\n",
      "\n",
      "# compute non-conformity score for test example\n",
      "test_example = X_test[0]\n",
      "pred_proba_test = clf.predict_proba(test_example.reshape(1, -1))\n",
      "score_test = 1 - pred_proba\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
